{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaa021fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all imported\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "# calculates f1 for 1:100 dataset with 95tp, 5fn, 55fp\n",
    "from sklearn.metrics import f1_score\n",
    "print('all imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68181300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 76  84  95]\n",
      "   [ 76  84  95]\n",
      "   [ 76  84  95]]\n",
      "\n",
      "  [[ 15  15  20]\n",
      "   [ 15  15  20]\n",
      "   [ 15  15  20]]\n",
      "\n",
      "  [[  8   7  11]\n",
      "   [  8   7  11]\n",
      "   [  8   7  11]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 16  14  25]\n",
      "   [ 16  14  25]\n",
      "   [ 16  14  25]]\n",
      "\n",
      "  [[ 13  13  22]\n",
      "   [ 13  13  22]\n",
      "   [ 13  13  22]]\n",
      "\n",
      "  [[  8   9  13]\n",
      "   [  8   9  13]\n",
      "   [  8   9  13]]]\n",
      "\n",
      "\n",
      " [[[ 88  96 107]\n",
      "   [ 88  96 107]\n",
      "   [ 88  96 107]]\n",
      "\n",
      "  [[ 17  17  22]\n",
      "   [ 17  17  22]\n",
      "   [ 17  17  22]]\n",
      "\n",
      "  [[  6   5   9]\n",
      "   [  6   5   9]\n",
      "   [  6   5   9]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 15  13  23]\n",
      "   [ 15  13  23]\n",
      "   [ 15  13  23]]\n",
      "\n",
      "  [[ 15  13  24]\n",
      "   [ 15  13  24]\n",
      "   [ 15  13  24]]\n",
      "\n",
      "  [[ 10  10  16]\n",
      "   [ 10  10  16]\n",
      "   [ 10  10  16]]]\n",
      "\n",
      "\n",
      " [[[ 86  91  98]\n",
      "   [ 86  91  98]\n",
      "   [ 86  91  98]]\n",
      "\n",
      "  [[ 14  13  17]\n",
      "   [ 14  13  17]\n",
      "   [ 14  13  17]]\n",
      "\n",
      "  [[  5   4   7]\n",
      "   [  5   4   7]\n",
      "   [  5   4   7]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 15  13  23]\n",
      "   [ 15  13  23]\n",
      "   [ 15  13  23]]\n",
      "\n",
      "  [[ 15  13  23]\n",
      "   [ 15  13  23]\n",
      "   [ 15  13  23]]\n",
      "\n",
      "  [[ 12  11  18]\n",
      "   [ 12  11  18]\n",
      "   [ 12  11  18]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[157 162 174]\n",
      "   [157 162 174]\n",
      "   [157 162 174]]\n",
      "\n",
      "  [[159 165 176]\n",
      "   [159 165 176]\n",
      "   [159 165 176]]\n",
      "\n",
      "  [[155 158 159]\n",
      "   [155 158 159]\n",
      "   [155 158 159]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[126 136 168]\n",
      "   [126 136 168]\n",
      "   [126 136 168]]\n",
      "\n",
      "  [[145 153 178]\n",
      "   [145 153 178]\n",
      "   [145 153 178]]\n",
      "\n",
      "  [[150 157 180]\n",
      "   [150 157 180]\n",
      "   [150 157 180]]]\n",
      "\n",
      "\n",
      " [[[158 162 174]\n",
      "   [158 162 174]\n",
      "   [158 162 174]]\n",
      "\n",
      "  [[158 164 177]\n",
      "   [158 164 177]\n",
      "   [158 164 177]]\n",
      "\n",
      "  [[160 167 179]\n",
      "   [160 167 179]\n",
      "   [160 167 179]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[134 144 172]\n",
      "   [134 144 172]\n",
      "   [134 144 172]]\n",
      "\n",
      "  [[143 151 177]\n",
      "   [143 151 177]\n",
      "   [143 151 177]]\n",
      "\n",
      "  [[151 158 180]\n",
      "   [151 158 180]\n",
      "   [151 158 180]]]\n",
      "\n",
      "\n",
      " [[[160 164 176]\n",
      "   [160 164 176]\n",
      "   [160 164 176]]\n",
      "\n",
      "  [[158 164 177]\n",
      "   [158 164 177]\n",
      "   [158 164 177]]\n",
      "\n",
      "  [[163 169 181]\n",
      "   [163 169 181]\n",
      "   [163 169 181]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[141 150 174]\n",
      "   [141 150 174]\n",
      "   [141 150 174]]\n",
      "\n",
      "  [[140 149 176]\n",
      "   [140 149 176]\n",
      "   [140 149 176]]\n",
      "\n",
      "  [[150 158 180]\n",
      "   [150 158 180]\n",
      "   [150 158 180]]]]\n"
     ]
    }
   ],
   "source": [
    "def readImage(filePath):\n",
    "    img = Image.open(filePath)\n",
    "    \n",
    "    size=(30,40)\n",
    "    #resize image\n",
    "    out = img.resize(size)\n",
    "\n",
    "    # asarray() class is used to convert\n",
    "    # PIL images into NumPy arrays\n",
    "    numpydata = asarray(out)\n",
    "    numpydata = np.repeat(numpydata[:, :, np.newaxis], 3, axis=2)\n",
    "    # <class 'numpy.ndarray'>\n",
    "    #print(type(numpydata))\n",
    "\n",
    "    #  shape\n",
    "    #print(numpydata.shape)\n",
    "    return numpydata\n",
    "print(readImage('data/disease/measles (1).png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fd11c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4164443_down-syndrome-down-syndrome-transparent-background-transparent-png.png', 'main-qimg-4c812ebe5c1bdc504625b6961032fd3b.png', 'measles (1).png', 'measles (10).jpg', 'measles (140).jpg']\n"
     ]
    }
   ],
   "source": [
    "listFiles = os.listdir('data/disease/')\n",
    "print(listFiles[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b74ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[[243, 243, 243],\n",
      "         [243, 243, 243],\n",
      "         [243, 243, 243]],\n",
      "\n",
      "        [[242, 242, 242],\n",
      "         [242, 242, 242],\n",
      "         [242, 242, 242]],\n",
      "\n",
      "        [[242, 242, 242],\n",
      "         [242, 242, 242],\n",
      "         [242, 242, 242]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[242, 242, 242],\n",
      "         [242, 242, 242],\n",
      "         [242, 242, 242]],\n",
      "\n",
      "        [[242, 242, 242],\n",
      "         [242, 242, 242],\n",
      "         [242, 242, 242]],\n",
      "\n",
      "        [[242, 242, 242],\n",
      "         [242, 242, 242],\n",
      "         [242, 242, 242]]],\n",
      "\n",
      "\n",
      "       [[[242, 242, 242],\n",
      "         [242, 242, 242],\n",
      "         [242, 242, 242]],\n",
      "\n",
      "        [[242, 242, 242],\n",
      "         [242, 242, 242],\n",
      "         [242, 242, 242]],\n",
      "\n",
      "        [[242, 242, 242],\n",
      "         [242, 242, 242],\n",
      "         [242, 242, 242]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[242, 242, 242],\n",
      "         [242, 242, 242],\n",
      "         [242, 242, 242]],\n",
      "\n",
      "        [[242, 242, 242],\n",
      "         [242, 242, 242],\n",
      "         [242, 242, 242]],\n",
      "\n",
      "        [[242, 242, 242],\n",
      "         [242, 242, 242],\n",
      "         [242, 242, 242]]],\n",
      "\n",
      "\n",
      "       [[[242, 242, 242],\n",
      "         [242, 242, 242],\n",
      "         [242, 242, 242]],\n",
      "\n",
      "        [[242, 242, 242],\n",
      "         [242, 242, 242],\n",
      "         [242, 242, 242]],\n",
      "\n",
      "        [[242, 242, 242],\n",
      "         [242, 242, 242],\n",
      "         [242, 242, 242]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[242, 242, 242],\n",
      "         [242, 242, 242],\n",
      "         [242, 242, 242]],\n",
      "\n",
      "        [[242, 242, 242],\n",
      "         [242, 242, 242],\n",
      "         [242, 242, 242]],\n",
      "\n",
      "        [[242, 242, 242],\n",
      "         [242, 242, 242],\n",
      "         [242, 242, 242]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[243, 242, 244],\n",
      "         [243, 242, 244],\n",
      "         [243, 242, 244]],\n",
      "\n",
      "        [[245, 242, 248],\n",
      "         [245, 242, 248],\n",
      "         [245, 242, 248]],\n",
      "\n",
      "        [[250, 250, 253],\n",
      "         [250, 250, 253],\n",
      "         [250, 250, 253]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[214, 218, 239],\n",
      "         [214, 218, 239],\n",
      "         [214, 218, 239]],\n",
      "\n",
      "        [[205, 209, 233],\n",
      "         [205, 209, 233],\n",
      "         [205, 209, 233]],\n",
      "\n",
      "        [[213, 215, 234],\n",
      "         [213, 215, 234],\n",
      "         [213, 215, 234]]],\n",
      "\n",
      "\n",
      "       [[[238, 236, 241],\n",
      "         [238, 236, 241],\n",
      "         [238, 236, 241]],\n",
      "\n",
      "        [[235, 230, 238],\n",
      "         [235, 230, 238],\n",
      "         [235, 230, 238]],\n",
      "\n",
      "        [[240, 236, 243],\n",
      "         [240, 236, 243],\n",
      "         [240, 236, 243]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[210, 214, 238],\n",
      "         [210, 214, 238],\n",
      "         [210, 214, 238]],\n",
      "\n",
      "        [[202, 206, 232],\n",
      "         [202, 206, 232],\n",
      "         [202, 206, 232]],\n",
      "\n",
      "        [[206, 208, 231],\n",
      "         [206, 208, 231],\n",
      "         [206, 208, 231]]],\n",
      "\n",
      "\n",
      "       [[[226, 219, 227],\n",
      "         [226, 219, 227],\n",
      "         [226, 219, 227]],\n",
      "\n",
      "        [[221, 211, 220],\n",
      "         [221, 211, 220],\n",
      "         [221, 211, 220]],\n",
      "\n",
      "        [[219, 210, 222],\n",
      "         [219, 210, 222],\n",
      "         [219, 210, 222]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[207, 212, 237],\n",
      "         [207, 212, 237],\n",
      "         [207, 212, 237]],\n",
      "\n",
      "        [[207, 212, 236],\n",
      "         [207, 212, 236],\n",
      "         [207, 212, 236]],\n",
      "\n",
      "        [[207, 210, 233],\n",
      "         [207, 210, 233],\n",
      "         [207, 210, 233]]]], dtype=uint8), array([[[[ 13,  44,  23, 255],\n",
      "         [ 13,  44,  23, 255],\n",
      "         [ 13,  44,  23, 255]],\n",
      "\n",
      "        [[ 14,  48,  26, 255],\n",
      "         [ 14,  48,  26, 255],\n",
      "         [ 14,  48,  26, 255]],\n",
      "\n",
      "        [[ 17,  54,  31, 255],\n",
      "         [ 17,  54,  31, 255],\n",
      "         [ 17,  54,  31, 255]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 56,  87,  59, 255],\n",
      "         [ 56,  87,  59, 255],\n",
      "         [ 56,  87,  59, 255]],\n",
      "\n",
      "        [[ 55,  83,  56, 255],\n",
      "         [ 55,  83,  56, 255],\n",
      "         [ 55,  83,  56, 255]],\n",
      "\n",
      "        [[ 55,  81,  54, 255],\n",
      "         [ 55,  81,  54, 255],\n",
      "         [ 55,  81,  54, 255]]],\n",
      "\n",
      "\n",
      "       [[[ 13,  44,  23, 255],\n",
      "         [ 13,  44,  23, 255],\n",
      "         [ 13,  44,  23, 255]],\n",
      "\n",
      "        [[ 14,  49,  27, 255],\n",
      "         [ 14,  49,  27, 255],\n",
      "         [ 14,  49,  27, 255]],\n",
      "\n",
      "        [[ 18,  55,  33, 255],\n",
      "         [ 18,  55,  33, 255],\n",
      "         [ 18,  55,  33, 255]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 59,  92,  65, 255],\n",
      "         [ 59,  92,  65, 255],\n",
      "         [ 59,  92,  65, 255]],\n",
      "\n",
      "        [[ 56,  85,  58, 255],\n",
      "         [ 56,  85,  58, 255],\n",
      "         [ 56,  85,  58, 255]],\n",
      "\n",
      "        [[ 56,  81,  54, 255],\n",
      "         [ 56,  81,  54, 255],\n",
      "         [ 56,  81,  54, 255]]],\n",
      "\n",
      "\n",
      "       [[[ 14,  45,  24, 255],\n",
      "         [ 14,  45,  24, 255],\n",
      "         [ 14,  45,  24, 255]],\n",
      "\n",
      "        [[ 15,  49,  27, 255],\n",
      "         [ 15,  49,  27, 255],\n",
      "         [ 15,  49,  27, 255]],\n",
      "\n",
      "        [[ 19,  56,  34, 255],\n",
      "         [ 19,  56,  34, 255],\n",
      "         [ 19,  56,  34, 255]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[107, 135, 112, 255],\n",
      "         [107, 135, 112, 255],\n",
      "         [107, 135, 112, 255]],\n",
      "\n",
      "        [[ 92, 118,  92, 255],\n",
      "         [ 92, 118,  92, 255],\n",
      "         [ 92, 118,  92, 255]],\n",
      "\n",
      "        [[ 60,  84,  56, 255],\n",
      "         [ 60,  84,  56, 255],\n",
      "         [ 60,  84,  56, 255]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[ 29,  32,  17, 255],\n",
      "         [ 29,  32,  17, 255],\n",
      "         [ 29,  32,  17, 255]],\n",
      "\n",
      "        [[ 53,  83,  57, 255],\n",
      "         [ 53,  83,  57, 255],\n",
      "         [ 53,  83,  57, 255]],\n",
      "\n",
      "        [[ 45,  70,  47, 255],\n",
      "         [ 45,  70,  47, 255],\n",
      "         [ 45,  70,  47, 255]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[156, 135, 101, 255],\n",
      "         [156, 135, 101, 255],\n",
      "         [156, 135, 101, 255]],\n",
      "\n",
      "        [[138, 122,  87, 255],\n",
      "         [138, 122,  87, 255],\n",
      "         [138, 122,  87, 255]],\n",
      "\n",
      "        [[142, 132,  99, 255],\n",
      "         [142, 132,  99, 255],\n",
      "         [142, 132,  99, 255]]],\n",
      "\n",
      "\n",
      "       [[[ 32,  49,  29, 255],\n",
      "         [ 32,  49,  29, 255],\n",
      "         [ 32,  49,  29, 255]],\n",
      "\n",
      "        [[ 50,  79,  53, 255],\n",
      "         [ 50,  79,  53, 255],\n",
      "         [ 50,  79,  53, 255]],\n",
      "\n",
      "        [[ 44,  68,  45, 255],\n",
      "         [ 44,  68,  45, 255],\n",
      "         [ 44,  68,  45, 255]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[158, 137, 102, 255],\n",
      "         [158, 137, 102, 255],\n",
      "         [158, 137, 102, 255]],\n",
      "\n",
      "        [[143, 127,  92, 255],\n",
      "         [143, 127,  92, 255],\n",
      "         [143, 127,  92, 255]],\n",
      "\n",
      "        [[147, 136, 103, 255],\n",
      "         [147, 136, 103, 255],\n",
      "         [147, 136, 103, 255]]],\n",
      "\n",
      "\n",
      "       [[[ 29,  50,  30, 255],\n",
      "         [ 29,  50,  30, 255],\n",
      "         [ 29,  50,  30, 255]],\n",
      "\n",
      "        [[ 44,  69,  45, 255],\n",
      "         [ 44,  69,  45, 255],\n",
      "         [ 44,  69,  45, 255]],\n",
      "\n",
      "        [[ 51,  76,  51, 255],\n",
      "         [ 51,  76,  51, 255],\n",
      "         [ 51,  76,  51, 255]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[156, 136, 101, 255],\n",
      "         [156, 136, 101, 255],\n",
      "         [156, 136, 101, 255]],\n",
      "\n",
      "        [[148, 132,  97, 255],\n",
      "         [148, 132,  97, 255],\n",
      "         [148, 132,  97, 255]],\n",
      "\n",
      "        [[149, 133,  98, 255],\n",
      "         [149, 133,  98, 255],\n",
      "         [149, 133,  98, 255]]]], dtype=uint8), array([[[[ 76,  84,  95],\n",
      "         [ 76,  84,  95],\n",
      "         [ 76,  84,  95]],\n",
      "\n",
      "        [[ 15,  15,  20],\n",
      "         [ 15,  15,  20],\n",
      "         [ 15,  15,  20]],\n",
      "\n",
      "        [[  8,   7,  11],\n",
      "         [  8,   7,  11],\n",
      "         [  8,   7,  11]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 16,  14,  25],\n",
      "         [ 16,  14,  25],\n",
      "         [ 16,  14,  25]],\n",
      "\n",
      "        [[ 13,  13,  22],\n",
      "         [ 13,  13,  22],\n",
      "         [ 13,  13,  22]],\n",
      "\n",
      "        [[  8,   9,  13],\n",
      "         [  8,   9,  13],\n",
      "         [  8,   9,  13]]],\n",
      "\n",
      "\n",
      "       [[[ 88,  96, 107],\n",
      "         [ 88,  96, 107],\n",
      "         [ 88,  96, 107]],\n",
      "\n",
      "        [[ 17,  17,  22],\n",
      "         [ 17,  17,  22],\n",
      "         [ 17,  17,  22]],\n",
      "\n",
      "        [[  6,   5,   9],\n",
      "         [  6,   5,   9],\n",
      "         [  6,   5,   9]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 15,  13,  23],\n",
      "         [ 15,  13,  23],\n",
      "         [ 15,  13,  23]],\n",
      "\n",
      "        [[ 15,  13,  24],\n",
      "         [ 15,  13,  24],\n",
      "         [ 15,  13,  24]],\n",
      "\n",
      "        [[ 10,  10,  16],\n",
      "         [ 10,  10,  16],\n",
      "         [ 10,  10,  16]]],\n",
      "\n",
      "\n",
      "       [[[ 86,  91,  98],\n",
      "         [ 86,  91,  98],\n",
      "         [ 86,  91,  98]],\n",
      "\n",
      "        [[ 14,  13,  17],\n",
      "         [ 14,  13,  17],\n",
      "         [ 14,  13,  17]],\n",
      "\n",
      "        [[  5,   4,   7],\n",
      "         [  5,   4,   7],\n",
      "         [  5,   4,   7]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 15,  13,  23],\n",
      "         [ 15,  13,  23],\n",
      "         [ 15,  13,  23]],\n",
      "\n",
      "        [[ 15,  13,  23],\n",
      "         [ 15,  13,  23],\n",
      "         [ 15,  13,  23]],\n",
      "\n",
      "        [[ 12,  11,  18],\n",
      "         [ 12,  11,  18],\n",
      "         [ 12,  11,  18]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[157, 162, 174],\n",
      "         [157, 162, 174],\n",
      "         [157, 162, 174]],\n",
      "\n",
      "        [[159, 165, 176],\n",
      "         [159, 165, 176],\n",
      "         [159, 165, 176]],\n",
      "\n",
      "        [[155, 158, 159],\n",
      "         [155, 158, 159],\n",
      "         [155, 158, 159]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[126, 136, 168],\n",
      "         [126, 136, 168],\n",
      "         [126, 136, 168]],\n",
      "\n",
      "        [[145, 153, 178],\n",
      "         [145, 153, 178],\n",
      "         [145, 153, 178]],\n",
      "\n",
      "        [[150, 157, 180],\n",
      "         [150, 157, 180],\n",
      "         [150, 157, 180]]],\n",
      "\n",
      "\n",
      "       [[[158, 162, 174],\n",
      "         [158, 162, 174],\n",
      "         [158, 162, 174]],\n",
      "\n",
      "        [[158, 164, 177],\n",
      "         [158, 164, 177],\n",
      "         [158, 164, 177]],\n",
      "\n",
      "        [[160, 167, 179],\n",
      "         [160, 167, 179],\n",
      "         [160, 167, 179]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[134, 144, 172],\n",
      "         [134, 144, 172],\n",
      "         [134, 144, 172]],\n",
      "\n",
      "        [[143, 151, 177],\n",
      "         [143, 151, 177],\n",
      "         [143, 151, 177]],\n",
      "\n",
      "        [[151, 158, 180],\n",
      "         [151, 158, 180],\n",
      "         [151, 158, 180]]],\n",
      "\n",
      "\n",
      "       [[[160, 164, 176],\n",
      "         [160, 164, 176],\n",
      "         [160, 164, 176]],\n",
      "\n",
      "        [[158, 164, 177],\n",
      "         [158, 164, 177],\n",
      "         [158, 164, 177]],\n",
      "\n",
      "        [[163, 169, 181],\n",
      "         [163, 169, 181],\n",
      "         [163, 169, 181]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[141, 150, 174],\n",
      "         [141, 150, 174],\n",
      "         [141, 150, 174]],\n",
      "\n",
      "        [[140, 149, 176],\n",
      "         [140, 149, 176],\n",
      "         [140, 149, 176]],\n",
      "\n",
      "        [[150, 158, 180],\n",
      "         [150, 158, 180],\n",
      "         [150, 158, 180]]]], dtype=uint8)]\n",
      "(40, 30, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "#load Benign\n",
    "listCancer = []\n",
    "listResults = []\n",
    "for elem in listFiles:\n",
    "    img = readImage('data/disease/'+elem)\n",
    "    listCancer += [img]\n",
    "    listResults += [np.array([1])]\n",
    "print(listCancer[:3])\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7cf790e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 (1).jpeg', '1 (1).jpg', '1 (1).png', '1 (10).jpeg', '1 (10).jpg']\n"
     ]
    }
   ],
   "source": [
    "listFiles = os.listdir('data/health/')\n",
    "print(listFiles[:5])\n",
    "for elem in listFiles:\n",
    "    img = readImage('data/health/'+elem)\n",
    "    listResults += [np.array([0])]\n",
    "    listCancer += [img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162b926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6134bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(len(listCancer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15204a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b91ed89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e031e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting LR for different number of Epochs\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    lr *= epoch / 100\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e7b97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic ResNet Building Block\n",
    "def resnet_layer(inputs, conv_first = False,\n",
    "\t\t\t\tnum_filters = 16,\n",
    "\t\t\t\tkernel_size = 3,\n",
    "\t\t\t\tstrides = 1,\n",
    "\t\t\t\tactivation ='relu',\n",
    "\t\t\t\tbatch_normalization = True):\n",
    "\tconv = Conv2D(num_filters,\n",
    "\t\t\t\tkernel_size = kernel_size,\n",
    "\t\t\t\tstrides = strides,\n",
    "\t\t\t\tpadding ='same',\n",
    "\t\t\t\tkernel_initializer ='he_normal',\n",
    "\t\t\t\tkernel_regularizer = l2(1e-4))\n",
    "\n",
    "\tx = inputs\n",
    "\tif conv_first:\n",
    "\t\tx = conv(x)\n",
    "\t\tif batch_normalization:\n",
    "\t\t\tx = BatchNormalization()(x)\n",
    "\t\tif activation is not None:\n",
    "\t\t\tx = Activation(activation)(x)\n",
    "\telse:\n",
    "\t\tif batch_normalization:\n",
    "\t\t\tx = BatchNormalization()(x)\n",
    "\t\tif activation is not None:\n",
    "\t\t\tx = Activation(activation)(x)\n",
    "\t\tx = conv(x)\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c3ccc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet V2 architecture\n",
    "def resnet_v2(input_shape, depth, num_classes = 10):\n",
    "\tif (depth - 2) % 9 != 0:\n",
    "\t\traise ValueError('depth should be 9n + 2 (eg 56 or 110 in [b])')\n",
    "\t# Start model definition.\n",
    "\tnum_filters_in = 16\n",
    "\tnum_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "\tinputs = Input(shape = input_shape)\n",
    "\t# v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "\tx = resnet_layer(inputs = inputs,num_filters = num_filters_in,conv_first = True)\n",
    "\n",
    "\t# Instantiate the stack of residual units\n",
    "\tfor stage in range(3):\n",
    "\t\tfor res_block in range(num_res_blocks):\n",
    "\t\t\tactivation = 'relu'\n",
    "\t\t\tbatch_normalization = True\n",
    "\t\t\tstrides = 1\n",
    "\t\t\tif stage == 0:\n",
    "\t\t\t\tnum_filters_out = num_filters_in * 4\n",
    "\t\t\t\tif res_block == 0: # first layer and first stage\n",
    "\t\t\t\t\tactivation = None\n",
    "\t\t\t\t\tbatch_normalization = False\n",
    "\t\t\telse:\n",
    "\t\t\t\tnum_filters_out = num_filters_in * 2\n",
    "\t\t\t\tif res_block == 0: # first layer but not first stage\n",
    "\t\t\t\t\tstrides = 2 # downsample\n",
    "\n",
    "\t\t\t# bottleneck residual unit\n",
    "\t\t\ty = resnet_layer(inputs = x,\n",
    "\t\t\t\t\t\t\tnum_filters = num_filters_in,\n",
    "\t\t\t\t\t\t\tkernel_size = 1,\n",
    "\t\t\t\t\t\t\tstrides = strides,\n",
    "\t\t\t\t\t\t\tactivation = activation,\n",
    "\t\t\t\t\t\t\tbatch_normalization = batch_normalization)\n",
    "\t\t\ty = resnet_layer(inputs = y,\n",
    "\t\t\t\t\t\t\tnum_filters = num_filters_in,\n",
    "\t\t\t\t\t\t\tconv_first = False)\n",
    "\t\t\ty = resnet_layer(inputs = y,\n",
    "\t\t\t\t\t\t\tnum_filters = num_filters_out,\n",
    "\t\t\t\t\t\t\tkernel_size = 1,\n",
    "\t\t\t\t\t\t\tconv_first = False)\n",
    "\t\t\tif res_block == 0:\n",
    "\t\t\t\t# linear projection residual shortcut connection to match\n",
    "\t\t\t\t# changed dims\n",
    "\t\t\t\tx = resnet_layer(inputs = x,\n",
    "\t\t\t\t\t\t\t\tnum_filters = num_filters_out,\n",
    "\t\t\t\t\t\t\t\tkernel_size = 1,\n",
    "\t\t\t\t\t\t\t\tstrides = strides,\n",
    "\t\t\t\t\t\t\t\tactivation = None,\n",
    "\t\t\t\t\t\t\t\tbatch_normalization = False)\n",
    "\t\t\tx = keras.layers.add([x, y])\n",
    "\n",
    "\t\tnum_filters_in = num_filters_out\n",
    "\n",
    "\t# Add classifier on top.\n",
    "\t# v2 has BN-ReLU before Pooling\n",
    "\tx = BatchNormalization()(x)\n",
    "\tx = Activation('relu')(x)\n",
    "\tx = AveragePooling2D(pool_size = 8)(x)\n",
    "\ty = Flatten()(x)\n",
    "\toutputs = Dense(num_classes,\n",
    "\t\t\t\t\tactivation ='softmax',\n",
    "\t\t\t\t\tkernel_initializer ='he_normal')(y)\n",
    "\n",
    "\t# Instantiate model.\n",
    "\tmodel = Model(inputs = inputs, outputs = outputs)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71df1011",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=[]\n",
    "y_train=[]\n",
    "accs=[]\n",
    "def main(pz, created_model):\n",
    "    \n",
    "    per = np.random.permutation(len(listCancer))\n",
    "    ln = int(len(listCancer) * 0.55)\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    positions = []\n",
    "    #for i in range(ln):\n",
    "    #    positions += [per[i]]\n",
    "    val = 1\n",
    "    while ln > 0:\n",
    "        val = 1-val\n",
    "        ln-=1\n",
    "        for i in range(len(per)):\n",
    "            if per[i] in positions:\n",
    "                continue\n",
    "            if listResults[per[i]] == np.array([val]):\n",
    "                positions += [per[i]]\n",
    "                break\n",
    "    for i in range(len(listCancer)):\n",
    "        if len(listCancer[i].shape)==3:\n",
    "            continue\n",
    "        if listCancer[i].shape[3] == 4:\n",
    "            continue\n",
    "        #print(listCancer[i].shape)\n",
    "        if i in positions:\n",
    "            x_train += [listCancer[i]]\n",
    "            y_train += [listResults[i]]\n",
    "        else:\n",
    "            x_test += [listCancer[i]]\n",
    "            y_test += [listResults[i]]\n",
    "            \n",
    "    ln = len(x_test) // 2\n",
    "    \n",
    "    x_valid = x_test[ln:]\n",
    "    y_valid = y_test[ln:]\n",
    "    \n",
    "    print(len(x_valid))\n",
    "    print(len(y_valid))\n",
    "    \n",
    "    \n",
    "    x_test = x_test[:ln]\n",
    "    y_test = y_test[:ln]\n",
    "    \n",
    "    \n",
    "    x_train = np.array(x_train)\n",
    "    x_train = x_train.reshape(len(x_train), 40, 30, 9)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "\n",
    "    x_test = np.array(x_test)\n",
    "    x_test = x_test.reshape(len(x_test), 40, 30, 9)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    x_valid = np.array(x_valid)\n",
    "    x_valid = x_valid.reshape(len(x_valid), 40, 30, 9)\n",
    "    y_valid = np.array(y_valid)\n",
    "    \n",
    "    \n",
    "    batch_size = 64 # original ResNet paper uses batch_size = 128 for training\n",
    "    epochs = 10\n",
    "    data_augmentation = True\n",
    "    num_classes = 10\n",
    "\n",
    "    # Data Preprocessing\n",
    "    subtract_pixel_mean = True\n",
    "    n = 3\n",
    "\n",
    "    # Select ResNet Version\n",
    "    version = 2\n",
    "\n",
    "    # Computed depth of\n",
    "    if version == 1:\n",
    "        depth = n * 6 + 2\n",
    "    elif version == 2:\n",
    "        depth = n * 9 + 2\n",
    "\n",
    "    # Model name, depth and version\n",
    "    model_type = 'ResNet % dv % d' % (depth, version)\n",
    "\n",
    "    # use the data\n",
    "    print(x_train[:3])\n",
    "    print(y_train[:3])\n",
    "    print(type(x_train[0]))\n",
    "    print(type(x_train))\n",
    "    print(type(y_train[0]))\n",
    "    print(type(y_train))\n",
    "\n",
    "    # Input image dimensions.\n",
    "    input_shape = x_train.shape[1:]\n",
    "    print(input_shape)\n",
    "\n",
    "    # Normalize data.\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "    x_valid = x_test.astype('float32') / 255\n",
    "\n",
    "\n",
    "    # Print Training and Test Samples\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    print('y_train shape:', y_train.shape)\n",
    "\n",
    "    # Convert class vectors to binary class matrices.\n",
    "    y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
    "    y_valid = keras.utils.np_utils.to_categorical(y_valid, num_classes)\n",
    "    \n",
    "    \n",
    "    model = resnet_v2(input_shape = input_shape, depth = depth)\n",
    "\n",
    "    model.compile(loss ='categorical_crossentropy',\n",
    "                optimizer = Adam(learning_rate = lr_schedule(0)),\n",
    "                metrics =['accuracy'])\n",
    "    model.summary()\n",
    "    print(model_type)\n",
    "\n",
    "    # Prepare model model saving directory.\n",
    "    save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "    model_name = 'cifar10_% s_model.{epoch:03d}.h5' % model_type\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "    # Prepare callbacks for model saving and for learning rate adjustment.\n",
    "    checkpoint = ModelCheckpoint(filepath = filepath,\n",
    "                                monitor ='val_acc',\n",
    "                                verbose = 1,\n",
    "                                save_best_only = True)\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "    lr_reducer = ReduceLROnPlateau(factor = np.sqrt(0.1),\n",
    "                                cooldown = 0,\n",
    "                                patience = 5,\n",
    "                                min_lr = 0.5e-6)\n",
    "\n",
    "    callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "    \n",
    "    while len(y_valid) > len(x_valid):\n",
    "        y_valid = np.delete(y_valid, len(y_valid)-1)\n",
    "\n",
    "    # Run training, with or without data augmentation.\n",
    "    if not data_augmentation:\n",
    "        print('Not using data augmentation.')\n",
    "        model.fit(x_train, y_train,\n",
    "                batch_size = batch_size,\n",
    "                epochs = epochs,\n",
    "                validation_data =(x_test, y_test),\n",
    "                shuffle = True,\n",
    "                callbacks = callbacks)\n",
    "    else:\n",
    "        print('Using real-time data augmentation.')\n",
    "        # This will do preprocessing and realtime data augmentation:\n",
    "        datagen = ImageDataGenerator(\n",
    "            # set input mean to 0 over the dataset\n",
    "            featurewise_center = False,\n",
    "            # set each sample mean to 0\n",
    "            samplewise_center = False,\n",
    "            # divide inputs by std of dataset\n",
    "            featurewise_std_normalization = False,\n",
    "            # divide each input by its std\n",
    "            samplewise_std_normalization = False,\n",
    "            # apply ZCA whitening\n",
    "            zca_whitening = False,\n",
    "            # epsilon for ZCA whitening\n",
    "            zca_epsilon = 1e-06,\n",
    "            # randomly rotate images in the range (deg 0 to 180)\n",
    "            rotation_range = 0,\n",
    "            # randomly shift images horizontally\n",
    "            width_shift_range = 0.1,\n",
    "            # randomly shift images vertically\n",
    "            height_shift_range = 0.1,\n",
    "            # set range for random shear\n",
    "            shear_range = 0.,\n",
    "            # set range for random zoom\n",
    "            zoom_range = 0.,\n",
    "            # set range for random channel shifts\n",
    "            channel_shift_range = 0.,\n",
    "            # set mode for filling points outside the input boundaries\n",
    "            fill_mode ='nearest',\n",
    "            # value used for fill_mode = \"constant\"\n",
    "            cval = 0.,\n",
    "            # randomly flip images\n",
    "            horizontal_flip = True,\n",
    "            # randomly flip images\n",
    "            vertical_flip = False,\n",
    "            # set rescaling factor (applied before any other transformation)\n",
    "            rescale = None,\n",
    "            # set function that will be applied on each input\n",
    "            preprocessing_function = None,\n",
    "            # image data format, either \"channels_first\" or \"channels_last\"\n",
    "            data_format = None,\n",
    "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "            validation_split = 0.1)\n",
    "\n",
    "        # Compute quantities required for featurewise normalization\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "        print('------------------')\n",
    "        print(x_train)\n",
    "        datagen.fit(x_train)\n",
    "\n",
    "        # Fit the model on the batches generated by datagen.flow().\n",
    "        model.fit_generator(datagen.flow(x_train, y_train, batch_size = batch_size),\n",
    "                            validation_data =(x_valid, y_valid),\n",
    "                            epochs = 100, verbose = 1, workers = 2,\n",
    "                            callbacks = callbacks)\n",
    "\n",
    "    # Score trained model.\n",
    "    scores = model.evaluate(x_test, y_test, verbose = 1)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    ret = model.predict(x_test)\n",
    "    result = []\n",
    "    pred = []\n",
    "    for a,b in zip(ret, y_test):\n",
    "        pred+=[a.argmax(axis=-1)]\n",
    "        result+=[b.argmax(axis=-1)]\n",
    "    print(pred)\n",
    "    print(result)\n",
    "    precision = precision_score(result, pred, average='weighted')\n",
    "    recall = recall_score(result, pred, average='weighted')\n",
    "     # calculates f1 for 1:100 dataset with 95tp, 5fn, 55fp\n",
    "    fmeasure = f1_score(result, pred, average='weighted')\n",
    "    print('Precision: ', precision)\n",
    "    print('Recall: ', recall)\n",
    "    print('F1 Score: ', fmeasure)\n",
    "    #precisions += [precision]\n",
    "    #recalls += [recall]\n",
    "    #model.save('saved_models/resNetV'+pz+'.h5')\n",
    "    return [scores[1], precision, recall, fmeasure]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee89ef05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "29\n",
      "[[[[ 76  84  95 ...  76  84  95]\n",
      "   [ 15  15  20 ...  15  15  20]\n",
      "   [  8   7  11 ...   8   7  11]\n",
      "   ...\n",
      "   [ 16  14  25 ...  16  14  25]\n",
      "   [ 13  13  22 ...  13  13  22]\n",
      "   [  8   9  13 ...   8   9  13]]\n",
      "\n",
      "  [[ 88  96 107 ...  88  96 107]\n",
      "   [ 17  17  22 ...  17  17  22]\n",
      "   [  6   5   9 ...   6   5   9]\n",
      "   ...\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 15  13  24 ...  15  13  24]\n",
      "   [ 10  10  16 ...  10  10  16]]\n",
      "\n",
      "  [[ 86  91  98 ...  86  91  98]\n",
      "   [ 14  13  17 ...  14  13  17]\n",
      "   [  5   4   7 ...   5   4   7]\n",
      "   ...\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 12  11  18 ...  12  11  18]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[157 162 174 ... 157 162 174]\n",
      "   [159 165 176 ... 159 165 176]\n",
      "   [155 158 159 ... 155 158 159]\n",
      "   ...\n",
      "   [126 136 168 ... 126 136 168]\n",
      "   [145 153 178 ... 145 153 178]\n",
      "   [150 157 180 ... 150 157 180]]\n",
      "\n",
      "  [[158 162 174 ... 158 162 174]\n",
      "   [158 164 177 ... 158 164 177]\n",
      "   [160 167 179 ... 160 167 179]\n",
      "   ...\n",
      "   [134 144 172 ... 134 144 172]\n",
      "   [143 151 177 ... 143 151 177]\n",
      "   [151 158 180 ... 151 158 180]]\n",
      "\n",
      "  [[160 164 176 ... 160 164 176]\n",
      "   [158 164 177 ... 158 164 177]\n",
      "   [163 169 181 ... 163 169 181]\n",
      "   ...\n",
      "   [141 150 174 ... 141 150 174]\n",
      "   [140 149 176 ... 140 149 176]\n",
      "   [150 158 180 ... 150 158 180]]]\n",
      "\n",
      "\n",
      " [[[ 93  64  41 ...  93  64  41]\n",
      "   [101  73  50 ... 101  73  50]\n",
      "   [109  83  61 ... 109  83  61]\n",
      "   ...\n",
      "   [ 92  57  35 ...  92  57  35]\n",
      "   [ 88  52  30 ...  88  52  30]\n",
      "   [ 83  47  26 ...  83  47  26]]\n",
      "\n",
      "  [[101  73  50 ... 101  73  50]\n",
      "   [109  82  60 ... 109  82  60]\n",
      "   [116  92  70 ... 116  92  70]\n",
      "   ...\n",
      "   [ 97  62  39 ...  97  62  39]\n",
      "   [ 92  56  34 ...  92  56  34]\n",
      "   [ 87  51  30 ...  87  51  30]]\n",
      "\n",
      "  [[108  82  59 ... 108  82  59]\n",
      "   [115  91  70 ... 115  91  70]\n",
      "   [122 100  81 ... 122 100  81]\n",
      "   ...\n",
      "   [101  66  44 ... 101  66  44]\n",
      "   [ 96  60  38 ...  96  60  38]\n",
      "   [ 91  54  33 ...  91  54  33]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 87  89  90 ...  87  89  90]\n",
      "   [ 98 100 102 ...  98 100 102]\n",
      "   [112 115 118 ... 112 115 118]\n",
      "   ...\n",
      "   [242 187 163 ... 242 187 163]\n",
      "   [230 169 145 ... 230 169 145]\n",
      "   [209 149 124 ... 209 149 124]]\n",
      "\n",
      "  [[ 81  83  83 ...  81  83  83]\n",
      "   [ 87  89  91 ...  87  89  91]\n",
      "   [106 108 111 ... 106 108 111]\n",
      "   ...\n",
      "   [235 180 155 ... 235 180 155]\n",
      "   [224 167 141 ... 224 167 141]\n",
      "   [204 143 118 ... 204 143 118]]\n",
      "\n",
      "  [[ 78  80  80 ...  78  80  80]\n",
      "   [ 79  80  82 ...  79  80  82]\n",
      "   [ 94  96  99 ...  94  96  99]\n",
      "   ...\n",
      "   [229 174 149 ... 229 174 149]\n",
      "   [218 160 135 ... 218 160 135]\n",
      "   [197 137 113 ... 197 137 113]]]\n",
      "\n",
      "\n",
      " [[[198 203 206 ... 198 203 206]\n",
      "   [192 196 197 ... 192 196 197]\n",
      "   [154 154 149 ... 154 154 149]\n",
      "   ...\n",
      "   [201 208 206 ... 201 208 206]\n",
      "   [199 207 205 ... 199 207 205]\n",
      "   [192 202 201 ... 192 202 201]]\n",
      "\n",
      "  [[200 204 207 ... 200 204 207]\n",
      "   [184 187 188 ... 184 187 188]\n",
      "   [109 107 101 ... 109 107 101]\n",
      "   ...\n",
      "   [199 206 206 ... 199 206 206]\n",
      "   [201 211 209 ... 201 211 209]\n",
      "   [196 207 205 ... 196 207 205]]\n",
      "\n",
      "  [[207 211 215 ... 207 211 215]\n",
      "   [168 171 170 ... 168 171 170]\n",
      "   [ 62  58  50 ...  62  58  50]\n",
      "   ...\n",
      "   [193 200 200 ... 193 200 200]\n",
      "   [198 207 206 ... 198 207 206]\n",
      "   [198 209 207 ... 198 209 207]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[217 216 217 ... 217 216 217]\n",
      "   [217 214 212 ... 217 214 212]\n",
      "   [223 219 217 ... 223 219 217]\n",
      "   ...\n",
      "   [165 114 111 ... 165 114 111]\n",
      "   [174 125 121 ... 174 125 121]\n",
      "   [175 127 123 ... 175 127 123]]\n",
      "\n",
      "  [[218 217 217 ... 218 217 217]\n",
      "   [218 216 217 ... 218 216 217]\n",
      "   [222 220 218 ... 222 220 218]\n",
      "   ...\n",
      "   [163 113 109 ... 163 113 109]\n",
      "   [171 123 119 ... 171 123 119]\n",
      "   [171 124 119 ... 171 124 119]]\n",
      "\n",
      "  [[216 216 216 ... 216 216 216]\n",
      "   [219 218 217 ... 219 218 217]\n",
      "   [222 221 220 ... 222 221 220]\n",
      "   ...\n",
      "   [162 113 109 ... 162 113 109]\n",
      "   [167 119 115 ... 167 119 115]\n",
      "   [166 120 115 ... 166 120 115]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (72, 40, 30, 9)\n",
      "72 train samples\n",
      "29 test samples\n",
      "y_train shape: (72, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_496 (Conv2D)             (None, 40, 30, 16)   1312        input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_448 (BatchN (None, 40, 30, 16)   64          conv2d_496[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_448 (Activation)     (None, 40, 30, 16)   0           batch_normalization_448[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_497 (Conv2D)             (None, 40, 30, 16)   272         activation_448[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_449 (BatchN (None, 40, 30, 16)   64          conv2d_497[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_449 (Activation)     (None, 40, 30, 16)   0           batch_normalization_449[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_498 (Conv2D)             (None, 40, 30, 16)   2320        activation_449[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_450 (BatchN (None, 40, 30, 16)   64          conv2d_498[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_450 (Activation)     (None, 40, 30, 16)   0           batch_normalization_450[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_500 (Conv2D)             (None, 40, 30, 64)   1088        activation_448[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_499 (Conv2D)             (None, 40, 30, 64)   1088        activation_450[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_144 (Add)                   (None, 40, 30, 64)   0           conv2d_500[0][0]                 \n",
      "                                                                 conv2d_499[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_451 (BatchN (None, 40, 30, 64)   256         add_144[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_451 (Activation)     (None, 40, 30, 64)   0           batch_normalization_451[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_501 (Conv2D)             (None, 40, 30, 16)   1040        activation_451[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_452 (BatchN (None, 40, 30, 16)   64          conv2d_501[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_452 (Activation)     (None, 40, 30, 16)   0           batch_normalization_452[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_502 (Conv2D)             (None, 40, 30, 16)   2320        activation_452[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_453 (BatchN (None, 40, 30, 16)   64          conv2d_502[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_453 (Activation)     (None, 40, 30, 16)   0           batch_normalization_453[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_503 (Conv2D)             (None, 40, 30, 64)   1088        activation_453[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_145 (Add)                   (None, 40, 30, 64)   0           add_144[0][0]                    \n",
      "                                                                 conv2d_503[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_454 (BatchN (None, 40, 30, 64)   256         add_145[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_454 (Activation)     (None, 40, 30, 64)   0           batch_normalization_454[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_504 (Conv2D)             (None, 40, 30, 16)   1040        activation_454[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_455 (BatchN (None, 40, 30, 16)   64          conv2d_504[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_455 (Activation)     (None, 40, 30, 16)   0           batch_normalization_455[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_505 (Conv2D)             (None, 40, 30, 16)   2320        activation_455[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_456 (BatchN (None, 40, 30, 16)   64          conv2d_505[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_456 (Activation)     (None, 40, 30, 16)   0           batch_normalization_456[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_506 (Conv2D)             (None, 40, 30, 64)   1088        activation_456[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_146 (Add)                   (None, 40, 30, 64)   0           add_145[0][0]                    \n",
      "                                                                 conv2d_506[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_457 (BatchN (None, 40, 30, 64)   256         add_146[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_457 (Activation)     (None, 40, 30, 64)   0           batch_normalization_457[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_507 (Conv2D)             (None, 20, 15, 64)   4160        activation_457[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_458 (BatchN (None, 20, 15, 64)   256         conv2d_507[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_458 (Activation)     (None, 20, 15, 64)   0           batch_normalization_458[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_508 (Conv2D)             (None, 20, 15, 64)   36928       activation_458[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_459 (BatchN (None, 20, 15, 64)   256         conv2d_508[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_459 (Activation)     (None, 20, 15, 64)   0           batch_normalization_459[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_510 (Conv2D)             (None, 20, 15, 128)  8320        add_146[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_509 (Conv2D)             (None, 20, 15, 128)  8320        activation_459[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_147 (Add)                   (None, 20, 15, 128)  0           conv2d_510[0][0]                 \n",
      "                                                                 conv2d_509[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_460 (BatchN (None, 20, 15, 128)  512         add_147[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_460 (Activation)     (None, 20, 15, 128)  0           batch_normalization_460[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_511 (Conv2D)             (None, 20, 15, 64)   8256        activation_460[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_461 (BatchN (None, 20, 15, 64)   256         conv2d_511[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_461 (Activation)     (None, 20, 15, 64)   0           batch_normalization_461[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_512 (Conv2D)             (None, 20, 15, 64)   36928       activation_461[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_462 (BatchN (None, 20, 15, 64)   256         conv2d_512[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_462 (Activation)     (None, 20, 15, 64)   0           batch_normalization_462[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_513 (Conv2D)             (None, 20, 15, 128)  8320        activation_462[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_148 (Add)                   (None, 20, 15, 128)  0           add_147[0][0]                    \n",
      "                                                                 conv2d_513[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_463 (BatchN (None, 20, 15, 128)  512         add_148[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_463 (Activation)     (None, 20, 15, 128)  0           batch_normalization_463[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_514 (Conv2D)             (None, 20, 15, 64)   8256        activation_463[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_464 (BatchN (None, 20, 15, 64)   256         conv2d_514[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_464 (Activation)     (None, 20, 15, 64)   0           batch_normalization_464[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_515 (Conv2D)             (None, 20, 15, 64)   36928       activation_464[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_465 (BatchN (None, 20, 15, 64)   256         conv2d_515[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_465 (Activation)     (None, 20, 15, 64)   0           batch_normalization_465[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_516 (Conv2D)             (None, 20, 15, 128)  8320        activation_465[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_149 (Add)                   (None, 20, 15, 128)  0           add_148[0][0]                    \n",
      "                                                                 conv2d_516[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_466 (BatchN (None, 20, 15, 128)  512         add_149[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_466 (Activation)     (None, 20, 15, 128)  0           batch_normalization_466[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_517 (Conv2D)             (None, 10, 8, 128)   16512       activation_466[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_467 (BatchN (None, 10, 8, 128)   512         conv2d_517[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_467 (Activation)     (None, 10, 8, 128)   0           batch_normalization_467[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_518 (Conv2D)             (None, 10, 8, 128)   147584      activation_467[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_468 (BatchN (None, 10, 8, 128)   512         conv2d_518[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_468 (Activation)     (None, 10, 8, 128)   0           batch_normalization_468[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_520 (Conv2D)             (None, 10, 8, 256)   33024       add_149[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_519 (Conv2D)             (None, 10, 8, 256)   33024       activation_468[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_150 (Add)                   (None, 10, 8, 256)   0           conv2d_520[0][0]                 \n",
      "                                                                 conv2d_519[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_469 (BatchN (None, 10, 8, 256)   1024        add_150[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_469 (Activation)     (None, 10, 8, 256)   0           batch_normalization_469[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_521 (Conv2D)             (None, 10, 8, 128)   32896       activation_469[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_470 (BatchN (None, 10, 8, 128)   512         conv2d_521[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_470 (Activation)     (None, 10, 8, 128)   0           batch_normalization_470[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_522 (Conv2D)             (None, 10, 8, 128)   147584      activation_470[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_471 (BatchN (None, 10, 8, 128)   512         conv2d_522[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_471 (Activation)     (None, 10, 8, 128)   0           batch_normalization_471[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_523 (Conv2D)             (None, 10, 8, 256)   33024       activation_471[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_151 (Add)                   (None, 10, 8, 256)   0           add_150[0][0]                    \n",
      "                                                                 conv2d_523[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_472 (BatchN (None, 10, 8, 256)   1024        add_151[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_472 (Activation)     (None, 10, 8, 256)   0           batch_normalization_472[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_524 (Conv2D)             (None, 10, 8, 128)   32896       activation_472[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_473 (BatchN (None, 10, 8, 128)   512         conv2d_524[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_473 (Activation)     (None, 10, 8, 128)   0           batch_normalization_473[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_525 (Conv2D)             (None, 10, 8, 128)   147584      activation_473[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_474 (BatchN (None, 10, 8, 128)   512         conv2d_525[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_474 (Activation)     (None, 10, 8, 128)   0           batch_normalization_474[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_526 (Conv2D)             (None, 10, 8, 256)   33024       activation_474[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_152 (Add)                   (None, 10, 8, 256)   0           add_151[0][0]                    \n",
      "                                                                 conv2d_526[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_475 (BatchN (None, 10, 8, 256)   1024        add_152[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_475 (Activation)     (None, 10, 8, 256)   0           batch_normalization_475[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_16 (AveragePo (None, 1, 1, 256)    0           activation_475[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 256)          0           average_pooling2d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 10)           2570        flatten_16[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[0.29803923 0.32941177 0.37254903 ... 0.29803923 0.32941177\n",
      "    0.37254903]\n",
      "   [0.05882353 0.05882353 0.07843138 ... 0.05882353 0.05882353\n",
      "    0.07843138]\n",
      "   [0.03137255 0.02745098 0.04313726 ... 0.03137255 0.02745098\n",
      "    0.04313726]\n",
      "   ...\n",
      "   [0.0627451  0.05490196 0.09803922 ... 0.0627451  0.05490196\n",
      "    0.09803922]\n",
      "   [0.05098039 0.05098039 0.08627451 ... 0.05098039 0.05098039\n",
      "    0.08627451]\n",
      "   [0.03137255 0.03529412 0.05098039 ... 0.03137255 0.03529412\n",
      "    0.05098039]]\n",
      "\n",
      "  [[0.34509805 0.3764706  0.41960785 ... 0.34509805 0.3764706\n",
      "    0.41960785]\n",
      "   [0.06666667 0.06666667 0.08627451 ... 0.06666667 0.06666667\n",
      "    0.08627451]\n",
      "   [0.02352941 0.01960784 0.03529412 ... 0.02352941 0.01960784\n",
      "    0.03529412]\n",
      "   ...\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.05882353 0.05098039 0.09411765 ... 0.05882353 0.05098039\n",
      "    0.09411765]\n",
      "   [0.03921569 0.03921569 0.0627451  ... 0.03921569 0.03921569\n",
      "    0.0627451 ]]\n",
      "\n",
      "  [[0.3372549  0.35686275 0.38431373 ... 0.3372549  0.35686275\n",
      "    0.38431373]\n",
      "   [0.05490196 0.05098039 0.06666667 ... 0.05490196 0.05098039\n",
      "    0.06666667]\n",
      "   [0.01960784 0.01568628 0.02745098 ... 0.01960784 0.01568628\n",
      "    0.02745098]\n",
      "   ...\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.04705882 0.04313726 0.07058824 ... 0.04705882 0.04313726\n",
      "    0.07058824]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6156863  0.63529414 0.68235296 ... 0.6156863  0.63529414\n",
      "    0.68235296]\n",
      "   [0.62352943 0.64705884 0.6901961  ... 0.62352943 0.64705884\n",
      "    0.6901961 ]\n",
      "   [0.60784316 0.61960787 0.62352943 ... 0.60784316 0.61960787\n",
      "    0.62352943]\n",
      "   ...\n",
      "   [0.49411765 0.53333336 0.65882355 ... 0.49411765 0.53333336\n",
      "    0.65882355]\n",
      "   [0.5686275  0.6        0.69803923 ... 0.5686275  0.6\n",
      "    0.69803923]\n",
      "   [0.5882353  0.6156863  0.7058824  ... 0.5882353  0.6156863\n",
      "    0.7058824 ]]\n",
      "\n",
      "  [[0.61960787 0.63529414 0.68235296 ... 0.61960787 0.63529414\n",
      "    0.68235296]\n",
      "   [0.61960787 0.6431373  0.69411767 ... 0.61960787 0.6431373\n",
      "    0.69411767]\n",
      "   [0.627451   0.654902   0.7019608  ... 0.627451   0.654902\n",
      "    0.7019608 ]\n",
      "   ...\n",
      "   [0.5254902  0.5647059  0.6745098  ... 0.5254902  0.5647059\n",
      "    0.6745098 ]\n",
      "   [0.56078434 0.5921569  0.69411767 ... 0.56078434 0.5921569\n",
      "    0.69411767]\n",
      "   [0.5921569  0.61960787 0.7058824  ... 0.5921569  0.61960787\n",
      "    0.7058824 ]]\n",
      "\n",
      "  [[0.627451   0.6431373  0.6901961  ... 0.627451   0.6431373\n",
      "    0.6901961 ]\n",
      "   [0.61960787 0.6431373  0.69411767 ... 0.61960787 0.6431373\n",
      "    0.69411767]\n",
      "   [0.6392157  0.6627451  0.70980394 ... 0.6392157  0.6627451\n",
      "    0.70980394]\n",
      "   ...\n",
      "   [0.5529412  0.5882353  0.68235296 ... 0.5529412  0.5882353\n",
      "    0.68235296]\n",
      "   [0.54901963 0.58431375 0.6901961  ... 0.54901963 0.58431375\n",
      "    0.6901961 ]\n",
      "   [0.5882353  0.61960787 0.7058824  ... 0.5882353  0.61960787\n",
      "    0.7058824 ]]]\n",
      "\n",
      "\n",
      " [[[0.3647059  0.2509804  0.16078432 ... 0.3647059  0.2509804\n",
      "    0.16078432]\n",
      "   [0.39607844 0.28627452 0.19607843 ... 0.39607844 0.28627452\n",
      "    0.19607843]\n",
      "   [0.42745098 0.3254902  0.23921569 ... 0.42745098 0.3254902\n",
      "    0.23921569]\n",
      "   ...\n",
      "   [0.36078432 0.22352941 0.13725491 ... 0.36078432 0.22352941\n",
      "    0.13725491]\n",
      "   [0.34509805 0.20392157 0.11764706 ... 0.34509805 0.20392157\n",
      "    0.11764706]\n",
      "   [0.3254902  0.18431373 0.10196079 ... 0.3254902  0.18431373\n",
      "    0.10196079]]\n",
      "\n",
      "  [[0.39607844 0.28627452 0.19607843 ... 0.39607844 0.28627452\n",
      "    0.19607843]\n",
      "   [0.42745098 0.32156864 0.23529412 ... 0.42745098 0.32156864\n",
      "    0.23529412]\n",
      "   [0.45490196 0.36078432 0.27450982 ... 0.45490196 0.36078432\n",
      "    0.27450982]\n",
      "   ...\n",
      "   [0.38039216 0.24313726 0.15294118 ... 0.38039216 0.24313726\n",
      "    0.15294118]\n",
      "   [0.36078432 0.21960784 0.13333334 ... 0.36078432 0.21960784\n",
      "    0.13333334]\n",
      "   [0.34117648 0.2        0.11764706 ... 0.34117648 0.2\n",
      "    0.11764706]]\n",
      "\n",
      "  [[0.42352942 0.32156864 0.23137255 ... 0.42352942 0.32156864\n",
      "    0.23137255]\n",
      "   [0.4509804  0.35686275 0.27450982 ... 0.4509804  0.35686275\n",
      "    0.27450982]\n",
      "   [0.47843137 0.39215687 0.31764707 ... 0.47843137 0.39215687\n",
      "    0.31764707]\n",
      "   ...\n",
      "   [0.39607844 0.25882354 0.17254902 ... 0.39607844 0.25882354\n",
      "    0.17254902]\n",
      "   [0.3764706  0.23529412 0.14901961 ... 0.3764706  0.23529412\n",
      "    0.14901961]\n",
      "   [0.35686275 0.21176471 0.12941177 ... 0.35686275 0.21176471\n",
      "    0.12941177]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.34117648 0.34901962 0.3529412  ... 0.34117648 0.34901962\n",
      "    0.3529412 ]\n",
      "   [0.38431373 0.39215687 0.4        ... 0.38431373 0.39215687\n",
      "    0.4       ]\n",
      "   [0.4392157  0.4509804  0.4627451  ... 0.4392157  0.4509804\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.9490196  0.73333335 0.6392157  ... 0.9490196  0.73333335\n",
      "    0.6392157 ]\n",
      "   [0.9019608  0.6627451  0.5686275  ... 0.9019608  0.6627451\n",
      "    0.5686275 ]\n",
      "   [0.81960785 0.58431375 0.4862745  ... 0.81960785 0.58431375\n",
      "    0.4862745 ]]\n",
      "\n",
      "  [[0.31764707 0.3254902  0.3254902  ... 0.31764707 0.3254902\n",
      "    0.3254902 ]\n",
      "   [0.34117648 0.34901962 0.35686275 ... 0.34117648 0.34901962\n",
      "    0.35686275]\n",
      "   [0.41568628 0.42352942 0.43529412 ... 0.41568628 0.42352942\n",
      "    0.43529412]\n",
      "   ...\n",
      "   [0.92156863 0.7058824  0.60784316 ... 0.92156863 0.7058824\n",
      "    0.60784316]\n",
      "   [0.8784314  0.654902   0.5529412  ... 0.8784314  0.654902\n",
      "    0.5529412 ]\n",
      "   [0.8        0.56078434 0.4627451  ... 0.8        0.56078434\n",
      "    0.4627451 ]]\n",
      "\n",
      "  [[0.30588236 0.3137255  0.3137255  ... 0.30588236 0.3137255\n",
      "    0.3137255 ]\n",
      "   [0.30980393 0.3137255  0.32156864 ... 0.30980393 0.3137255\n",
      "    0.32156864]\n",
      "   [0.36862746 0.3764706  0.3882353  ... 0.36862746 0.3764706\n",
      "    0.3882353 ]\n",
      "   ...\n",
      "   [0.8980392  0.68235296 0.58431375 ... 0.8980392  0.68235296\n",
      "    0.58431375]\n",
      "   [0.85490197 0.627451   0.5294118  ... 0.85490197 0.627451\n",
      "    0.5294118 ]\n",
      "   [0.77254903 0.5372549  0.44313726 ... 0.77254903 0.5372549\n",
      "    0.44313726]]]\n",
      "\n",
      "\n",
      " [[[0.7764706  0.79607844 0.80784315 ... 0.7764706  0.79607844\n",
      "    0.80784315]\n",
      "   [0.7529412  0.76862746 0.77254903 ... 0.7529412  0.76862746\n",
      "    0.77254903]\n",
      "   [0.6039216  0.6039216  0.58431375 ... 0.6039216  0.6039216\n",
      "    0.58431375]\n",
      "   ...\n",
      "   [0.7882353  0.8156863  0.80784315 ... 0.7882353  0.8156863\n",
      "    0.80784315]\n",
      "   [0.78039217 0.8117647  0.8039216  ... 0.78039217 0.8117647\n",
      "    0.8039216 ]\n",
      "   [0.7529412  0.7921569  0.7882353  ... 0.7529412  0.7921569\n",
      "    0.7882353 ]]\n",
      "\n",
      "  [[0.78431374 0.8        0.8117647  ... 0.78431374 0.8\n",
      "    0.8117647 ]\n",
      "   [0.72156864 0.73333335 0.7372549  ... 0.72156864 0.73333335\n",
      "    0.7372549 ]\n",
      "   [0.42745098 0.41960785 0.39607844 ... 0.42745098 0.41960785\n",
      "    0.39607844]\n",
      "   ...\n",
      "   [0.78039217 0.80784315 0.80784315 ... 0.78039217 0.80784315\n",
      "    0.80784315]\n",
      "   [0.7882353  0.827451   0.81960785 ... 0.7882353  0.827451\n",
      "    0.81960785]\n",
      "   [0.76862746 0.8117647  0.8039216  ... 0.76862746 0.8117647\n",
      "    0.8039216 ]]\n",
      "\n",
      "  [[0.8117647  0.827451   0.84313726 ... 0.8117647  0.827451\n",
      "    0.84313726]\n",
      "   [0.65882355 0.67058825 0.6666667  ... 0.65882355 0.67058825\n",
      "    0.6666667 ]\n",
      "   [0.24313726 0.22745098 0.19607843 ... 0.24313726 0.22745098\n",
      "    0.19607843]\n",
      "   ...\n",
      "   [0.75686276 0.78431374 0.78431374 ... 0.75686276 0.78431374\n",
      "    0.78431374]\n",
      "   [0.7764706  0.8117647  0.80784315 ... 0.7764706  0.8117647\n",
      "    0.80784315]\n",
      "   [0.7764706  0.81960785 0.8117647  ... 0.7764706  0.81960785\n",
      "    0.8117647 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8509804  0.84705883 0.8509804  ... 0.8509804  0.84705883\n",
      "    0.8509804 ]\n",
      "   [0.8509804  0.8392157  0.83137256 ... 0.8509804  0.8392157\n",
      "    0.83137256]\n",
      "   [0.8745098  0.85882354 0.8509804  ... 0.8745098  0.85882354\n",
      "    0.8509804 ]\n",
      "   ...\n",
      "   [0.64705884 0.44705883 0.43529412 ... 0.64705884 0.44705883\n",
      "    0.43529412]\n",
      "   [0.68235296 0.49019608 0.4745098  ... 0.68235296 0.49019608\n",
      "    0.4745098 ]\n",
      "   [0.6862745  0.49803922 0.48235294 ... 0.6862745  0.49803922\n",
      "    0.48235294]]\n",
      "\n",
      "  [[0.85490197 0.8509804  0.8509804  ... 0.85490197 0.8509804\n",
      "    0.8509804 ]\n",
      "   [0.85490197 0.84705883 0.8509804  ... 0.85490197 0.84705883\n",
      "    0.8509804 ]\n",
      "   [0.87058824 0.8627451  0.85490197 ... 0.87058824 0.8627451\n",
      "    0.85490197]\n",
      "   ...\n",
      "   [0.6392157  0.44313726 0.42745098 ... 0.6392157  0.44313726\n",
      "    0.42745098]\n",
      "   [0.67058825 0.48235294 0.46666667 ... 0.67058825 0.48235294\n",
      "    0.46666667]\n",
      "   [0.67058825 0.4862745  0.46666667 ... 0.67058825 0.4862745\n",
      "    0.46666667]]\n",
      "\n",
      "  [[0.84705883 0.84705883 0.84705883 ... 0.84705883 0.84705883\n",
      "    0.84705883]\n",
      "   [0.85882354 0.85490197 0.8509804  ... 0.85882354 0.85490197\n",
      "    0.8509804 ]\n",
      "   [0.87058824 0.8666667  0.8627451  ... 0.87058824 0.8666667\n",
      "    0.8627451 ]\n",
      "   ...\n",
      "   [0.63529414 0.44313726 0.42745098 ... 0.63529414 0.44313726\n",
      "    0.42745098]\n",
      "   [0.654902   0.46666667 0.4509804  ... 0.654902   0.46666667\n",
      "    0.4509804 ]\n",
      "   [0.6509804  0.47058824 0.4509804  ... 0.6509804  0.47058824\n",
      "    0.4509804 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.49803922 0.49803922 0.5176471  ... 0.49803922 0.49803922\n",
      "    0.5176471 ]\n",
      "   [0.5058824  0.5019608  0.52156866 ... 0.5058824  0.5019608\n",
      "    0.52156866]\n",
      "   [0.50980395 0.5058824  0.5254902  ... 0.50980395 0.5058824\n",
      "    0.5254902 ]\n",
      "   ...\n",
      "   [0.5686275  0.57254905 0.5882353  ... 0.5686275  0.57254905\n",
      "    0.5882353 ]\n",
      "   [0.57254905 0.5764706  0.5921569  ... 0.57254905 0.5764706\n",
      "    0.5921569 ]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]]\n",
      "\n",
      "  [[0.50980395 0.50980395 0.5254902  ... 0.50980395 0.50980395\n",
      "    0.5254902 ]\n",
      "   [0.5137255  0.5137255  0.5294118  ... 0.5137255  0.5137255\n",
      "    0.5294118 ]\n",
      "   [0.5176471  0.5176471  0.53333336 ... 0.5176471  0.5176471\n",
      "    0.53333336]\n",
      "   ...\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5803922  0.58431375 0.6        ... 0.5803922  0.58431375\n",
      "    0.6       ]]\n",
      "\n",
      "  [[0.5176471  0.5176471  0.5372549  ... 0.5176471  0.5176471\n",
      "    0.5372549 ]\n",
      "   [0.52156866 0.5254902  0.5411765  ... 0.52156866 0.5254902\n",
      "    0.5411765 ]\n",
      "   [0.5176471  0.52156866 0.5372549  ... 0.5176471  0.52156866\n",
      "    0.5372549 ]\n",
      "   ...\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.5882353  0.5921569  0.60784316 ... 0.5882353  0.5921569\n",
      "    0.60784316]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8901961  0.8392157  0.8039216  ... 0.8901961  0.8392157\n",
      "    0.8039216 ]\n",
      "   [0.91764706 0.8745098  0.8392157  ... 0.91764706 0.8745098\n",
      "    0.8392157 ]\n",
      "   [0.8627451  0.79607844 0.7490196  ... 0.8627451  0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9372549  0.90588236 0.8784314  ... 0.9372549  0.90588236\n",
      "    0.8784314 ]\n",
      "   [0.90588236 0.85882354 0.8235294  ... 0.90588236 0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.87058824 0.8117647  0.7764706  ... 0.87058824 0.8117647\n",
      "    0.7764706 ]]\n",
      "\n",
      "  [[0.88235295 0.8235294  0.78431374 ... 0.88235295 0.8235294\n",
      "    0.78431374]\n",
      "   [0.89411765 0.8392157  0.8        ... 0.89411765 0.8392157\n",
      "    0.8       ]\n",
      "   [0.87058824 0.8039216  0.7607843  ... 0.87058824 0.8039216\n",
      "    0.7607843 ]\n",
      "   ...\n",
      "   [0.92156863 0.8862745  0.8509804  ... 0.92156863 0.8862745\n",
      "    0.8509804 ]\n",
      "   [0.9098039  0.87058824 0.8352941  ... 0.9098039  0.87058824\n",
      "    0.8352941 ]\n",
      "   [0.8745098  0.8156863  0.77254903 ... 0.8745098  0.8156863\n",
      "    0.77254903]]\n",
      "\n",
      "  [[0.8627451  0.7921569  0.7411765  ... 0.8627451  0.7921569\n",
      "    0.7411765 ]\n",
      "   [0.8666667  0.8039216  0.7529412  ... 0.8666667  0.8039216\n",
      "    0.7529412 ]\n",
      "   [0.85882354 0.79607844 0.7490196  ... 0.85882354 0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9098039  0.85882354 0.8235294  ... 0.9098039  0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.90588236 0.85490197 0.8156863  ... 0.90588236 0.85490197\n",
      "    0.8156863 ]\n",
      "   [0.8784314  0.8156863  0.7647059  ... 0.8784314  0.8156863\n",
      "    0.7647059 ]]]\n",
      "\n",
      "\n",
      " [[[0.65882355 0.5411765  0.34901962 ... 0.65882355 0.5411765\n",
      "    0.34901962]\n",
      "   [0.79607844 0.6862745  0.44313726 ... 0.79607844 0.6862745\n",
      "    0.44313726]\n",
      "   [0.84705883 0.7372549  0.4862745  ... 0.84705883 0.7372549\n",
      "    0.4862745 ]\n",
      "   ...\n",
      "   [0.9019608  0.78431374 0.57254905 ... 0.9019608  0.78431374\n",
      "    0.57254905]\n",
      "   [0.8784314  0.77254903 0.5803922  ... 0.8784314  0.77254903\n",
      "    0.5803922 ]\n",
      "   [0.8666667  0.7607843  0.58431375 ... 0.8666667  0.7607843\n",
      "    0.58431375]]\n",
      "\n",
      "  [[0.6313726  0.5058824  0.27450982 ... 0.6313726  0.5058824\n",
      "    0.27450982]\n",
      "   [0.7372549  0.61960787 0.3764706  ... 0.7372549  0.61960787\n",
      "    0.3764706 ]\n",
      "   [0.83137256 0.72156864 0.4627451  ... 0.83137256 0.72156864\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.8784314  0.7607843  0.5568628  ... 0.8784314  0.7607843\n",
      "    0.5568628 ]\n",
      "   [0.85882354 0.7490196  0.56078434 ... 0.85882354 0.7490196\n",
      "    0.56078434]\n",
      "   [0.85490197 0.7529412  0.5647059  ... 0.85490197 0.7529412\n",
      "    0.5647059 ]]\n",
      "\n",
      "  [[0.6        0.46666667 0.22352941 ... 0.6        0.46666667\n",
      "    0.22352941]\n",
      "   [0.6313726  0.49803922 0.27058825 ... 0.6313726  0.49803922\n",
      "    0.27058825]\n",
      "   [0.80784315 0.69803923 0.44705883 ... 0.80784315 0.69803923\n",
      "    0.44705883]\n",
      "   ...\n",
      "   [0.8352941  0.72156864 0.5372549  ... 0.8352941  0.72156864\n",
      "    0.5372549 ]\n",
      "   [0.83137256 0.7254902  0.5411765  ... 0.83137256 0.7254902\n",
      "    0.5411765 ]\n",
      "   [0.8352941  0.73333335 0.5372549  ... 0.8352941  0.73333335\n",
      "    0.5372549 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6039216  0.3764706  0.27058825 ... 0.6039216  0.3764706\n",
      "    0.27058825]\n",
      "   [0.627451   0.40784314 0.31764707 ... 0.627451   0.40784314\n",
      "    0.31764707]\n",
      "   [0.67058825 0.47058824 0.4        ... 0.67058825 0.47058824\n",
      "    0.4       ]\n",
      "   ...\n",
      "   [0.76862746 0.58431375 0.4745098  ... 0.76862746 0.58431375\n",
      "    0.4745098 ]\n",
      "   [0.7058824  0.49803922 0.38431373 ... 0.7058824  0.49803922\n",
      "    0.38431373]\n",
      "   [0.65882355 0.44313726 0.32156864 ... 0.65882355 0.44313726\n",
      "    0.32156864]]\n",
      "\n",
      "  [[0.6        0.38039216 0.26666668 ... 0.6        0.38039216\n",
      "    0.26666668]\n",
      "   [0.63529414 0.41568628 0.32156864 ... 0.63529414 0.41568628\n",
      "    0.32156864]\n",
      "   [0.7372549  0.56078434 0.4627451  ... 0.7372549  0.56078434\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.83137256 0.654902   0.54901963 ... 0.83137256 0.654902\n",
      "    0.54901963]\n",
      "   [0.77254903 0.5647059  0.44313726 ... 0.77254903 0.5647059\n",
      "    0.44313726]\n",
      "   [0.7176471  0.5019608  0.38039216 ... 0.7176471  0.5019608\n",
      "    0.38039216]]\n",
      "\n",
      "  [[0.627451   0.41960785 0.29411766 ... 0.627451   0.41960785\n",
      "    0.29411766]\n",
      "   [0.68235296 0.46666667 0.3882353  ... 0.68235296 0.46666667\n",
      "    0.3882353 ]\n",
      "   [0.8509804  0.7294118  0.49019608 ... 0.8509804  0.7294118\n",
      "    0.49019608]\n",
      "   ...\n",
      "   [0.89411765 0.72156864 0.6117647  ... 0.89411765 0.72156864\n",
      "    0.6117647 ]\n",
      "   [0.8392157  0.627451   0.50980395 ... 0.8392157  0.627451\n",
      "    0.50980395]\n",
      "   [0.76862746 0.56078434 0.43529412 ... 0.76862746 0.56078434\n",
      "    0.43529412]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.99215686 0.99215686 0.99215686 ... 0.99215686 0.99215686\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6509804  0.6117647  0.60784316 ... 0.6509804  0.6117647\n",
      "    0.60784316]\n",
      "   [0.6431373  0.6        0.5921569  ... 0.6431373  0.6\n",
      "    0.5921569 ]\n",
      "   [0.63529414 0.5921569  0.5803922  ... 0.63529414 0.5921569\n",
      "    0.5803922 ]\n",
      "   ...\n",
      "   [0.80784315 0.69803923 0.67058825 ... 0.80784315 0.69803923\n",
      "    0.67058825]\n",
      "   [0.80784315 0.7176471  0.6901961  ... 0.80784315 0.7176471\n",
      "    0.6901961 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]]\n",
      "\n",
      "  [[0.64705884 0.59607846 0.59607846 ... 0.64705884 0.59607846\n",
      "    0.59607846]\n",
      "   [0.63529414 0.5921569  0.58431375 ... 0.63529414 0.5921569\n",
      "    0.58431375]\n",
      "   [0.62352943 0.5803922  0.57254905 ... 0.62352943 0.5803922\n",
      "    0.57254905]\n",
      "   ...\n",
      "   [0.8        0.69803923 0.6745098  ... 0.8        0.69803923\n",
      "    0.6745098 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]\n",
      "   [0.7921569  0.7019608  0.68235296 ... 0.7921569  0.7019608\n",
      "    0.68235296]]\n",
      "\n",
      "  [[0.6431373  0.59607846 0.5921569  ... 0.6431373  0.59607846\n",
      "    0.5921569 ]\n",
      "   [0.6313726  0.5882353  0.5803922  ... 0.6313726  0.5882353\n",
      "    0.5803922 ]\n",
      "   [0.627451   0.57254905 0.5686275  ... 0.627451   0.57254905\n",
      "    0.5686275 ]\n",
      "   ...\n",
      "   [0.7921569  0.6901961  0.6784314  ... 0.7921569  0.6901961\n",
      "    0.6784314 ]\n",
      "   [0.80784315 0.7058824  0.6901961  ... 0.80784315 0.7058824\n",
      "    0.6901961 ]\n",
      "   [0.80784315 0.7176471  0.7019608  ... 0.80784315 0.7176471\n",
      "    0.7019608 ]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (72, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (72, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 8s 2s/step - loss: 2.3974 - accuracy: 0.5278 - val_loss: 2.8400 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/100\n",
      "Learning rate:  1e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.3965 - accuracy: 0.4722 - val_loss: 2.7963 - val_accuracy: 0.9655\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/100\n",
      "Learning rate:  2e-05\n",
      "2/2 [==============================] - 2s 443ms/step - loss: 2.3861 - accuracy: 0.5000 - val_loss: 2.7647 - val_accuracy: 0.0345\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/100\n",
      "Learning rate:  3e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.3708 - accuracy: 0.5278 - val_loss: 2.7370 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/100\n",
      "Learning rate:  4e-05\n",
      "2/2 [==============================] - 2s 458ms/step - loss: 2.3567 - accuracy: 0.5000 - val_loss: 2.7116 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/100\n",
      "Learning rate:  5e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.2951 - accuracy: 0.6111 - val_loss: 2.6808 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/100\n",
      "Learning rate:  6e-05\n",
      "2/2 [==============================] - 3s 2s/step - loss: 2.2523 - accuracy: 0.7083 - val_loss: 2.6487 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/100\n",
      "Learning rate:  7.000000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.2265 - accuracy: 0.7639 - val_loss: 2.6094 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/100\n",
      "Learning rate:  8e-05\n",
      "2/2 [==============================] - 2s 454ms/step - loss: 2.1935 - accuracy: 0.7917 - val_loss: 2.5748 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/100\n",
      "Learning rate:  8.999999999999999e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.1251 - accuracy: 0.8056 - val_loss: 2.5323 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/100\n",
      "Learning rate:  0.0001\n",
      "2/2 [==============================] - 2s 471ms/step - loss: 2.0738 - accuracy: 0.8472 - val_loss: 2.4841 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/100\n",
      "Learning rate:  0.00011\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.0157 - accuracy: 0.8333 - val_loss: 2.4365 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/100\n",
      "Learning rate:  0.00012\n",
      "2/2 [==============================] - 2s 446ms/step - loss: 1.9765 - accuracy: 0.8472 - val_loss: 2.3849 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/100\n",
      "Learning rate:  0.00013000000000000002\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.8779 - accuracy: 0.7917 - val_loss: 2.3122 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/100\n",
      "Learning rate:  0.00014000000000000001\n",
      "2/2 [==============================] - 2s 441ms/step - loss: 1.8533 - accuracy: 0.8333 - val_loss: 2.2337 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/100\n",
      "Learning rate:  0.00015\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.7468 - accuracy: 0.8472 - val_loss: 2.1657 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/100\n",
      "Learning rate:  0.00016\n",
      "2/2 [==============================] - 3s 2s/step - loss: 1.6735 - accuracy: 0.8194 - val_loss: 2.0756 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/100\n",
      "Learning rate:  0.00017\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.6231 - accuracy: 0.8611 - val_loss: 1.9759 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/100\n",
      "Learning rate:  0.00017999999999999998\n",
      "2/2 [==============================] - 3s 751ms/step - loss: 1.5821 - accuracy: 0.8333 - val_loss: 1.8619 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/100\n",
      "Learning rate:  0.00019\n",
      "2/2 [==============================] - 3s 2s/step - loss: 1.5187 - accuracy: 0.8472 - val_loss: 1.7390 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 21/100\n",
      "Learning rate:  0.0002\n",
      "2/2 [==============================] - 4s 4s/step - loss: 1.4634 - accuracy: 0.8472 - val_loss: 1.6599 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 22/100\n",
      "Learning rate:  0.00021\n",
      "2/2 [==============================] - 3s 3s/step - loss: 1.4097 - accuracy: 0.8472 - val_loss: 1.5798 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 23/100\n",
      "Learning rate:  0.00022\n",
      "2/2 [==============================] - 3s 2s/step - loss: 1.3715 - accuracy: 0.8194 - val_loss: 1.5047 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 24/100\n",
      "Learning rate:  0.00023\n",
      "2/2 [==============================] - 2s 518ms/step - loss: 1.3813 - accuracy: 0.8750 - val_loss: 1.4494 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 25/100\n",
      "Learning rate:  0.00024\n",
      "2/2 [==============================] - 2s 462ms/step - loss: 1.2718 - accuracy: 0.9028 - val_loss: 1.3718 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 26/100\n",
      "Learning rate:  0.00025\n",
      "2/2 [==============================] - 2s 436ms/step - loss: 1.2447 - accuracy: 0.8750 - val_loss: 1.3260 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 27/100\n",
      "Learning rate:  0.00026000000000000003\n",
      "2/2 [==============================] - 2s 456ms/step - loss: 1.1670 - accuracy: 0.9028 - val_loss: 1.2942 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 28/100\n",
      "Learning rate:  0.00027\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1916 - accuracy: 0.86 - 2s 2s/step - loss: 1.1916 - accuracy: 0.8611 - val_loss: 1.2445 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 29/100\n",
      "Learning rate:  0.00028000000000000003\n",
      "2/2 [==============================] - 2s 493ms/step - loss: 1.1488 - accuracy: 0.8889 - val_loss: 1.2282 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 30/100\n",
      "Learning rate:  0.00029\n",
      "2/2 [==============================] - 2s 466ms/step - loss: 1.1267 - accuracy: 0.8472 - val_loss: 1.2395 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 31/100\n",
      "Learning rate:  0.0003\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0722 - accuracy: 0.8889 - val_loss: 1.2188 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 32/100\n",
      "Learning rate:  0.00031\n",
      "2/2 [==============================] - 2s 424ms/step - loss: 1.0622 - accuracy: 0.8750 - val_loss: 1.1789 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 33/100\n",
      "Learning rate:  0.00032\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0345 - accuracy: 0.8611 - val_loss: 1.1527 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 34/100\n",
      "Learning rate:  0.00033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 2s/step - loss: 1.0241 - accuracy: 0.9028 - val_loss: 1.1495 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 35/100\n",
      "Learning rate:  0.00034\n",
      "2/2 [==============================] - 3s 615ms/step - loss: 1.0402 - accuracy: 0.8611 - val_loss: 1.1374 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 36/100\n",
      "Learning rate:  0.00035\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.9985 - accuracy: 0.9028 - val_loss: 1.1333 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 37/100\n",
      "Learning rate:  0.00035999999999999997\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9860 - accuracy: 0.8889 - val_loss: 1.0902 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 38/100\n",
      "Learning rate:  0.00037\n",
      "2/2 [==============================] - 2s 441ms/step - loss: 0.9401 - accuracy: 0.8750 - val_loss: 1.0147 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 39/100\n",
      "Learning rate:  0.00038\n",
      "2/2 [==============================] - 2s 467ms/step - loss: 0.9752 - accuracy: 0.8750 - val_loss: 0.9139 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 40/100\n",
      "Learning rate:  0.00039000000000000005\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9248 - accuracy: 0.9306 - val_loss: 0.8564 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 41/100\n",
      "Learning rate:  0.0004\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8969 - accuracy: 0.9444 - val_loss: 0.8442 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 42/100\n",
      "Learning rate:  0.00041\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8890 - accuracy: 0.9167 - val_loss: 0.8112 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 43/100\n",
      "Learning rate:  0.00042\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8518 - accuracy: 0.9444 - val_loss: 0.7901 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 44/100\n",
      "Learning rate:  0.00043\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8583 - accuracy: 0.9306 - val_loss: 0.8004 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 45/100\n",
      "Learning rate:  0.00044\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.8361 - accuracy: 0.9306 - val_loss: 0.8170 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 46/100\n",
      "Learning rate:  0.00045000000000000004\n",
      "2/2 [==============================] - 2s 519ms/step - loss: 0.8465 - accuracy: 0.9306 - val_loss: 0.9137 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 47/100\n",
      "Learning rate:  0.00046\n",
      "2/2 [==============================] - 2s 479ms/step - loss: 0.7844 - accuracy: 0.9583 - val_loss: 1.0911 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 48/100\n",
      "Learning rate:  0.00047\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8191 - accuracy: 0.9167 - val_loss: 1.3378 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 49/100\n",
      "Learning rate:  0.00048\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8105 - accuracy: 0.9583 - val_loss: 1.4739 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 50/100\n",
      "Learning rate:  0.00049\n",
      "2/2 [==============================] - 2s 503ms/step - loss: 0.7784 - accuracy: 0.9583 - val_loss: 1.4311 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 51/100\n",
      "Learning rate:  0.0005\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.7710 - accuracy: 0.9722 - val_loss: 1.2799 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 52/100\n",
      "Learning rate:  0.00051\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7481 - accuracy: 0.9722 - val_loss: 1.1320 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 53/100\n",
      "Learning rate:  0.0005200000000000001\n",
      "2/2 [==============================] - 2s 446ms/step - loss: 0.7583 - accuracy: 0.9722 - val_loss: 1.0600 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 54/100\n",
      "Learning rate:  0.0005300000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7365 - accuracy: 0.9583 - val_loss: 1.0743 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 55/100\n",
      "Learning rate:  0.00054\n",
      "2/2 [==============================] - 2s 491ms/step - loss: 0.7271 - accuracy: 0.9722 - val_loss: 1.0416 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 56/100\n",
      "Learning rate:  0.00055\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7552 - accuracy: 0.9722 - val_loss: 0.8918 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 57/100\n",
      "Learning rate:  0.0005600000000000001\n",
      "2/2 [==============================] - 2s 494ms/step - loss: 0.7271 - accuracy: 0.9722 - val_loss: 0.8493 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 58/100\n",
      "Learning rate:  0.00057\n",
      "2/2 [==============================] - 2s 439ms/step - loss: 0.7617 - accuracy: 0.9583 - val_loss: 0.8870 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 59/100\n",
      "Learning rate:  0.00058\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.7611 - accuracy: 0.9444 - val_loss: 0.9695 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 60/100\n",
      "Learning rate:  0.00059\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6989 - accuracy: 0.9583 - val_loss: 1.0673 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 61/100\n",
      "Learning rate:  0.0006\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7309 - accuracy: 0.9583 - val_loss: 1.1429 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 62/100\n",
      "Learning rate:  0.00061\n",
      "2/2 [==============================] - 2s 570ms/step - loss: 0.7433 - accuracy: 0.9583 - val_loss: 1.2576 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 63/100\n",
      "Learning rate:  0.00062\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6962 - accuracy: 0.9583 - val_loss: 1.2097 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 64/100\n",
      "Learning rate:  0.00063\n",
      "2/2 [==============================] - 3s 475ms/step - loss: 0.7540 - accuracy: 0.9444 - val_loss: 1.2976 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 65/100\n",
      "Learning rate:  0.00064\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7130 - accuracy: 0.9583 - val_loss: 1.0340 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 66/100\n",
      "Learning rate:  0.0006500000000000001\n",
      "2/2 [==============================] - 2s 463ms/step - loss: 0.7311 - accuracy: 0.9722 - val_loss: 0.8663 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 67/100\n",
      "Learning rate:  0.00066\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7395 - accuracy: 0.9722 - val_loss: 0.6882 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 68/100\n",
      "Learning rate:  0.00067\n",
      "2/2 [==============================] - 2s 459ms/step - loss: 0.7853 - accuracy: 0.9444 - val_loss: 0.6503 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 69/100\n",
      "Learning rate:  0.00068\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7230 - accuracy: 0.9861 - val_loss: 0.9021 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 70/100\n",
      "Learning rate:  0.00069\n",
      "2/2 [==============================] - 2s 489ms/step - loss: 0.6889 - accuracy: 0.9722 - val_loss: 1.1876 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 71/100\n",
      "Learning rate:  0.0007\n",
      "2/2 [==============================] - 2s 461ms/step - loss: 0.6849 - accuracy: 0.9722 - val_loss: 1.3660 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 72/100\n",
      "Learning rate:  0.00071\n",
      "2/2 [==============================] - 2s 454ms/step - loss: 0.6885 - accuracy: 0.9722 - val_loss: 1.6135 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 73/100\n",
      "Learning rate:  0.0007199999999999999\n",
      "2/2 [==============================] - 2s 495ms/step - loss: 0.6893 - accuracy: 1.0000 - val_loss: 1.9467 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 74/100\n",
      "Learning rate:  0.00073\n",
      "2/2 [==============================] - 2s 444ms/step - loss: 0.7023 - accuracy: 0.9583 - val_loss: 1.8266 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 75/100\n",
      "Learning rate:  0.00074\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6975 - accuracy: 0.9722 - val_loss: 1.4522 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 76/100\n",
      "Learning rate:  0.00075\n",
      "2/2 [==============================] - 2s 502ms/step - loss: 0.7006 - accuracy: 0.9722 - val_loss: 1.0977 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 77/100\n",
      "Learning rate:  0.00076\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6649 - accuracy: 0.9722 - val_loss: 0.8804 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 78/100\n",
      "Learning rate:  0.0007700000000000001\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.7403 - accuracy: 0.9306 - val_loss: 0.8175 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 79/100\n",
      "Learning rate:  0.0007800000000000001\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.7190 - accuracy: 0.9306 - val_loss: 0.7864 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 80/100\n",
      "Learning rate:  0.00079\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.7933 - accuracy: 0.9028 - val_loss: 0.7434 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 81/100\n",
      "Learning rate:  0.0008\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.7842 - accuracy: 0.9028 - val_loss: 0.7003 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 82/100\n",
      "Learning rate:  8.1e-05\n",
      "2/2 [==============================] - 3s 531ms/step - loss: 0.7744 - accuracy: 0.9167 - val_loss: 0.6967 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 83/100\n",
      "Learning rate:  8.2e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8505 - accuracy: 0.9028 - val_loss: 0.6980 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 84/100\n",
      "Learning rate:  8.3e-05\n",
      "2/2 [==============================] - 2s 469ms/step - loss: 0.7388 - accuracy: 0.9306 - val_loss: 0.6981 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 85/100\n",
      "Learning rate:  8.4e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7087 - accuracy: 0.9583 - val_loss: 0.6980 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 86/100\n",
      "Learning rate:  8.5e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7189 - accuracy: 0.9444 - val_loss: 0.6970 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 87/100\n",
      "Learning rate:  8.6e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8017 - accuracy: 0.9028 - val_loss: 0.6967 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 88/100\n",
      "Learning rate:  8.7e-05\n",
      "2/2 [==============================] - 2s 450ms/step - loss: 0.6947 - accuracy: 0.9583 - val_loss: 0.6955 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 89/100\n",
      "Learning rate:  8.800000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8136 - accuracy: 0.9306 - val_loss: 0.7001 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 90/100\n",
      "Learning rate:  8.900000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6903 - accuracy: 0.9444 - val_loss: 0.7028 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 91/100\n",
      "Learning rate:  9e-05\n",
      "2/2 [==============================] - 2s 534ms/step - loss: 0.7098 - accuracy: 0.9583 - val_loss: 0.7053 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 92/100\n",
      "Learning rate:  9.1e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7586 - accuracy: 0.9444 - val_loss: 0.7091 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 93/100\n",
      "Learning rate:  9.200000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7069 - accuracy: 0.9583 - val_loss: 0.7200 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 94/100\n",
      "Learning rate:  9.300000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7018 - accuracy: 0.9583 - val_loss: 0.7311 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 95/100\n",
      "Learning rate:  9.4e-05\n",
      "2/2 [==============================] - 2s 442ms/step - loss: 0.6614 - accuracy: 0.9583 - val_loss: 0.7401 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 96/100\n",
      "Learning rate:  9.5e-05\n",
      "2/2 [==============================] - 2s 547ms/step - loss: 0.6535 - accuracy: 0.9861 - val_loss: 0.7472 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 97/100\n",
      "Learning rate:  9.6e-05\n",
      "2/2 [==============================] - 2s 431ms/step - loss: 0.6678 - accuracy: 0.9722 - val_loss: 0.7522 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 98/100\n",
      "Learning rate:  9.7e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6443 - accuracy: 0.9722 - val_loss: 0.7538 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 99/100\n",
      "Learning rate:  9.8e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7129 - accuracy: 0.9444 - val_loss: 0.7459 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 100/100\n",
      "Learning rate:  9.900000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6584 - accuracy: 0.9861 - val_loss: 0.7484 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 218ms/step - loss: 1.2892 - accuracy: 0.7241\n",
      "Test loss: 1.2892245054244995\n",
      "Test accuracy: 0.7241379022598267\n",
      "[1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision:  0.871264367816092\n",
      "Recall:  0.7241379310344828\n",
      "F1 Score:  0.7436433298502263\n",
      "30\n",
      "30\n",
      "[[[[  9   9   2 ...   9   9   2]\n",
      "   [  6   6   2 ...   6   6   2]\n",
      "   [  4   3   1 ...   4   3   1]\n",
      "   ...\n",
      "   [  0   1   1 ...   0   1   1]\n",
      "   [  1   2   2 ...   1   2   2]\n",
      "   [  9  10   7 ...   9  10   7]]\n",
      "\n",
      "  [[  9   8   4 ...   9   8   4]\n",
      "   [  9   9   4 ...   9   9   4]\n",
      "   [  9   8   3 ...   9   8   3]\n",
      "   ...\n",
      "   [  1   1   1 ...   1   1   1]\n",
      "   [  4   4   3 ...   4   4   3]\n",
      "   [  4   5   3 ...   4   5   3]]\n",
      "\n",
      "  [[ 26  23  14 ...  26  23  14]\n",
      "   [ 33  27  15 ...  33  27  15]\n",
      "   [ 45  37  24 ...  45  37  24]\n",
      "   ...\n",
      "   [  2   3   2 ...   2   3   2]\n",
      "   [  2   2   2 ...   2   2   2]\n",
      "   [  4   4   3 ...   4   4   3]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[188 178 161 ... 188 178 161]\n",
      "   [190 180 162 ... 190 180 162]\n",
      "   [187 177 158 ... 187 177 158]\n",
      "   ...\n",
      "   [150 142 124 ... 150 142 124]\n",
      "   [145 135 118 ... 145 135 118]\n",
      "   [145 135 117 ... 145 135 117]]\n",
      "\n",
      "  [[197 186 170 ... 197 186 170]\n",
      "   [194 184 168 ... 194 184 168]\n",
      "   [192 181 164 ... 192 181 164]\n",
      "   ...\n",
      "   [148 139 120 ... 148 139 120]\n",
      "   [149 138 119 ... 149 138 119]\n",
      "   [147 136 117 ... 147 136 117]]\n",
      "\n",
      "  [[201 191 174 ... 201 191 174]\n",
      "   [198 188 172 ... 198 188 172]\n",
      "   [194 183 167 ... 194 183 167]\n",
      "   ...\n",
      "   [150 139 120 ... 150 139 120]\n",
      "   [152 141 121 ... 152 141 121]\n",
      "   [152 142 124 ... 152 142 124]]]\n",
      "\n",
      "\n",
      " [[[255 255 255 ... 255 255 255]\n",
      "   [243 246 242 ... 243 246 242]\n",
      "   [214 220 205 ... 214 220 205]\n",
      "   ...\n",
      "   [106 120  98 ... 106 120  98]\n",
      "   [210 215 205 ... 210 215 205]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [246 248 245 ... 246 248 245]\n",
      "   [217 226 213 ... 217 226 213]\n",
      "   ...\n",
      "   [108 123  99 ... 108 123  99]\n",
      "   [211 216 206 ... 211 216 206]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [248 251 248 ... 248 251 248]\n",
      "   [221 232 221 ... 221 232 221]\n",
      "   ...\n",
      "   [110 125 100 ... 110 125 100]\n",
      "   [212 217 206 ... 212 217 206]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [249 250 249 ... 249 250 249]\n",
      "   [232 229 221 ... 232 229 221]\n",
      "   ...\n",
      "   [ 86 101  95 ...  86 101  95]\n",
      "   [204 210 208 ... 204 210 208]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [250 250 249 ... 250 250 249]\n",
      "   [234 236 231 ... 234 236 231]\n",
      "   ...\n",
      "   [102 111 101 ... 102 111 101]\n",
      "   [202 208 207 ... 202 208 207]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [250 250 249 ... 250 250 249]\n",
      "   [237 240 235 ... 237 240 235]\n",
      "   ...\n",
      "   [128 125 110 ... 128 125 110]\n",
      "   [199 206 205 ... 199 206 205]\n",
      "   [255 255 255 ... 255 255 255]]]\n",
      "\n",
      "\n",
      " [[[ 93  64  41 ...  93  64  41]\n",
      "   [101  73  50 ... 101  73  50]\n",
      "   [109  83  61 ... 109  83  61]\n",
      "   ...\n",
      "   [ 92  57  35 ...  92  57  35]\n",
      "   [ 88  52  30 ...  88  52  30]\n",
      "   [ 83  47  26 ...  83  47  26]]\n",
      "\n",
      "  [[101  73  50 ... 101  73  50]\n",
      "   [109  82  60 ... 109  82  60]\n",
      "   [116  92  70 ... 116  92  70]\n",
      "   ...\n",
      "   [ 97  62  39 ...  97  62  39]\n",
      "   [ 92  56  34 ...  92  56  34]\n",
      "   [ 87  51  30 ...  87  51  30]]\n",
      "\n",
      "  [[108  82  59 ... 108  82  59]\n",
      "   [115  91  70 ... 115  91  70]\n",
      "   [122 100  81 ... 122 100  81]\n",
      "   ...\n",
      "   [101  66  44 ... 101  66  44]\n",
      "   [ 96  60  38 ...  96  60  38]\n",
      "   [ 91  54  33 ...  91  54  33]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 87  89  90 ...  87  89  90]\n",
      "   [ 98 100 102 ...  98 100 102]\n",
      "   [112 115 118 ... 112 115 118]\n",
      "   ...\n",
      "   [242 187 163 ... 242 187 163]\n",
      "   [230 169 145 ... 230 169 145]\n",
      "   [209 149 124 ... 209 149 124]]\n",
      "\n",
      "  [[ 81  83  83 ...  81  83  83]\n",
      "   [ 87  89  91 ...  87  89  91]\n",
      "   [106 108 111 ... 106 108 111]\n",
      "   ...\n",
      "   [235 180 155 ... 235 180 155]\n",
      "   [224 167 141 ... 224 167 141]\n",
      "   [204 143 118 ... 204 143 118]]\n",
      "\n",
      "  [[ 78  80  80 ...  78  80  80]\n",
      "   [ 79  80  82 ...  79  80  82]\n",
      "   [ 94  96  99 ...  94  96  99]\n",
      "   ...\n",
      "   [229 174 149 ... 229 174 149]\n",
      "   [218 160 135 ... 218 160 135]\n",
      "   [197 137 113 ... 197 137 113]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (70, 40, 30, 9)\n",
      "70 train samples\n",
      "30 test samples\n",
      "y_train shape: (70, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_527 (Conv2D)             (None, 40, 30, 16)   1312        input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_476 (BatchN (None, 40, 30, 16)   64          conv2d_527[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_476 (Activation)     (None, 40, 30, 16)   0           batch_normalization_476[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_528 (Conv2D)             (None, 40, 30, 16)   272         activation_476[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_477 (BatchN (None, 40, 30, 16)   64          conv2d_528[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_477 (Activation)     (None, 40, 30, 16)   0           batch_normalization_477[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_529 (Conv2D)             (None, 40, 30, 16)   2320        activation_477[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_478 (BatchN (None, 40, 30, 16)   64          conv2d_529[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_478 (Activation)     (None, 40, 30, 16)   0           batch_normalization_478[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_531 (Conv2D)             (None, 40, 30, 64)   1088        activation_476[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_530 (Conv2D)             (None, 40, 30, 64)   1088        activation_478[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_153 (Add)                   (None, 40, 30, 64)   0           conv2d_531[0][0]                 \n",
      "                                                                 conv2d_530[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_479 (BatchN (None, 40, 30, 64)   256         add_153[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_479 (Activation)     (None, 40, 30, 64)   0           batch_normalization_479[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_532 (Conv2D)             (None, 40, 30, 16)   1040        activation_479[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_480 (BatchN (None, 40, 30, 16)   64          conv2d_532[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_480 (Activation)     (None, 40, 30, 16)   0           batch_normalization_480[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_533 (Conv2D)             (None, 40, 30, 16)   2320        activation_480[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_481 (BatchN (None, 40, 30, 16)   64          conv2d_533[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_481 (Activation)     (None, 40, 30, 16)   0           batch_normalization_481[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_534 (Conv2D)             (None, 40, 30, 64)   1088        activation_481[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_154 (Add)                   (None, 40, 30, 64)   0           add_153[0][0]                    \n",
      "                                                                 conv2d_534[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_482 (BatchN (None, 40, 30, 64)   256         add_154[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_482 (Activation)     (None, 40, 30, 64)   0           batch_normalization_482[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_535 (Conv2D)             (None, 40, 30, 16)   1040        activation_482[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_483 (BatchN (None, 40, 30, 16)   64          conv2d_535[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_483 (Activation)     (None, 40, 30, 16)   0           batch_normalization_483[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_536 (Conv2D)             (None, 40, 30, 16)   2320        activation_483[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_484 (BatchN (None, 40, 30, 16)   64          conv2d_536[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_484 (Activation)     (None, 40, 30, 16)   0           batch_normalization_484[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_537 (Conv2D)             (None, 40, 30, 64)   1088        activation_484[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_155 (Add)                   (None, 40, 30, 64)   0           add_154[0][0]                    \n",
      "                                                                 conv2d_537[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_485 (BatchN (None, 40, 30, 64)   256         add_155[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_485 (Activation)     (None, 40, 30, 64)   0           batch_normalization_485[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_538 (Conv2D)             (None, 20, 15, 64)   4160        activation_485[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_486 (BatchN (None, 20, 15, 64)   256         conv2d_538[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_486 (Activation)     (None, 20, 15, 64)   0           batch_normalization_486[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_539 (Conv2D)             (None, 20, 15, 64)   36928       activation_486[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_487 (BatchN (None, 20, 15, 64)   256         conv2d_539[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_487 (Activation)     (None, 20, 15, 64)   0           batch_normalization_487[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_541 (Conv2D)             (None, 20, 15, 128)  8320        add_155[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_540 (Conv2D)             (None, 20, 15, 128)  8320        activation_487[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_156 (Add)                   (None, 20, 15, 128)  0           conv2d_541[0][0]                 \n",
      "                                                                 conv2d_540[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_488 (BatchN (None, 20, 15, 128)  512         add_156[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_488 (Activation)     (None, 20, 15, 128)  0           batch_normalization_488[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_542 (Conv2D)             (None, 20, 15, 64)   8256        activation_488[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_489 (BatchN (None, 20, 15, 64)   256         conv2d_542[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_489 (Activation)     (None, 20, 15, 64)   0           batch_normalization_489[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_543 (Conv2D)             (None, 20, 15, 64)   36928       activation_489[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_490 (BatchN (None, 20, 15, 64)   256         conv2d_543[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_490 (Activation)     (None, 20, 15, 64)   0           batch_normalization_490[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_544 (Conv2D)             (None, 20, 15, 128)  8320        activation_490[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_157 (Add)                   (None, 20, 15, 128)  0           add_156[0][0]                    \n",
      "                                                                 conv2d_544[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_491 (BatchN (None, 20, 15, 128)  512         add_157[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_491 (Activation)     (None, 20, 15, 128)  0           batch_normalization_491[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_545 (Conv2D)             (None, 20, 15, 64)   8256        activation_491[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_492 (BatchN (None, 20, 15, 64)   256         conv2d_545[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_492 (Activation)     (None, 20, 15, 64)   0           batch_normalization_492[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_546 (Conv2D)             (None, 20, 15, 64)   36928       activation_492[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_493 (BatchN (None, 20, 15, 64)   256         conv2d_546[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_493 (Activation)     (None, 20, 15, 64)   0           batch_normalization_493[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_547 (Conv2D)             (None, 20, 15, 128)  8320        activation_493[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_158 (Add)                   (None, 20, 15, 128)  0           add_157[0][0]                    \n",
      "                                                                 conv2d_547[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_494 (BatchN (None, 20, 15, 128)  512         add_158[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_494 (Activation)     (None, 20, 15, 128)  0           batch_normalization_494[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_548 (Conv2D)             (None, 10, 8, 128)   16512       activation_494[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_495 (BatchN (None, 10, 8, 128)   512         conv2d_548[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_495 (Activation)     (None, 10, 8, 128)   0           batch_normalization_495[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_549 (Conv2D)             (None, 10, 8, 128)   147584      activation_495[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_496 (BatchN (None, 10, 8, 128)   512         conv2d_549[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_496 (Activation)     (None, 10, 8, 128)   0           batch_normalization_496[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_551 (Conv2D)             (None, 10, 8, 256)   33024       add_158[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_550 (Conv2D)             (None, 10, 8, 256)   33024       activation_496[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_159 (Add)                   (None, 10, 8, 256)   0           conv2d_551[0][0]                 \n",
      "                                                                 conv2d_550[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_497 (BatchN (None, 10, 8, 256)   1024        add_159[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_497 (Activation)     (None, 10, 8, 256)   0           batch_normalization_497[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_552 (Conv2D)             (None, 10, 8, 128)   32896       activation_497[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_498 (BatchN (None, 10, 8, 128)   512         conv2d_552[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_498 (Activation)     (None, 10, 8, 128)   0           batch_normalization_498[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_553 (Conv2D)             (None, 10, 8, 128)   147584      activation_498[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_499 (BatchN (None, 10, 8, 128)   512         conv2d_553[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_499 (Activation)     (None, 10, 8, 128)   0           batch_normalization_499[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_554 (Conv2D)             (None, 10, 8, 256)   33024       activation_499[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_160 (Add)                   (None, 10, 8, 256)   0           add_159[0][0]                    \n",
      "                                                                 conv2d_554[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_500 (BatchN (None, 10, 8, 256)   1024        add_160[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_500 (Activation)     (None, 10, 8, 256)   0           batch_normalization_500[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_555 (Conv2D)             (None, 10, 8, 128)   32896       activation_500[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_501 (BatchN (None, 10, 8, 128)   512         conv2d_555[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_501 (Activation)     (None, 10, 8, 128)   0           batch_normalization_501[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_556 (Conv2D)             (None, 10, 8, 128)   147584      activation_501[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_502 (BatchN (None, 10, 8, 128)   512         conv2d_556[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_502 (Activation)     (None, 10, 8, 128)   0           batch_normalization_502[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_557 (Conv2D)             (None, 10, 8, 256)   33024       activation_502[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_161 (Add)                   (None, 10, 8, 256)   0           add_160[0][0]                    \n",
      "                                                                 conv2d_557[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_503 (BatchN (None, 10, 8, 256)   1024        add_161[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_503 (Activation)     (None, 10, 8, 256)   0           batch_normalization_503[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_17 (AveragePo (None, 1, 1, 256)    0           activation_503[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 256)          0           average_pooling2d_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 10)           2570        flatten_17[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[0.03529412 0.03529412 0.00784314 ... 0.03529412 0.03529412\n",
      "    0.00784314]\n",
      "   [0.02352941 0.02352941 0.00784314 ... 0.02352941 0.02352941\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01176471 0.00392157 ... 0.01568628 0.01176471\n",
      "    0.00392157]\n",
      "   ...\n",
      "   [0.         0.00392157 0.00392157 ... 0.         0.00392157\n",
      "    0.00392157]\n",
      "   [0.00392157 0.00784314 0.00784314 ... 0.00392157 0.00784314\n",
      "    0.00784314]\n",
      "   [0.03529412 0.03921569 0.02745098 ... 0.03529412 0.03921569\n",
      "    0.02745098]]\n",
      "\n",
      "  [[0.03529412 0.03137255 0.01568628 ... 0.03529412 0.03137255\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03529412 0.01568628 ... 0.03529412 0.03529412\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03137255 0.01176471 ... 0.03529412 0.03137255\n",
      "    0.01176471]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157 ... 0.00392157 0.00392157\n",
      "    0.00392157]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]\n",
      "   [0.01568628 0.01960784 0.01176471 ... 0.01568628 0.01960784\n",
      "    0.01176471]]\n",
      "\n",
      "  [[0.10196079 0.09019608 0.05490196 ... 0.10196079 0.09019608\n",
      "    0.05490196]\n",
      "   [0.12941177 0.10588235 0.05882353 ... 0.12941177 0.10588235\n",
      "    0.05882353]\n",
      "   [0.1764706  0.14509805 0.09411765 ... 0.1764706  0.14509805\n",
      "    0.09411765]\n",
      "   ...\n",
      "   [0.00784314 0.01176471 0.00784314 ... 0.00784314 0.01176471\n",
      "    0.00784314]\n",
      "   [0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.7372549  0.69803923 0.6313726  ... 0.7372549  0.69803923\n",
      "    0.6313726 ]\n",
      "   [0.74509805 0.7058824  0.63529414 ... 0.74509805 0.7058824\n",
      "    0.63529414]\n",
      "   [0.73333335 0.69411767 0.61960787 ... 0.73333335 0.69411767\n",
      "    0.61960787]\n",
      "   ...\n",
      "   [0.5882353  0.5568628  0.4862745  ... 0.5882353  0.5568628\n",
      "    0.4862745 ]\n",
      "   [0.5686275  0.5294118  0.4627451  ... 0.5686275  0.5294118\n",
      "    0.4627451 ]\n",
      "   [0.5686275  0.5294118  0.45882353 ... 0.5686275  0.5294118\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.77254903 0.7294118  0.6666667  ... 0.77254903 0.7294118\n",
      "    0.6666667 ]\n",
      "   [0.7607843  0.72156864 0.65882355 ... 0.7607843  0.72156864\n",
      "    0.65882355]\n",
      "   [0.7529412  0.70980394 0.6431373  ... 0.7529412  0.70980394\n",
      "    0.6431373 ]\n",
      "   ...\n",
      "   [0.5803922  0.54509807 0.47058824 ... 0.5803922  0.54509807\n",
      "    0.47058824]\n",
      "   [0.58431375 0.5411765  0.46666667 ... 0.58431375 0.5411765\n",
      "    0.46666667]\n",
      "   [0.5764706  0.53333336 0.45882353 ... 0.5764706  0.53333336\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.7882353  0.7490196  0.68235296 ... 0.7882353  0.7490196\n",
      "    0.68235296]\n",
      "   [0.7764706  0.7372549  0.6745098  ... 0.7764706  0.7372549\n",
      "    0.6745098 ]\n",
      "   [0.7607843  0.7176471  0.654902   ... 0.7607843  0.7176471\n",
      "    0.654902  ]\n",
      "   ...\n",
      "   [0.5882353  0.54509807 0.47058824 ... 0.5882353  0.54509807\n",
      "    0.47058824]\n",
      "   [0.59607846 0.5529412  0.4745098  ... 0.59607846 0.5529412\n",
      "    0.4745098 ]\n",
      "   [0.59607846 0.5568628  0.4862745  ... 0.59607846 0.5568628\n",
      "    0.4862745 ]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9529412  0.9647059  0.9490196  ... 0.9529412  0.9647059\n",
      "    0.9490196 ]\n",
      "   [0.8392157  0.8627451  0.8039216  ... 0.8392157  0.8627451\n",
      "    0.8039216 ]\n",
      "   ...\n",
      "   [0.41568628 0.47058824 0.38431373 ... 0.41568628 0.47058824\n",
      "    0.38431373]\n",
      "   [0.8235294  0.84313726 0.8039216  ... 0.8235294  0.84313726\n",
      "    0.8039216 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9647059  0.972549   0.9607843  ... 0.9647059  0.972549\n",
      "    0.9607843 ]\n",
      "   [0.8509804  0.8862745  0.8352941  ... 0.8509804  0.8862745\n",
      "    0.8352941 ]\n",
      "   ...\n",
      "   [0.42352942 0.48235294 0.3882353  ... 0.42352942 0.48235294\n",
      "    0.3882353 ]\n",
      "   [0.827451   0.84705883 0.80784315 ... 0.827451   0.84705883\n",
      "    0.80784315]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.972549   0.9843137  0.972549   ... 0.972549   0.9843137\n",
      "    0.972549  ]\n",
      "   [0.8666667  0.9098039  0.8666667  ... 0.8666667  0.9098039\n",
      "    0.8666667 ]\n",
      "   ...\n",
      "   [0.43137255 0.49019608 0.39215687 ... 0.43137255 0.49019608\n",
      "    0.39215687]\n",
      "   [0.83137256 0.8509804  0.80784315 ... 0.83137256 0.8509804\n",
      "    0.80784315]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9764706  0.98039216 0.9764706  ... 0.9764706  0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.9098039  0.8980392  0.8666667  ... 0.9098039  0.8980392\n",
      "    0.8666667 ]\n",
      "   ...\n",
      "   [0.3372549  0.39607844 0.37254903 ... 0.3372549  0.39607844\n",
      "    0.37254903]\n",
      "   [0.8        0.8235294  0.8156863  ... 0.8        0.8235294\n",
      "    0.8156863 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.98039216 0.98039216 0.9764706  ... 0.98039216 0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.91764706 0.9254902  0.90588236 ... 0.91764706 0.9254902\n",
      "    0.90588236]\n",
      "   ...\n",
      "   [0.4        0.43529412 0.39607844 ... 0.4        0.43529412\n",
      "    0.39607844]\n",
      "   [0.7921569  0.8156863  0.8117647  ... 0.7921569  0.8156863\n",
      "    0.8117647 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.98039216 0.98039216 0.9764706  ... 0.98039216 0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.92941177 0.9411765  0.92156863 ... 0.92941177 0.9411765\n",
      "    0.92156863]\n",
      "   ...\n",
      "   [0.5019608  0.49019608 0.43137255 ... 0.5019608  0.49019608\n",
      "    0.43137255]\n",
      "   [0.78039217 0.80784315 0.8039216  ... 0.78039217 0.80784315\n",
      "    0.8039216 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]]\n",
      "\n",
      "\n",
      " [[[0.3647059  0.2509804  0.16078432 ... 0.3647059  0.2509804\n",
      "    0.16078432]\n",
      "   [0.39607844 0.28627452 0.19607843 ... 0.39607844 0.28627452\n",
      "    0.19607843]\n",
      "   [0.42745098 0.3254902  0.23921569 ... 0.42745098 0.3254902\n",
      "    0.23921569]\n",
      "   ...\n",
      "   [0.36078432 0.22352941 0.13725491 ... 0.36078432 0.22352941\n",
      "    0.13725491]\n",
      "   [0.34509805 0.20392157 0.11764706 ... 0.34509805 0.20392157\n",
      "    0.11764706]\n",
      "   [0.3254902  0.18431373 0.10196079 ... 0.3254902  0.18431373\n",
      "    0.10196079]]\n",
      "\n",
      "  [[0.39607844 0.28627452 0.19607843 ... 0.39607844 0.28627452\n",
      "    0.19607843]\n",
      "   [0.42745098 0.32156864 0.23529412 ... 0.42745098 0.32156864\n",
      "    0.23529412]\n",
      "   [0.45490196 0.36078432 0.27450982 ... 0.45490196 0.36078432\n",
      "    0.27450982]\n",
      "   ...\n",
      "   [0.38039216 0.24313726 0.15294118 ... 0.38039216 0.24313726\n",
      "    0.15294118]\n",
      "   [0.36078432 0.21960784 0.13333334 ... 0.36078432 0.21960784\n",
      "    0.13333334]\n",
      "   [0.34117648 0.2        0.11764706 ... 0.34117648 0.2\n",
      "    0.11764706]]\n",
      "\n",
      "  [[0.42352942 0.32156864 0.23137255 ... 0.42352942 0.32156864\n",
      "    0.23137255]\n",
      "   [0.4509804  0.35686275 0.27450982 ... 0.4509804  0.35686275\n",
      "    0.27450982]\n",
      "   [0.47843137 0.39215687 0.31764707 ... 0.47843137 0.39215687\n",
      "    0.31764707]\n",
      "   ...\n",
      "   [0.39607844 0.25882354 0.17254902 ... 0.39607844 0.25882354\n",
      "    0.17254902]\n",
      "   [0.3764706  0.23529412 0.14901961 ... 0.3764706  0.23529412\n",
      "    0.14901961]\n",
      "   [0.35686275 0.21176471 0.12941177 ... 0.35686275 0.21176471\n",
      "    0.12941177]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.34117648 0.34901962 0.3529412  ... 0.34117648 0.34901962\n",
      "    0.3529412 ]\n",
      "   [0.38431373 0.39215687 0.4        ... 0.38431373 0.39215687\n",
      "    0.4       ]\n",
      "   [0.4392157  0.4509804  0.4627451  ... 0.4392157  0.4509804\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.9490196  0.73333335 0.6392157  ... 0.9490196  0.73333335\n",
      "    0.6392157 ]\n",
      "   [0.9019608  0.6627451  0.5686275  ... 0.9019608  0.6627451\n",
      "    0.5686275 ]\n",
      "   [0.81960785 0.58431375 0.4862745  ... 0.81960785 0.58431375\n",
      "    0.4862745 ]]\n",
      "\n",
      "  [[0.31764707 0.3254902  0.3254902  ... 0.31764707 0.3254902\n",
      "    0.3254902 ]\n",
      "   [0.34117648 0.34901962 0.35686275 ... 0.34117648 0.34901962\n",
      "    0.35686275]\n",
      "   [0.41568628 0.42352942 0.43529412 ... 0.41568628 0.42352942\n",
      "    0.43529412]\n",
      "   ...\n",
      "   [0.92156863 0.7058824  0.60784316 ... 0.92156863 0.7058824\n",
      "    0.60784316]\n",
      "   [0.8784314  0.654902   0.5529412  ... 0.8784314  0.654902\n",
      "    0.5529412 ]\n",
      "   [0.8        0.56078434 0.4627451  ... 0.8        0.56078434\n",
      "    0.4627451 ]]\n",
      "\n",
      "  [[0.30588236 0.3137255  0.3137255  ... 0.30588236 0.3137255\n",
      "    0.3137255 ]\n",
      "   [0.30980393 0.3137255  0.32156864 ... 0.30980393 0.3137255\n",
      "    0.32156864]\n",
      "   [0.36862746 0.3764706  0.3882353  ... 0.36862746 0.3764706\n",
      "    0.3882353 ]\n",
      "   ...\n",
      "   [0.8980392  0.68235296 0.58431375 ... 0.8980392  0.68235296\n",
      "    0.58431375]\n",
      "   [0.85490197 0.627451   0.5294118  ... 0.85490197 0.627451\n",
      "    0.5294118 ]\n",
      "   [0.77254903 0.5372549  0.44313726 ... 0.77254903 0.5372549\n",
      "    0.44313726]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.49803922 0.49803922 0.5176471  ... 0.49803922 0.49803922\n",
      "    0.5176471 ]\n",
      "   [0.5058824  0.5019608  0.52156866 ... 0.5058824  0.5019608\n",
      "    0.52156866]\n",
      "   [0.50980395 0.5058824  0.5254902  ... 0.50980395 0.5058824\n",
      "    0.5254902 ]\n",
      "   ...\n",
      "   [0.5686275  0.57254905 0.5882353  ... 0.5686275  0.57254905\n",
      "    0.5882353 ]\n",
      "   [0.57254905 0.5764706  0.5921569  ... 0.57254905 0.5764706\n",
      "    0.5921569 ]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]]\n",
      "\n",
      "  [[0.50980395 0.50980395 0.5254902  ... 0.50980395 0.50980395\n",
      "    0.5254902 ]\n",
      "   [0.5137255  0.5137255  0.5294118  ... 0.5137255  0.5137255\n",
      "    0.5294118 ]\n",
      "   [0.5176471  0.5176471  0.53333336 ... 0.5176471  0.5176471\n",
      "    0.53333336]\n",
      "   ...\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5803922  0.58431375 0.6        ... 0.5803922  0.58431375\n",
      "    0.6       ]]\n",
      "\n",
      "  [[0.5176471  0.5176471  0.5372549  ... 0.5176471  0.5176471\n",
      "    0.5372549 ]\n",
      "   [0.52156866 0.5254902  0.5411765  ... 0.52156866 0.5254902\n",
      "    0.5411765 ]\n",
      "   [0.5176471  0.52156866 0.5372549  ... 0.5176471  0.52156866\n",
      "    0.5372549 ]\n",
      "   ...\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.5882353  0.5921569  0.60784316 ... 0.5882353  0.5921569\n",
      "    0.60784316]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8901961  0.8392157  0.8039216  ... 0.8901961  0.8392157\n",
      "    0.8039216 ]\n",
      "   [0.91764706 0.8745098  0.8392157  ... 0.91764706 0.8745098\n",
      "    0.8392157 ]\n",
      "   [0.8627451  0.79607844 0.7490196  ... 0.8627451  0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9372549  0.90588236 0.8784314  ... 0.9372549  0.90588236\n",
      "    0.8784314 ]\n",
      "   [0.90588236 0.85882354 0.8235294  ... 0.90588236 0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.87058824 0.8117647  0.7764706  ... 0.87058824 0.8117647\n",
      "    0.7764706 ]]\n",
      "\n",
      "  [[0.88235295 0.8235294  0.78431374 ... 0.88235295 0.8235294\n",
      "    0.78431374]\n",
      "   [0.89411765 0.8392157  0.8        ... 0.89411765 0.8392157\n",
      "    0.8       ]\n",
      "   [0.87058824 0.8039216  0.7607843  ... 0.87058824 0.8039216\n",
      "    0.7607843 ]\n",
      "   ...\n",
      "   [0.92156863 0.8862745  0.8509804  ... 0.92156863 0.8862745\n",
      "    0.8509804 ]\n",
      "   [0.9098039  0.87058824 0.8352941  ... 0.9098039  0.87058824\n",
      "    0.8352941 ]\n",
      "   [0.8745098  0.8156863  0.77254903 ... 0.8745098  0.8156863\n",
      "    0.77254903]]\n",
      "\n",
      "  [[0.8627451  0.7921569  0.7411765  ... 0.8627451  0.7921569\n",
      "    0.7411765 ]\n",
      "   [0.8666667  0.8039216  0.7529412  ... 0.8666667  0.8039216\n",
      "    0.7529412 ]\n",
      "   [0.85882354 0.79607844 0.7490196  ... 0.85882354 0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9098039  0.85882354 0.8235294  ... 0.9098039  0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.90588236 0.85490197 0.8156863  ... 0.90588236 0.85490197\n",
      "    0.8156863 ]\n",
      "   [0.8784314  0.8156863  0.7647059  ... 0.8784314  0.8156863\n",
      "    0.7647059 ]]]\n",
      "\n",
      "\n",
      " [[[0.65882355 0.5411765  0.34901962 ... 0.65882355 0.5411765\n",
      "    0.34901962]\n",
      "   [0.79607844 0.6862745  0.44313726 ... 0.79607844 0.6862745\n",
      "    0.44313726]\n",
      "   [0.84705883 0.7372549  0.4862745  ... 0.84705883 0.7372549\n",
      "    0.4862745 ]\n",
      "   ...\n",
      "   [0.9019608  0.78431374 0.57254905 ... 0.9019608  0.78431374\n",
      "    0.57254905]\n",
      "   [0.8784314  0.77254903 0.5803922  ... 0.8784314  0.77254903\n",
      "    0.5803922 ]\n",
      "   [0.8666667  0.7607843  0.58431375 ... 0.8666667  0.7607843\n",
      "    0.58431375]]\n",
      "\n",
      "  [[0.6313726  0.5058824  0.27450982 ... 0.6313726  0.5058824\n",
      "    0.27450982]\n",
      "   [0.7372549  0.61960787 0.3764706  ... 0.7372549  0.61960787\n",
      "    0.3764706 ]\n",
      "   [0.83137256 0.72156864 0.4627451  ... 0.83137256 0.72156864\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.8784314  0.7607843  0.5568628  ... 0.8784314  0.7607843\n",
      "    0.5568628 ]\n",
      "   [0.85882354 0.7490196  0.56078434 ... 0.85882354 0.7490196\n",
      "    0.56078434]\n",
      "   [0.85490197 0.7529412  0.5647059  ... 0.85490197 0.7529412\n",
      "    0.5647059 ]]\n",
      "\n",
      "  [[0.6        0.46666667 0.22352941 ... 0.6        0.46666667\n",
      "    0.22352941]\n",
      "   [0.6313726  0.49803922 0.27058825 ... 0.6313726  0.49803922\n",
      "    0.27058825]\n",
      "   [0.80784315 0.69803923 0.44705883 ... 0.80784315 0.69803923\n",
      "    0.44705883]\n",
      "   ...\n",
      "   [0.8352941  0.72156864 0.5372549  ... 0.8352941  0.72156864\n",
      "    0.5372549 ]\n",
      "   [0.83137256 0.7254902  0.5411765  ... 0.83137256 0.7254902\n",
      "    0.5411765 ]\n",
      "   [0.8352941  0.73333335 0.5372549  ... 0.8352941  0.73333335\n",
      "    0.5372549 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6039216  0.3764706  0.27058825 ... 0.6039216  0.3764706\n",
      "    0.27058825]\n",
      "   [0.627451   0.40784314 0.31764707 ... 0.627451   0.40784314\n",
      "    0.31764707]\n",
      "   [0.67058825 0.47058824 0.4        ... 0.67058825 0.47058824\n",
      "    0.4       ]\n",
      "   ...\n",
      "   [0.76862746 0.58431375 0.4745098  ... 0.76862746 0.58431375\n",
      "    0.4745098 ]\n",
      "   [0.7058824  0.49803922 0.38431373 ... 0.7058824  0.49803922\n",
      "    0.38431373]\n",
      "   [0.65882355 0.44313726 0.32156864 ... 0.65882355 0.44313726\n",
      "    0.32156864]]\n",
      "\n",
      "  [[0.6        0.38039216 0.26666668 ... 0.6        0.38039216\n",
      "    0.26666668]\n",
      "   [0.63529414 0.41568628 0.32156864 ... 0.63529414 0.41568628\n",
      "    0.32156864]\n",
      "   [0.7372549  0.56078434 0.4627451  ... 0.7372549  0.56078434\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.83137256 0.654902   0.54901963 ... 0.83137256 0.654902\n",
      "    0.54901963]\n",
      "   [0.77254903 0.5647059  0.44313726 ... 0.77254903 0.5647059\n",
      "    0.44313726]\n",
      "   [0.7176471  0.5019608  0.38039216 ... 0.7176471  0.5019608\n",
      "    0.38039216]]\n",
      "\n",
      "  [[0.627451   0.41960785 0.29411766 ... 0.627451   0.41960785\n",
      "    0.29411766]\n",
      "   [0.68235296 0.46666667 0.3882353  ... 0.68235296 0.46666667\n",
      "    0.3882353 ]\n",
      "   [0.8509804  0.7294118  0.49019608 ... 0.8509804  0.7294118\n",
      "    0.49019608]\n",
      "   ...\n",
      "   [0.89411765 0.72156864 0.6117647  ... 0.89411765 0.72156864\n",
      "    0.6117647 ]\n",
      "   [0.8392157  0.627451   0.50980395 ... 0.8392157  0.627451\n",
      "    0.50980395]\n",
      "   [0.76862746 0.56078434 0.43529412 ... 0.76862746 0.56078434\n",
      "    0.43529412]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.99215686 0.99215686 0.99215686 ... 0.99215686 0.99215686\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6509804  0.6117647  0.60784316 ... 0.6509804  0.6117647\n",
      "    0.60784316]\n",
      "   [0.6431373  0.6        0.5921569  ... 0.6431373  0.6\n",
      "    0.5921569 ]\n",
      "   [0.63529414 0.5921569  0.5803922  ... 0.63529414 0.5921569\n",
      "    0.5803922 ]\n",
      "   ...\n",
      "   [0.80784315 0.69803923 0.67058825 ... 0.80784315 0.69803923\n",
      "    0.67058825]\n",
      "   [0.80784315 0.7176471  0.6901961  ... 0.80784315 0.7176471\n",
      "    0.6901961 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]]\n",
      "\n",
      "  [[0.64705884 0.59607846 0.59607846 ... 0.64705884 0.59607846\n",
      "    0.59607846]\n",
      "   [0.63529414 0.5921569  0.58431375 ... 0.63529414 0.5921569\n",
      "    0.58431375]\n",
      "   [0.62352943 0.5803922  0.57254905 ... 0.62352943 0.5803922\n",
      "    0.57254905]\n",
      "   ...\n",
      "   [0.8        0.69803923 0.6745098  ... 0.8        0.69803923\n",
      "    0.6745098 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]\n",
      "   [0.7921569  0.7019608  0.68235296 ... 0.7921569  0.7019608\n",
      "    0.68235296]]\n",
      "\n",
      "  [[0.6431373  0.59607846 0.5921569  ... 0.6431373  0.59607846\n",
      "    0.5921569 ]\n",
      "   [0.6313726  0.5882353  0.5803922  ... 0.6313726  0.5882353\n",
      "    0.5803922 ]\n",
      "   [0.627451   0.57254905 0.5686275  ... 0.627451   0.57254905\n",
      "    0.5686275 ]\n",
      "   ...\n",
      "   [0.7921569  0.6901961  0.6784314  ... 0.7921569  0.6901961\n",
      "    0.6784314 ]\n",
      "   [0.80784315 0.7058824  0.6901961  ... 0.80784315 0.7058824\n",
      "    0.6901961 ]\n",
      "   [0.80784315 0.7176471  0.7019608  ... 0.80784315 0.7176471\n",
      "    0.7019608 ]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (70, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (70, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 7s 3s/step - loss: 2.2187 - accuracy: 0.6286 - val_loss: 2.5777 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/100\n",
      "Learning rate:  1e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.2194 - accuracy: 0.6286 - val_loss: 2.4017 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/100\n",
      "Learning rate:  2e-05\n",
      "2/2 [==============================] - 2s 375ms/step - loss: 2.2106 - accuracy: 0.6286 - val_loss: 2.2825 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/100\n",
      "Learning rate:  3e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.1910 - accuracy: 0.6286 - val_loss: 2.2034 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/100\n",
      "Learning rate:  4e-05\n",
      "2/2 [==============================] - 2s 533ms/step - loss: 2.1851 - accuracy: 0.6286 - val_loss: 2.1409 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/100\n",
      "Learning rate:  5e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.1169 - accuracy: 0.6286 - val_loss: 2.0698 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/100\n",
      "Learning rate:  6e-05\n",
      "2/2 [==============================] - 2s 418ms/step - loss: 2.1405 - accuracy: 0.6286 - val_loss: 1.9996 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/100\n",
      "Learning rate:  7.000000000000001e-05\n",
      "2/2 [==============================] - 2s 407ms/step - loss: 2.0686 - accuracy: 0.6286 - val_loss: 1.9281 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/100\n",
      "Learning rate:  8e-05\n",
      "2/2 [==============================] - 2s 373ms/step - loss: 2.0601 - accuracy: 0.6286 - val_loss: 1.8588 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/100\n",
      "Learning rate:  8.999999999999999e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.0126 - accuracy: 0.6286 - val_loss: 1.7739 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/100\n",
      "Learning rate:  0.0001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.9858 - accuracy: 0.6286 - val_loss: 1.6921 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/100\n",
      "Learning rate:  0.00011\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.9257 - accuracy: 0.6286 - val_loss: 1.6176 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/100\n",
      "Learning rate:  0.00012\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.8700 - accuracy: 0.6286 - val_loss: 1.5385 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/100\n",
      "Learning rate:  0.00013000000000000002\n",
      "2/2 [==============================] - 2s 382ms/step - loss: 1.8405 - accuracy: 0.6429 - val_loss: 1.4500 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/100\n",
      "Learning rate:  0.00014000000000000001\n",
      "2/2 [==============================] - 2s 447ms/step - loss: 1.7807 - accuracy: 0.6286 - val_loss: 1.3656 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/100\n",
      "Learning rate:  0.00015\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.6929 - accuracy: 0.7000 - val_loss: 1.2686 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/100\n",
      "Learning rate:  0.00016\n",
      "2/2 [==============================] - 2s 390ms/step - loss: 1.6916 - accuracy: 0.7286 - val_loss: 1.1910 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/100\n",
      "Learning rate:  0.00017\n",
      "2/2 [==============================] - 2s 383ms/step - loss: 1.6121 - accuracy: 0.7286 - val_loss: 1.1074 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/100\n",
      "Learning rate:  0.00017999999999999998\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.5393 - accuracy: 0.7571 - val_loss: 1.0498 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/100\n",
      "Learning rate:  0.00019\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.5263 - accuracy: 0.7857 - val_loss: 0.9969 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 21/100\n",
      "Learning rate:  0.0002\n",
      "2/2 [==============================] - 3s 404ms/step - loss: 1.4668 - accuracy: 0.7714 - val_loss: 0.9607 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 22/100\n",
      "Learning rate:  0.00021\n",
      "2/2 [==============================] - 2s 377ms/step - loss: 1.4521 - accuracy: 0.7714 - val_loss: 0.9478 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 23/100\n",
      "Learning rate:  0.00022\n",
      "2/2 [==============================] - 2s 406ms/step - loss: 1.3881 - accuracy: 0.7714 - val_loss: 0.9406 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 24/100\n",
      "Learning rate:  0.00023\n",
      "2/2 [==============================] - 2s 412ms/step - loss: 1.3362 - accuracy: 0.8000 - val_loss: 0.9170 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 25/100\n",
      "Learning rate:  0.00024\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.2630 - accuracy: 0.8000 - val_loss: 0.8995 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 26/100\n",
      "Learning rate:  0.00025\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.1988 - accuracy: 0.8143 - val_loss: 0.8674 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 27/100\n",
      "Learning rate:  0.00026000000000000003\n",
      "2/2 [==============================] - 2s 587ms/step - loss: 1.1826 - accuracy: 0.8143 - val_loss: 0.8279 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 28/100\n",
      "Learning rate:  0.00027\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.2090 - accuracy: 0.8000 - val_loss: 0.7785 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 29/100\n",
      "Learning rate:  0.00028000000000000003\n",
      "2/2 [==============================] - 2s 427ms/step - loss: 1.1520 - accuracy: 0.8143 - val_loss: 0.7460 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 30/100\n",
      "Learning rate:  0.00029\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0822 - accuracy: 0.8143 - val_loss: 0.7271 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 31/100\n",
      "Learning rate:  0.0003\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0564 - accuracy: 0.8143 - val_loss: 0.7206 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 32/100\n",
      "Learning rate:  0.00031\n",
      "2/2 [==============================] - 2s 375ms/step - loss: 1.0436 - accuracy: 0.8571 - val_loss: 0.7130 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 33/100\n",
      "Learning rate:  0.00032\n",
      "2/2 [==============================] - 2s 414ms/step - loss: 1.0661 - accuracy: 0.8286 - val_loss: 0.7022 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 34/100\n",
      "Learning rate:  0.00033\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9909 - accuracy: 0.8571 - val_loss: 0.7011 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 35/100\n",
      "Learning rate:  0.00034\n",
      "2/2 [==============================] - 2s 374ms/step - loss: 0.9654 - accuracy: 0.8714 - val_loss: 0.6956 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 36/100\n",
      "Learning rate:  0.00035\n",
      "2/2 [==============================] - 2s 412ms/step - loss: 0.9748 - accuracy: 0.8857 - val_loss: 0.6865 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 37/100\n",
      "Learning rate:  0.00035999999999999997\n",
      "2/2 [==============================] - 2s 396ms/step - loss: 0.9471 - accuracy: 0.8714 - val_loss: 0.6803 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 38/100\n",
      "Learning rate:  0.00037\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9474 - accuracy: 0.8429 - val_loss: 0.6525 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 39/100\n",
      "Learning rate:  0.00038\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9115 - accuracy: 0.8571 - val_loss: 0.6373 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 40/100\n",
      "Learning rate:  0.00039000000000000005\n",
      "2/2 [==============================] - 2s 379ms/step - loss: 0.9503 - accuracy: 0.8571 - val_loss: 0.6301 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 41/100\n",
      "Learning rate:  0.0004\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9342 - accuracy: 0.8714 - val_loss: 0.6347 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 42/100\n",
      "Learning rate:  0.00041\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8408 - accuracy: 0.9143 - val_loss: 0.6434 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 43/100\n",
      "Learning rate:  0.00042\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8709 - accuracy: 0.9143 - val_loss: 0.6609 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 44/100\n",
      "Learning rate:  0.00043\n",
      "2/2 [==============================] - 2s 379ms/step - loss: 0.8734 - accuracy: 0.9000 - val_loss: 0.6759 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 45/100\n",
      "Learning rate:  0.00044\n",
      "2/2 [==============================] - 2s 411ms/step - loss: 0.8473 - accuracy: 0.9429 - val_loss: 0.6934 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 46/100\n",
      "Learning rate:  0.00045000000000000004\n",
      "2/2 [==============================] - 2s 382ms/step - loss: 0.8706 - accuracy: 0.9143 - val_loss: 0.7259 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 47/100\n",
      "Learning rate:  0.00046\n",
      "2/2 [==============================] - 2s 420ms/step - loss: 0.8228 - accuracy: 0.9571 - val_loss: 0.7368 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 48/100\n",
      "Learning rate:  0.00047\n",
      "2/2 [==============================] - 2s 383ms/step - loss: 0.8133 - accuracy: 0.9571 - val_loss: 0.7247 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 49/100\n",
      "Learning rate:  0.00048\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8315 - accuracy: 0.9429 - val_loss: 0.6879 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 50/100\n",
      "Learning rate:  0.00049\n",
      "2/2 [==============================] - 2s 394ms/step - loss: 0.8283 - accuracy: 0.9429 - val_loss: 0.6562 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 51/100\n",
      "Learning rate:  0.0005\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8117 - accuracy: 0.9429 - val_loss: 0.6247 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 52/100\n",
      "Learning rate:  0.00051\n",
      "2/2 [==============================] - 2s 420ms/step - loss: 0.7706 - accuracy: 0.9571 - val_loss: 0.6072 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 53/100\n",
      "Learning rate:  0.0005200000000000001\n",
      "2/2 [==============================] - 2s 385ms/step - loss: 0.8135 - accuracy: 0.9571 - val_loss: 0.6018 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 54/100\n",
      "Learning rate:  0.0005300000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7539 - accuracy: 0.9714 - val_loss: 0.5997 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 55/100\n",
      "Learning rate:  0.00054\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7475 - accuracy: 0.9714 - val_loss: 0.5987 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 56/100\n",
      "Learning rate:  0.00055\n",
      "2/2 [==============================] - 2s 403ms/step - loss: 0.8874 - accuracy: 0.8857 - val_loss: 0.5985 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 57/100\n",
      "Learning rate:  0.0005600000000000001\n",
      "2/2 [==============================] - 2s 423ms/step - loss: 0.8312 - accuracy: 0.9143 - val_loss: 0.5990 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 58/100\n",
      "Learning rate:  0.00057\n",
      "2/2 [==============================] - 2s 385ms/step - loss: 0.8404 - accuracy: 0.8857 - val_loss: 0.5994 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 59/100\n",
      "Learning rate:  0.00058\n",
      "2/2 [==============================] - 2s 368ms/step - loss: 0.8580 - accuracy: 0.9429 - val_loss: 0.6000 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 60/100\n",
      "Learning rate:  0.00059\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7793 - accuracy: 0.9143 - val_loss: 0.5995 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 61/100\n",
      "Learning rate:  0.0006\n",
      "2/2 [==============================] - 2s 467ms/step - loss: 0.7412 - accuracy: 0.9714 - val_loss: 0.5987 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 62/100\n",
      "Learning rate:  0.00061\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7392 - accuracy: 0.9857 - val_loss: 0.5974 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 63/100\n",
      "Learning rate:  0.00062\n",
      "2/2 [==============================] - 2s 370ms/step - loss: 0.7428 - accuracy: 0.9571 - val_loss: 0.5963 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 64/100\n",
      "Learning rate:  0.00063\n",
      "2/2 [==============================] - 2s 380ms/step - loss: 0.8183 - accuracy: 0.9143 - val_loss: 0.5982 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 65/100\n",
      "Learning rate:  0.00064\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7774 - accuracy: 0.9429 - val_loss: 0.6124 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 66/100\n",
      "Learning rate:  0.0006500000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7889 - accuracy: 0.9429 - val_loss: 0.6622 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 67/100\n",
      "Learning rate:  0.00066\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.7742 - accuracy: 0.9286 - val_loss: 0.6530 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 68/100\n",
      "Learning rate:  0.00067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 466ms/step - loss: 0.8013 - accuracy: 0.9143 - val_loss: 0.6230 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 69/100\n",
      "Learning rate:  0.00068\n",
      "2/2 [==============================] - 3s 510ms/step - loss: 0.6986 - accuracy: 1.0000 - val_loss: 0.6051 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 70/100\n",
      "Learning rate:  0.00069\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7540 - accuracy: 0.9286 - val_loss: 0.5969 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 71/100\n",
      "Learning rate:  0.0007\n",
      "2/2 [==============================] - 2s 518ms/step - loss: 0.7350 - accuracy: 0.9571 - val_loss: 0.5930 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 72/100\n",
      "Learning rate:  0.00071\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7413 - accuracy: 0.9571 - val_loss: 0.5912 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 73/100\n",
      "Learning rate:  0.0007199999999999999\n",
      "2/2 [==============================] - 2s 501ms/step - loss: 0.7104 - accuracy: 0.9571 - val_loss: 0.5907 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 74/100\n",
      "Learning rate:  0.00073\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7078 - accuracy: 0.9714 - val_loss: 0.5912 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 75/100\n",
      "Learning rate:  0.00074\n",
      "2/2 [==============================] - 2s 427ms/step - loss: 0.6843 - accuracy: 0.9857 - val_loss: 0.5923 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 76/100\n",
      "Learning rate:  0.00075\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.6755 - accuracy: 0.9857 - val_loss: 0.5951 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 77/100\n",
      "Learning rate:  0.00076\n",
      "2/2 [==============================] - 2s 327ms/step - loss: 0.6999 - accuracy: 0.9714 - val_loss: 0.5956 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 78/100\n",
      "Learning rate:  0.0007700000000000001\n",
      "2/2 [==============================] - 2s 453ms/step - loss: 0.6759 - accuracy: 0.9714 - val_loss: 0.5922 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 79/100\n",
      "Learning rate:  0.0007800000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7145 - accuracy: 0.9429 - val_loss: 0.5890 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 80/100\n",
      "Learning rate:  0.00079\n",
      "2/2 [==============================] - 2s 393ms/step - loss: 0.6670 - accuracy: 0.9857 - val_loss: 0.5871 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 81/100\n",
      "Learning rate:  0.0008\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.6745 - accuracy: 1.0000 - val_loss: 0.5857 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 82/100\n",
      "Learning rate:  8.1e-05\n",
      "2/2 [==============================] - 2s 325ms/step - loss: 0.7874 - accuracy: 0.9571 - val_loss: 0.5856 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 83/100\n",
      "Learning rate:  8.2e-05\n",
      "2/2 [==============================] - 2s 326ms/step - loss: 0.6824 - accuracy: 0.9857 - val_loss: 0.5855 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 84/100\n",
      "Learning rate:  8.3e-05\n",
      "2/2 [==============================] - 2s 319ms/step - loss: 0.8040 - accuracy: 0.8857 - val_loss: 0.5854 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 85/100\n",
      "Learning rate:  8.4e-05\n",
      "2/2 [==============================] - 3s 741ms/step - loss: 0.6663 - accuracy: 0.9857 - val_loss: 0.5854 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 86/100\n",
      "Learning rate:  8.5e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6989 - accuracy: 0.9571 - val_loss: 0.5854 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 87/100\n",
      "Learning rate:  8.6e-05\n",
      "2/2 [==============================] - 2s 492ms/step - loss: 0.7622 - accuracy: 0.9429 - val_loss: 0.5854 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 88/100\n",
      "Learning rate:  8.7e-05\n",
      "2/2 [==============================] - 3s 666ms/step - loss: 0.6532 - accuracy: 0.9857 - val_loss: 0.5854 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 89/100\n",
      "Learning rate:  8.800000000000001e-05\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.6584 - accuracy: 0.9857 - val_loss: 0.5855 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 90/100\n",
      "Learning rate:  8.900000000000001e-05\n",
      "2/2 [==============================] - 3s 3s/step - loss: 0.6531 - accuracy: 0.9857 - val_loss: 0.5856 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 91/100\n",
      "Learning rate:  9e-05\n",
      "2/2 [==============================] - 2s 545ms/step - loss: 0.6409 - accuracy: 1.0000 - val_loss: 0.5856 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 92/100\n",
      "Learning rate:  9.1e-05\n",
      "2/2 [==============================] - 3s 666ms/step - loss: 0.6603 - accuracy: 0.9857 - val_loss: 0.5857 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 93/100\n",
      "Learning rate:  9.200000000000001e-05\n",
      "2/2 [==============================] - 2s 405ms/step - loss: 0.6528 - accuracy: 1.0000 - val_loss: 0.5859 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 94/100\n",
      "Learning rate:  9.300000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6290 - accuracy: 1.0000 - val_loss: 0.5860 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 95/100\n",
      "Learning rate:  9.4e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6372 - accuracy: 0.9857 - val_loss: 0.5861 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 96/100\n",
      "Learning rate:  9.5e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6649 - accuracy: 0.9857 - val_loss: 0.5862 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 97/100\n",
      "Learning rate:  9.6e-05\n",
      "2/2 [==============================] - 2s 483ms/step - loss: 0.6336 - accuracy: 1.0000 - val_loss: 0.5864 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 98/100\n",
      "Learning rate:  9.7e-05\n",
      "2/2 [==============================] - 2s 384ms/step - loss: 0.6486 - accuracy: 0.9857 - val_loss: 0.5867 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 99/100\n",
      "Learning rate:  9.8e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6329 - accuracy: 1.0000 - val_loss: 0.5871 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 100/100\n",
      "Learning rate:  9.900000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7278 - accuracy: 0.9429 - val_loss: 0.5869 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 1.4174 - accuracy: 0.8333\n",
      "Test loss: 1.4174115657806396\n",
      "Test accuracy: 0.8333333134651184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision:  0.8888888888888888\n",
      "Recall:  0.8333333333333334\n",
      "F1 Score:  0.8380952380952381\n",
      "29\n",
      "29\n",
      "[[[[  9   9   2 ...   9   9   2]\n",
      "   [  6   6   2 ...   6   6   2]\n",
      "   [  4   3   1 ...   4   3   1]\n",
      "   ...\n",
      "   [  0   1   1 ...   0   1   1]\n",
      "   [  1   2   2 ...   1   2   2]\n",
      "   [  9  10   7 ...   9  10   7]]\n",
      "\n",
      "  [[  9   8   4 ...   9   8   4]\n",
      "   [  9   9   4 ...   9   9   4]\n",
      "   [  9   8   3 ...   9   8   3]\n",
      "   ...\n",
      "   [  1   1   1 ...   1   1   1]\n",
      "   [  4   4   3 ...   4   4   3]\n",
      "   [  4   5   3 ...   4   5   3]]\n",
      "\n",
      "  [[ 26  23  14 ...  26  23  14]\n",
      "   [ 33  27  15 ...  33  27  15]\n",
      "   [ 45  37  24 ...  45  37  24]\n",
      "   ...\n",
      "   [  2   3   2 ...   2   3   2]\n",
      "   [  2   2   2 ...   2   2   2]\n",
      "   [  4   4   3 ...   4   4   3]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[188 178 161 ... 188 178 161]\n",
      "   [190 180 162 ... 190 180 162]\n",
      "   [187 177 158 ... 187 177 158]\n",
      "   ...\n",
      "   [150 142 124 ... 150 142 124]\n",
      "   [145 135 118 ... 145 135 118]\n",
      "   [145 135 117 ... 145 135 117]]\n",
      "\n",
      "  [[197 186 170 ... 197 186 170]\n",
      "   [194 184 168 ... 194 184 168]\n",
      "   [192 181 164 ... 192 181 164]\n",
      "   ...\n",
      "   [148 139 120 ... 148 139 120]\n",
      "   [149 138 119 ... 149 138 119]\n",
      "   [147 136 117 ... 147 136 117]]\n",
      "\n",
      "  [[201 191 174 ... 201 191 174]\n",
      "   [198 188 172 ... 198 188 172]\n",
      "   [194 183 167 ... 194 183 167]\n",
      "   ...\n",
      "   [150 139 120 ... 150 139 120]\n",
      "   [152 141 121 ... 152 141 121]\n",
      "   [152 142 124 ... 152 142 124]]]\n",
      "\n",
      "\n",
      " [[[255 255 255 ... 255 255 255]\n",
      "   [243 246 242 ... 243 246 242]\n",
      "   [214 220 205 ... 214 220 205]\n",
      "   ...\n",
      "   [106 120  98 ... 106 120  98]\n",
      "   [210 215 205 ... 210 215 205]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [246 248 245 ... 246 248 245]\n",
      "   [217 226 213 ... 217 226 213]\n",
      "   ...\n",
      "   [108 123  99 ... 108 123  99]\n",
      "   [211 216 206 ... 211 216 206]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [248 251 248 ... 248 251 248]\n",
      "   [221 232 221 ... 221 232 221]\n",
      "   ...\n",
      "   [110 125 100 ... 110 125 100]\n",
      "   [212 217 206 ... 212 217 206]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [249 250 249 ... 249 250 249]\n",
      "   [232 229 221 ... 232 229 221]\n",
      "   ...\n",
      "   [ 86 101  95 ...  86 101  95]\n",
      "   [204 210 208 ... 204 210 208]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [250 250 249 ... 250 250 249]\n",
      "   [234 236 231 ... 234 236 231]\n",
      "   ...\n",
      "   [102 111 101 ... 102 111 101]\n",
      "   [202 208 207 ... 202 208 207]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [250 250 249 ... 250 250 249]\n",
      "   [237 240 235 ... 237 240 235]\n",
      "   ...\n",
      "   [128 125 110 ... 128 125 110]\n",
      "   [199 206 205 ... 199 206 205]\n",
      "   [255 255 255 ... 255 255 255]]]\n",
      "\n",
      "\n",
      " [[[ 93  64  41 ...  93  64  41]\n",
      "   [101  73  50 ... 101  73  50]\n",
      "   [109  83  61 ... 109  83  61]\n",
      "   ...\n",
      "   [ 92  57  35 ...  92  57  35]\n",
      "   [ 88  52  30 ...  88  52  30]\n",
      "   [ 83  47  26 ...  83  47  26]]\n",
      "\n",
      "  [[101  73  50 ... 101  73  50]\n",
      "   [109  82  60 ... 109  82  60]\n",
      "   [116  92  70 ... 116  92  70]\n",
      "   ...\n",
      "   [ 97  62  39 ...  97  62  39]\n",
      "   [ 92  56  34 ...  92  56  34]\n",
      "   [ 87  51  30 ...  87  51  30]]\n",
      "\n",
      "  [[108  82  59 ... 108  82  59]\n",
      "   [115  91  70 ... 115  91  70]\n",
      "   [122 100  81 ... 122 100  81]\n",
      "   ...\n",
      "   [101  66  44 ... 101  66  44]\n",
      "   [ 96  60  38 ...  96  60  38]\n",
      "   [ 91  54  33 ...  91  54  33]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 87  89  90 ...  87  89  90]\n",
      "   [ 98 100 102 ...  98 100 102]\n",
      "   [112 115 118 ... 112 115 118]\n",
      "   ...\n",
      "   [242 187 163 ... 242 187 163]\n",
      "   [230 169 145 ... 230 169 145]\n",
      "   [209 149 124 ... 209 149 124]]\n",
      "\n",
      "  [[ 81  83  83 ...  81  83  83]\n",
      "   [ 87  89  91 ...  87  89  91]\n",
      "   [106 108 111 ... 106 108 111]\n",
      "   ...\n",
      "   [235 180 155 ... 235 180 155]\n",
      "   [224 167 141 ... 224 167 141]\n",
      "   [204 143 118 ... 204 143 118]]\n",
      "\n",
      "  [[ 78  80  80 ...  78  80  80]\n",
      "   [ 79  80  82 ...  79  80  82]\n",
      "   [ 94  96  99 ...  94  96  99]\n",
      "   ...\n",
      "   [229 174 149 ... 229 174 149]\n",
      "   [218 160 135 ... 218 160 135]\n",
      "   [197 137 113 ... 197 137 113]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (73, 40, 30, 9)\n",
      "73 train samples\n",
      "28 test samples\n",
      "y_train shape: (73, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_558 (Conv2D)             (None, 40, 30, 16)   1312        input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_504 (BatchN (None, 40, 30, 16)   64          conv2d_558[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_504 (Activation)     (None, 40, 30, 16)   0           batch_normalization_504[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_559 (Conv2D)             (None, 40, 30, 16)   272         activation_504[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_505 (BatchN (None, 40, 30, 16)   64          conv2d_559[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_505 (Activation)     (None, 40, 30, 16)   0           batch_normalization_505[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_560 (Conv2D)             (None, 40, 30, 16)   2320        activation_505[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_506 (BatchN (None, 40, 30, 16)   64          conv2d_560[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_506 (Activation)     (None, 40, 30, 16)   0           batch_normalization_506[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_562 (Conv2D)             (None, 40, 30, 64)   1088        activation_504[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_561 (Conv2D)             (None, 40, 30, 64)   1088        activation_506[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_162 (Add)                   (None, 40, 30, 64)   0           conv2d_562[0][0]                 \n",
      "                                                                 conv2d_561[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_507 (BatchN (None, 40, 30, 64)   256         add_162[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_507 (Activation)     (None, 40, 30, 64)   0           batch_normalization_507[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_563 (Conv2D)             (None, 40, 30, 16)   1040        activation_507[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_508 (BatchN (None, 40, 30, 16)   64          conv2d_563[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_508 (Activation)     (None, 40, 30, 16)   0           batch_normalization_508[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_564 (Conv2D)             (None, 40, 30, 16)   2320        activation_508[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_509 (BatchN (None, 40, 30, 16)   64          conv2d_564[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_509 (Activation)     (None, 40, 30, 16)   0           batch_normalization_509[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_565 (Conv2D)             (None, 40, 30, 64)   1088        activation_509[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_163 (Add)                   (None, 40, 30, 64)   0           add_162[0][0]                    \n",
      "                                                                 conv2d_565[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_510 (BatchN (None, 40, 30, 64)   256         add_163[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_510 (Activation)     (None, 40, 30, 64)   0           batch_normalization_510[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_566 (Conv2D)             (None, 40, 30, 16)   1040        activation_510[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_511 (BatchN (None, 40, 30, 16)   64          conv2d_566[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_511 (Activation)     (None, 40, 30, 16)   0           batch_normalization_511[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_567 (Conv2D)             (None, 40, 30, 16)   2320        activation_511[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_512 (BatchN (None, 40, 30, 16)   64          conv2d_567[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_512 (Activation)     (None, 40, 30, 16)   0           batch_normalization_512[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_568 (Conv2D)             (None, 40, 30, 64)   1088        activation_512[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_164 (Add)                   (None, 40, 30, 64)   0           add_163[0][0]                    \n",
      "                                                                 conv2d_568[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_513 (BatchN (None, 40, 30, 64)   256         add_164[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_513 (Activation)     (None, 40, 30, 64)   0           batch_normalization_513[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_569 (Conv2D)             (None, 20, 15, 64)   4160        activation_513[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_514 (BatchN (None, 20, 15, 64)   256         conv2d_569[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_514 (Activation)     (None, 20, 15, 64)   0           batch_normalization_514[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_570 (Conv2D)             (None, 20, 15, 64)   36928       activation_514[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_515 (BatchN (None, 20, 15, 64)   256         conv2d_570[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_515 (Activation)     (None, 20, 15, 64)   0           batch_normalization_515[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_572 (Conv2D)             (None, 20, 15, 128)  8320        add_164[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_571 (Conv2D)             (None, 20, 15, 128)  8320        activation_515[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_165 (Add)                   (None, 20, 15, 128)  0           conv2d_572[0][0]                 \n",
      "                                                                 conv2d_571[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_516 (BatchN (None, 20, 15, 128)  512         add_165[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_516 (Activation)     (None, 20, 15, 128)  0           batch_normalization_516[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_573 (Conv2D)             (None, 20, 15, 64)   8256        activation_516[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_517 (BatchN (None, 20, 15, 64)   256         conv2d_573[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_517 (Activation)     (None, 20, 15, 64)   0           batch_normalization_517[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_574 (Conv2D)             (None, 20, 15, 64)   36928       activation_517[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_518 (BatchN (None, 20, 15, 64)   256         conv2d_574[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_518 (Activation)     (None, 20, 15, 64)   0           batch_normalization_518[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_575 (Conv2D)             (None, 20, 15, 128)  8320        activation_518[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_166 (Add)                   (None, 20, 15, 128)  0           add_165[0][0]                    \n",
      "                                                                 conv2d_575[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_519 (BatchN (None, 20, 15, 128)  512         add_166[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_519 (Activation)     (None, 20, 15, 128)  0           batch_normalization_519[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_576 (Conv2D)             (None, 20, 15, 64)   8256        activation_519[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_520 (BatchN (None, 20, 15, 64)   256         conv2d_576[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_520 (Activation)     (None, 20, 15, 64)   0           batch_normalization_520[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_577 (Conv2D)             (None, 20, 15, 64)   36928       activation_520[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_521 (BatchN (None, 20, 15, 64)   256         conv2d_577[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_521 (Activation)     (None, 20, 15, 64)   0           batch_normalization_521[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_578 (Conv2D)             (None, 20, 15, 128)  8320        activation_521[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_167 (Add)                   (None, 20, 15, 128)  0           add_166[0][0]                    \n",
      "                                                                 conv2d_578[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_522 (BatchN (None, 20, 15, 128)  512         add_167[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_522 (Activation)     (None, 20, 15, 128)  0           batch_normalization_522[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_579 (Conv2D)             (None, 10, 8, 128)   16512       activation_522[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_523 (BatchN (None, 10, 8, 128)   512         conv2d_579[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_523 (Activation)     (None, 10, 8, 128)   0           batch_normalization_523[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_580 (Conv2D)             (None, 10, 8, 128)   147584      activation_523[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_524 (BatchN (None, 10, 8, 128)   512         conv2d_580[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_524 (Activation)     (None, 10, 8, 128)   0           batch_normalization_524[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_582 (Conv2D)             (None, 10, 8, 256)   33024       add_167[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_581 (Conv2D)             (None, 10, 8, 256)   33024       activation_524[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_168 (Add)                   (None, 10, 8, 256)   0           conv2d_582[0][0]                 \n",
      "                                                                 conv2d_581[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_525 (BatchN (None, 10, 8, 256)   1024        add_168[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_525 (Activation)     (None, 10, 8, 256)   0           batch_normalization_525[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_583 (Conv2D)             (None, 10, 8, 128)   32896       activation_525[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_526 (BatchN (None, 10, 8, 128)   512         conv2d_583[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_526 (Activation)     (None, 10, 8, 128)   0           batch_normalization_526[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_584 (Conv2D)             (None, 10, 8, 128)   147584      activation_526[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_527 (BatchN (None, 10, 8, 128)   512         conv2d_584[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_527 (Activation)     (None, 10, 8, 128)   0           batch_normalization_527[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_585 (Conv2D)             (None, 10, 8, 256)   33024       activation_527[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_169 (Add)                   (None, 10, 8, 256)   0           add_168[0][0]                    \n",
      "                                                                 conv2d_585[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_528 (BatchN (None, 10, 8, 256)   1024        add_169[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_528 (Activation)     (None, 10, 8, 256)   0           batch_normalization_528[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_586 (Conv2D)             (None, 10, 8, 128)   32896       activation_528[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_529 (BatchN (None, 10, 8, 128)   512         conv2d_586[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_529 (Activation)     (None, 10, 8, 128)   0           batch_normalization_529[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_587 (Conv2D)             (None, 10, 8, 128)   147584      activation_529[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_530 (BatchN (None, 10, 8, 128)   512         conv2d_587[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_530 (Activation)     (None, 10, 8, 128)   0           batch_normalization_530[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_588 (Conv2D)             (None, 10, 8, 256)   33024       activation_530[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_170 (Add)                   (None, 10, 8, 256)   0           add_169[0][0]                    \n",
      "                                                                 conv2d_588[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_531 (BatchN (None, 10, 8, 256)   1024        add_170[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_531 (Activation)     (None, 10, 8, 256)   0           batch_normalization_531[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_18 (AveragePo (None, 1, 1, 256)    0           activation_531[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 256)          0           average_pooling2d_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 10)           2570        flatten_18[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[0.03529412 0.03529412 0.00784314 ... 0.03529412 0.03529412\n",
      "    0.00784314]\n",
      "   [0.02352941 0.02352941 0.00784314 ... 0.02352941 0.02352941\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01176471 0.00392157 ... 0.01568628 0.01176471\n",
      "    0.00392157]\n",
      "   ...\n",
      "   [0.         0.00392157 0.00392157 ... 0.         0.00392157\n",
      "    0.00392157]\n",
      "   [0.00392157 0.00784314 0.00784314 ... 0.00392157 0.00784314\n",
      "    0.00784314]\n",
      "   [0.03529412 0.03921569 0.02745098 ... 0.03529412 0.03921569\n",
      "    0.02745098]]\n",
      "\n",
      "  [[0.03529412 0.03137255 0.01568628 ... 0.03529412 0.03137255\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03529412 0.01568628 ... 0.03529412 0.03529412\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03137255 0.01176471 ... 0.03529412 0.03137255\n",
      "    0.01176471]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157 ... 0.00392157 0.00392157\n",
      "    0.00392157]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]\n",
      "   [0.01568628 0.01960784 0.01176471 ... 0.01568628 0.01960784\n",
      "    0.01176471]]\n",
      "\n",
      "  [[0.10196079 0.09019608 0.05490196 ... 0.10196079 0.09019608\n",
      "    0.05490196]\n",
      "   [0.12941177 0.10588235 0.05882353 ... 0.12941177 0.10588235\n",
      "    0.05882353]\n",
      "   [0.1764706  0.14509805 0.09411765 ... 0.1764706  0.14509805\n",
      "    0.09411765]\n",
      "   ...\n",
      "   [0.00784314 0.01176471 0.00784314 ... 0.00784314 0.01176471\n",
      "    0.00784314]\n",
      "   [0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.7372549  0.69803923 0.6313726  ... 0.7372549  0.69803923\n",
      "    0.6313726 ]\n",
      "   [0.74509805 0.7058824  0.63529414 ... 0.74509805 0.7058824\n",
      "    0.63529414]\n",
      "   [0.73333335 0.69411767 0.61960787 ... 0.73333335 0.69411767\n",
      "    0.61960787]\n",
      "   ...\n",
      "   [0.5882353  0.5568628  0.4862745  ... 0.5882353  0.5568628\n",
      "    0.4862745 ]\n",
      "   [0.5686275  0.5294118  0.4627451  ... 0.5686275  0.5294118\n",
      "    0.4627451 ]\n",
      "   [0.5686275  0.5294118  0.45882353 ... 0.5686275  0.5294118\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.77254903 0.7294118  0.6666667  ... 0.77254903 0.7294118\n",
      "    0.6666667 ]\n",
      "   [0.7607843  0.72156864 0.65882355 ... 0.7607843  0.72156864\n",
      "    0.65882355]\n",
      "   [0.7529412  0.70980394 0.6431373  ... 0.7529412  0.70980394\n",
      "    0.6431373 ]\n",
      "   ...\n",
      "   [0.5803922  0.54509807 0.47058824 ... 0.5803922  0.54509807\n",
      "    0.47058824]\n",
      "   [0.58431375 0.5411765  0.46666667 ... 0.58431375 0.5411765\n",
      "    0.46666667]\n",
      "   [0.5764706  0.53333336 0.45882353 ... 0.5764706  0.53333336\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.7882353  0.7490196  0.68235296 ... 0.7882353  0.7490196\n",
      "    0.68235296]\n",
      "   [0.7764706  0.7372549  0.6745098  ... 0.7764706  0.7372549\n",
      "    0.6745098 ]\n",
      "   [0.7607843  0.7176471  0.654902   ... 0.7607843  0.7176471\n",
      "    0.654902  ]\n",
      "   ...\n",
      "   [0.5882353  0.54509807 0.47058824 ... 0.5882353  0.54509807\n",
      "    0.47058824]\n",
      "   [0.59607846 0.5529412  0.4745098  ... 0.59607846 0.5529412\n",
      "    0.4745098 ]\n",
      "   [0.59607846 0.5568628  0.4862745  ... 0.59607846 0.5568628\n",
      "    0.4862745 ]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9529412  0.9647059  0.9490196  ... 0.9529412  0.9647059\n",
      "    0.9490196 ]\n",
      "   [0.8392157  0.8627451  0.8039216  ... 0.8392157  0.8627451\n",
      "    0.8039216 ]\n",
      "   ...\n",
      "   [0.41568628 0.47058824 0.38431373 ... 0.41568628 0.47058824\n",
      "    0.38431373]\n",
      "   [0.8235294  0.84313726 0.8039216  ... 0.8235294  0.84313726\n",
      "    0.8039216 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9647059  0.972549   0.9607843  ... 0.9647059  0.972549\n",
      "    0.9607843 ]\n",
      "   [0.8509804  0.8862745  0.8352941  ... 0.8509804  0.8862745\n",
      "    0.8352941 ]\n",
      "   ...\n",
      "   [0.42352942 0.48235294 0.3882353  ... 0.42352942 0.48235294\n",
      "    0.3882353 ]\n",
      "   [0.827451   0.84705883 0.80784315 ... 0.827451   0.84705883\n",
      "    0.80784315]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.972549   0.9843137  0.972549   ... 0.972549   0.9843137\n",
      "    0.972549  ]\n",
      "   [0.8666667  0.9098039  0.8666667  ... 0.8666667  0.9098039\n",
      "    0.8666667 ]\n",
      "   ...\n",
      "   [0.43137255 0.49019608 0.39215687 ... 0.43137255 0.49019608\n",
      "    0.39215687]\n",
      "   [0.83137256 0.8509804  0.80784315 ... 0.83137256 0.8509804\n",
      "    0.80784315]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9764706  0.98039216 0.9764706  ... 0.9764706  0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.9098039  0.8980392  0.8666667  ... 0.9098039  0.8980392\n",
      "    0.8666667 ]\n",
      "   ...\n",
      "   [0.3372549  0.39607844 0.37254903 ... 0.3372549  0.39607844\n",
      "    0.37254903]\n",
      "   [0.8        0.8235294  0.8156863  ... 0.8        0.8235294\n",
      "    0.8156863 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.98039216 0.98039216 0.9764706  ... 0.98039216 0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.91764706 0.9254902  0.90588236 ... 0.91764706 0.9254902\n",
      "    0.90588236]\n",
      "   ...\n",
      "   [0.4        0.43529412 0.39607844 ... 0.4        0.43529412\n",
      "    0.39607844]\n",
      "   [0.7921569  0.8156863  0.8117647  ... 0.7921569  0.8156863\n",
      "    0.8117647 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.98039216 0.98039216 0.9764706  ... 0.98039216 0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.92941177 0.9411765  0.92156863 ... 0.92941177 0.9411765\n",
      "    0.92156863]\n",
      "   ...\n",
      "   [0.5019608  0.49019608 0.43137255 ... 0.5019608  0.49019608\n",
      "    0.43137255]\n",
      "   [0.78039217 0.80784315 0.8039216  ... 0.78039217 0.80784315\n",
      "    0.8039216 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]]\n",
      "\n",
      "\n",
      " [[[0.3647059  0.2509804  0.16078432 ... 0.3647059  0.2509804\n",
      "    0.16078432]\n",
      "   [0.39607844 0.28627452 0.19607843 ... 0.39607844 0.28627452\n",
      "    0.19607843]\n",
      "   [0.42745098 0.3254902  0.23921569 ... 0.42745098 0.3254902\n",
      "    0.23921569]\n",
      "   ...\n",
      "   [0.36078432 0.22352941 0.13725491 ... 0.36078432 0.22352941\n",
      "    0.13725491]\n",
      "   [0.34509805 0.20392157 0.11764706 ... 0.34509805 0.20392157\n",
      "    0.11764706]\n",
      "   [0.3254902  0.18431373 0.10196079 ... 0.3254902  0.18431373\n",
      "    0.10196079]]\n",
      "\n",
      "  [[0.39607844 0.28627452 0.19607843 ... 0.39607844 0.28627452\n",
      "    0.19607843]\n",
      "   [0.42745098 0.32156864 0.23529412 ... 0.42745098 0.32156864\n",
      "    0.23529412]\n",
      "   [0.45490196 0.36078432 0.27450982 ... 0.45490196 0.36078432\n",
      "    0.27450982]\n",
      "   ...\n",
      "   [0.38039216 0.24313726 0.15294118 ... 0.38039216 0.24313726\n",
      "    0.15294118]\n",
      "   [0.36078432 0.21960784 0.13333334 ... 0.36078432 0.21960784\n",
      "    0.13333334]\n",
      "   [0.34117648 0.2        0.11764706 ... 0.34117648 0.2\n",
      "    0.11764706]]\n",
      "\n",
      "  [[0.42352942 0.32156864 0.23137255 ... 0.42352942 0.32156864\n",
      "    0.23137255]\n",
      "   [0.4509804  0.35686275 0.27450982 ... 0.4509804  0.35686275\n",
      "    0.27450982]\n",
      "   [0.47843137 0.39215687 0.31764707 ... 0.47843137 0.39215687\n",
      "    0.31764707]\n",
      "   ...\n",
      "   [0.39607844 0.25882354 0.17254902 ... 0.39607844 0.25882354\n",
      "    0.17254902]\n",
      "   [0.3764706  0.23529412 0.14901961 ... 0.3764706  0.23529412\n",
      "    0.14901961]\n",
      "   [0.35686275 0.21176471 0.12941177 ... 0.35686275 0.21176471\n",
      "    0.12941177]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.34117648 0.34901962 0.3529412  ... 0.34117648 0.34901962\n",
      "    0.3529412 ]\n",
      "   [0.38431373 0.39215687 0.4        ... 0.38431373 0.39215687\n",
      "    0.4       ]\n",
      "   [0.4392157  0.4509804  0.4627451  ... 0.4392157  0.4509804\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.9490196  0.73333335 0.6392157  ... 0.9490196  0.73333335\n",
      "    0.6392157 ]\n",
      "   [0.9019608  0.6627451  0.5686275  ... 0.9019608  0.6627451\n",
      "    0.5686275 ]\n",
      "   [0.81960785 0.58431375 0.4862745  ... 0.81960785 0.58431375\n",
      "    0.4862745 ]]\n",
      "\n",
      "  [[0.31764707 0.3254902  0.3254902  ... 0.31764707 0.3254902\n",
      "    0.3254902 ]\n",
      "   [0.34117648 0.34901962 0.35686275 ... 0.34117648 0.34901962\n",
      "    0.35686275]\n",
      "   [0.41568628 0.42352942 0.43529412 ... 0.41568628 0.42352942\n",
      "    0.43529412]\n",
      "   ...\n",
      "   [0.92156863 0.7058824  0.60784316 ... 0.92156863 0.7058824\n",
      "    0.60784316]\n",
      "   [0.8784314  0.654902   0.5529412  ... 0.8784314  0.654902\n",
      "    0.5529412 ]\n",
      "   [0.8        0.56078434 0.4627451  ... 0.8        0.56078434\n",
      "    0.4627451 ]]\n",
      "\n",
      "  [[0.30588236 0.3137255  0.3137255  ... 0.30588236 0.3137255\n",
      "    0.3137255 ]\n",
      "   [0.30980393 0.3137255  0.32156864 ... 0.30980393 0.3137255\n",
      "    0.32156864]\n",
      "   [0.36862746 0.3764706  0.3882353  ... 0.36862746 0.3764706\n",
      "    0.3882353 ]\n",
      "   ...\n",
      "   [0.8980392  0.68235296 0.58431375 ... 0.8980392  0.68235296\n",
      "    0.58431375]\n",
      "   [0.85490197 0.627451   0.5294118  ... 0.85490197 0.627451\n",
      "    0.5294118 ]\n",
      "   [0.77254903 0.5372549  0.44313726 ... 0.77254903 0.5372549\n",
      "    0.44313726]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.99215686 0.99215686 0.99215686 ... 0.99215686 0.99215686\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6509804  0.6117647  0.60784316 ... 0.6509804  0.6117647\n",
      "    0.60784316]\n",
      "   [0.6431373  0.6        0.5921569  ... 0.6431373  0.6\n",
      "    0.5921569 ]\n",
      "   [0.63529414 0.5921569  0.5803922  ... 0.63529414 0.5921569\n",
      "    0.5803922 ]\n",
      "   ...\n",
      "   [0.80784315 0.69803923 0.67058825 ... 0.80784315 0.69803923\n",
      "    0.67058825]\n",
      "   [0.80784315 0.7176471  0.6901961  ... 0.80784315 0.7176471\n",
      "    0.6901961 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]]\n",
      "\n",
      "  [[0.64705884 0.59607846 0.59607846 ... 0.64705884 0.59607846\n",
      "    0.59607846]\n",
      "   [0.63529414 0.5921569  0.58431375 ... 0.63529414 0.5921569\n",
      "    0.58431375]\n",
      "   [0.62352943 0.5803922  0.57254905 ... 0.62352943 0.5803922\n",
      "    0.57254905]\n",
      "   ...\n",
      "   [0.8        0.69803923 0.6745098  ... 0.8        0.69803923\n",
      "    0.6745098 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]\n",
      "   [0.7921569  0.7019608  0.68235296 ... 0.7921569  0.7019608\n",
      "    0.68235296]]\n",
      "\n",
      "  [[0.6431373  0.59607846 0.5921569  ... 0.6431373  0.59607846\n",
      "    0.5921569 ]\n",
      "   [0.6313726  0.5882353  0.5803922  ... 0.6313726  0.5882353\n",
      "    0.5803922 ]\n",
      "   [0.627451   0.57254905 0.5686275  ... 0.627451   0.57254905\n",
      "    0.5686275 ]\n",
      "   ...\n",
      "   [0.7921569  0.6901961  0.6784314  ... 0.7921569  0.6901961\n",
      "    0.6784314 ]\n",
      "   [0.80784315 0.7058824  0.6901961  ... 0.80784315 0.7058824\n",
      "    0.6901961 ]\n",
      "   [0.80784315 0.7176471  0.7019608  ... 0.80784315 0.7176471\n",
      "    0.7019608 ]]]\n",
      "\n",
      "\n",
      " [[[0.04705882 0.04705882 0.04705882 ... 0.04705882 0.04705882\n",
      "    0.04705882]\n",
      "   [0.01176471 0.01176471 0.01176471 ... 0.01176471 0.01176471\n",
      "    0.01176471]\n",
      "   [0.03529412 0.03529412 0.03529412 ... 0.03529412 0.03529412\n",
      "    0.03529412]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.01176471 0.01176471 0.01176471 ... 0.01176471 0.01176471\n",
      "    0.01176471]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.49803922 0.49803922 0.5176471  ... 0.49803922 0.49803922\n",
      "    0.5176471 ]\n",
      "   [0.5058824  0.5019608  0.52156866 ... 0.5058824  0.5019608\n",
      "    0.52156866]\n",
      "   [0.50980395 0.5058824  0.5254902  ... 0.50980395 0.5058824\n",
      "    0.5254902 ]\n",
      "   ...\n",
      "   [0.5686275  0.57254905 0.5882353  ... 0.5686275  0.57254905\n",
      "    0.5882353 ]\n",
      "   [0.57254905 0.5764706  0.5921569  ... 0.57254905 0.5764706\n",
      "    0.5921569 ]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]]\n",
      "\n",
      "  [[0.50980395 0.50980395 0.5254902  ... 0.50980395 0.50980395\n",
      "    0.5254902 ]\n",
      "   [0.5137255  0.5137255  0.5294118  ... 0.5137255  0.5137255\n",
      "    0.5294118 ]\n",
      "   [0.5176471  0.5176471  0.53333336 ... 0.5176471  0.5176471\n",
      "    0.53333336]\n",
      "   ...\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5803922  0.58431375 0.6        ... 0.5803922  0.58431375\n",
      "    0.6       ]]\n",
      "\n",
      "  [[0.5176471  0.5176471  0.5372549  ... 0.5176471  0.5176471\n",
      "    0.5372549 ]\n",
      "   [0.52156866 0.5254902  0.5411765  ... 0.52156866 0.5254902\n",
      "    0.5411765 ]\n",
      "   [0.5176471  0.52156866 0.5372549  ... 0.5176471  0.52156866\n",
      "    0.5372549 ]\n",
      "   ...\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.5882353  0.5921569  0.60784316 ... 0.5882353  0.5921569\n",
      "    0.60784316]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8901961  0.8392157  0.8039216  ... 0.8901961  0.8392157\n",
      "    0.8039216 ]\n",
      "   [0.91764706 0.8745098  0.8392157  ... 0.91764706 0.8745098\n",
      "    0.8392157 ]\n",
      "   [0.8627451  0.79607844 0.7490196  ... 0.8627451  0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9372549  0.90588236 0.8784314  ... 0.9372549  0.90588236\n",
      "    0.8784314 ]\n",
      "   [0.90588236 0.85882354 0.8235294  ... 0.90588236 0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.87058824 0.8117647  0.7764706  ... 0.87058824 0.8117647\n",
      "    0.7764706 ]]\n",
      "\n",
      "  [[0.88235295 0.8235294  0.78431374 ... 0.88235295 0.8235294\n",
      "    0.78431374]\n",
      "   [0.89411765 0.8392157  0.8        ... 0.89411765 0.8392157\n",
      "    0.8       ]\n",
      "   [0.87058824 0.8039216  0.7607843  ... 0.87058824 0.8039216\n",
      "    0.7607843 ]\n",
      "   ...\n",
      "   [0.92156863 0.8862745  0.8509804  ... 0.92156863 0.8862745\n",
      "    0.8509804 ]\n",
      "   [0.9098039  0.87058824 0.8352941  ... 0.9098039  0.87058824\n",
      "    0.8352941 ]\n",
      "   [0.8745098  0.8156863  0.77254903 ... 0.8745098  0.8156863\n",
      "    0.77254903]]\n",
      "\n",
      "  [[0.8627451  0.7921569  0.7411765  ... 0.8627451  0.7921569\n",
      "    0.7411765 ]\n",
      "   [0.8666667  0.8039216  0.7529412  ... 0.8666667  0.8039216\n",
      "    0.7529412 ]\n",
      "   [0.85882354 0.79607844 0.7490196  ... 0.85882354 0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9098039  0.85882354 0.8235294  ... 0.9098039  0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.90588236 0.85490197 0.8156863  ... 0.90588236 0.85490197\n",
      "    0.8156863 ]\n",
      "   [0.8784314  0.8156863  0.7647059  ... 0.8784314  0.8156863\n",
      "    0.7647059 ]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (73, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (73, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 2.4935 - accuracy: 0.287731\n",
      "31\n",
      "[[[[ 76  84  95 ...  76  84  95]\n",
      "   [ 15  15  20 ...  15  15  20]\n",
      "   [  8   7  11 ...   8   7  11]\n",
      "   ...\n",
      "   [ 16  14  25 ...  16  14  25]\n",
      "   [ 13  13  22 ...  13  13  22]\n",
      "   [  8   9  13 ...   8   9  13]]\n",
      "\n",
      "  [[ 88  96 107 ...  88  96 107]\n",
      "   [ 17  17  22 ...  17  17  22]\n",
      "   [  6   5   9 ...   6   5   9]\n",
      "   ...\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 15  13  24 ...  15  13  24]\n",
      "   [ 10  10  16 ...  10  10  16]]\n",
      "\n",
      "  [[ 86  91  98 ...  86  91  98]\n",
      "   [ 14  13  17 ...  14  13  17]\n",
      "   [  5   4   7 ...   5   4   7]\n",
      "   ...\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 12  11  18 ...  12  11  18]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[157 162 174 ... 157 162 174]\n",
      "   [159 165 176 ... 159 165 176]\n",
      "   [155 158 159 ... 155 158 159]\n",
      "   ...\n",
      "   [126 136 168 ... 126 136 168]\n",
      "   [145 153 178 ... 145 153 178]\n",
      "   [150 157 180 ... 150 157 180]]\n",
      "\n",
      "  [[158 162 174 ... 158 162 174]\n",
      "   [158 164 177 ... 158 164 177]\n",
      "   [160 167 179 ... 160 167 179]\n",
      "   ...\n",
      "   [134 144 172 ... 134 144 172]\n",
      "   [143 151 177 ... 143 151 177]\n",
      "   [151 158 180 ... 151 158 180]]\n",
      "\n",
      "  [[160 164 176 ... 160 164 176]\n",
      "   [158 164 177 ... 158 164 177]\n",
      "   [163 169 181 ... 163 169 181]\n",
      "   ...\n",
      "   [141 150 174 ... 141 150 174]\n",
      "   [140 149 176 ... 140 149 176]\n",
      "   [150 158 180 ... 150 158 180]]]\n",
      "\n",
      "\n",
      " [[[ 30  95 105 ...  30  95 105]\n",
      "   [ 31  97 107 ...  31  97 107]\n",
      "   [ 30  96 106 ...  30  96 106]\n",
      "   ...\n",
      "   [ 26  91 103 ...  26  91 103]\n",
      "   [ 25  92 102 ...  25  92 102]\n",
      "   [ 24  91 101 ...  24  91 101]]\n",
      "\n",
      "  [[ 27 111 122 ...  27 111 122]\n",
      "   [ 27 111 123 ...  27 111 123]\n",
      "   [ 28 111 123 ...  28 111 123]\n",
      "   ...\n",
      "   [ 22 107 119 ...  22 107 119]\n",
      "   [ 22 108 118 ...  22 108 118]\n",
      "   [ 23 108 119 ...  23 108 119]]\n",
      "\n",
      "  [[ 24 105 116 ...  24 105 116]\n",
      "   [ 25 107 118 ...  25 107 118]\n",
      "   [ 28 108 119 ...  28 108 119]\n",
      "   ...\n",
      "   [ 26 107 117 ...  26 107 117]\n",
      "   [ 23 105 115 ...  23 105 115]\n",
      "   [ 22 104 116 ...  22 104 116]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  7  29  34 ...   7  29  34]\n",
      "   [ 87  67  56 ...  87  67  56]\n",
      "   [179 128 119 ... 179 128 119]\n",
      "   ...\n",
      "   [195 132 133 ... 195 132 133]\n",
      "   [119  77  72 ... 119  77  72]\n",
      "   [ 50  35  35 ...  50  35  35]]\n",
      "\n",
      "  [[  1  41  49 ...   1  41  49]\n",
      "   [ 72  61  52 ...  72  61  52]\n",
      "   [169 118 107 ... 169 118 107]\n",
      "   ...\n",
      "   [186 131 126 ... 186 131 126]\n",
      "   [ 97  65  58 ...  97  65  58]\n",
      "   [ 30  27  31 ...  30  27  31]]\n",
      "\n",
      "  [[  5  56  65 ...   5  56  65]\n",
      "   [ 63  60  53 ...  63  60  53]\n",
      "   [161 113 103 ... 161 113 103]\n",
      "   ...\n",
      "   [176 123 117 ... 176 123 117]\n",
      "   [ 76  58  54 ...  76  58  54]\n",
      "   [ 19  35  42 ...  19  35  42]]]\n",
      "\n",
      "\n",
      " [[[ 76  71  73 ...  76  71  73]\n",
      "   [ 75  73  74 ...  75  73  74]\n",
      "   [ 76  74  74 ...  76  74  74]\n",
      "   ...\n",
      "   [ 84  76  73 ...  84  76  73]\n",
      "   [ 85  79  75 ...  85  79  75]\n",
      "   [ 91  84  79 ...  91  84  79]]\n",
      "\n",
      "  [[ 73  71  72 ...  73  71  72]\n",
      "   [ 72  69  70 ...  72  69  70]\n",
      "   [ 74  69  71 ...  74  69  71]\n",
      "   ...\n",
      "   [ 82  76  73 ...  82  76  73]\n",
      "   [ 87  79  75 ...  87  79  75]\n",
      "   [ 93  87  81 ...  93  87  81]]\n",
      "\n",
      "  [[ 72  71  71 ...  72  71  71]\n",
      "   [ 71  69  70 ...  71  69  70]\n",
      "   [ 76  71  72 ...  76  71  72]\n",
      "   ...\n",
      "   [ 83  75  72 ...  83  75  72]\n",
      "   [ 85  76  74 ...  85  76  74]\n",
      "   [ 91  85  81 ...  91  85  81]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[137 117 115 ... 137 117 115]\n",
      "   [132 109 106 ... 132 109 106]\n",
      "   [132 109 107 ... 132 109 107]\n",
      "   ...\n",
      "   [ 82 129 143 ...  82 129 143]\n",
      "   [ 85 129 145 ...  85 129 145]\n",
      "   [ 84 125 139 ...  84 125 139]]\n",
      "\n",
      "  [[128 113 110 ... 128 113 110]\n",
      "   [126 110 107 ... 126 110 107]\n",
      "   [124 108 106 ... 124 108 106]\n",
      "   ...\n",
      "   [ 85 133 151 ...  85 133 151]\n",
      "   [ 83 129 146 ...  83 129 146]\n",
      "   [ 85 127 142 ...  85 127 142]]\n",
      "\n",
      "  [[127 111 107 ... 127 111 107]\n",
      "   [122 106 102 ... 122 106 102]\n",
      "   [120 105 101 ... 120 105 101]\n",
      "   ...\n",
      "   [ 90 136 155 ...  90 136 155]\n",
      "   [ 87 131 148 ...  87 131 148]\n",
      "   [ 89 128 142 ...  89 128 142]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (69, 40, 30, 9)\n",
      "69 train samples\n",
      "30 test samples\n",
      "y_train shape: (69, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_589 (Conv2D)             (None, 40, 30, 16)   1312        input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_532 (BatchN (None, 40, 30, 16)   64          conv2d_589[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_532 (Activation)     (None, 40, 30, 16)   0           batch_normalization_532[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_590 (Conv2D)             (None, 40, 30, 16)   272         activation_532[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_533 (BatchN (None, 40, 30, 16)   64          conv2d_590[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_533 (Activation)     (None, 40, 30, 16)   0           batch_normalization_533[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_591 (Conv2D)             (None, 40, 30, 16)   2320        activation_533[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_534 (BatchN (None, 40, 30, 16)   64          conv2d_591[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_534 (Activation)     (None, 40, 30, 16)   0           batch_normalization_534[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_593 (Conv2D)             (None, 40, 30, 64)   1088        activation_532[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_592 (Conv2D)             (None, 40, 30, 64)   1088        activation_534[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_171 (Add)                   (None, 40, 30, 64)   0           conv2d_593[0][0]                 \n",
      "                                                                 conv2d_592[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_535 (BatchN (None, 40, 30, 64)   256         add_171[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_535 (Activation)     (None, 40, 30, 64)   0           batch_normalization_535[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_594 (Conv2D)             (None, 40, 30, 16)   1040        activation_535[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_536 (BatchN (None, 40, 30, 16)   64          conv2d_594[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_536 (Activation)     (None, 40, 30, 16)   0           batch_normalization_536[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_595 (Conv2D)             (None, 40, 30, 16)   2320        activation_536[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_537 (BatchN (None, 40, 30, 16)   64          conv2d_595[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_537 (Activation)     (None, 40, 30, 16)   0           batch_normalization_537[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_596 (Conv2D)             (None, 40, 30, 64)   1088        activation_537[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_172 (Add)                   (None, 40, 30, 64)   0           add_171[0][0]                    \n",
      "                                                                 conv2d_596[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_538 (BatchN (None, 40, 30, 64)   256         add_172[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_538 (Activation)     (None, 40, 30, 64)   0           batch_normalization_538[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_597 (Conv2D)             (None, 40, 30, 16)   1040        activation_538[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_539 (BatchN (None, 40, 30, 16)   64          conv2d_597[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_539 (Activation)     (None, 40, 30, 16)   0           batch_normalization_539[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_598 (Conv2D)             (None, 40, 30, 16)   2320        activation_539[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_540 (BatchN (None, 40, 30, 16)   64          conv2d_598[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_540 (Activation)     (None, 40, 30, 16)   0           batch_normalization_540[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_599 (Conv2D)             (None, 40, 30, 64)   1088        activation_540[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_173 (Add)                   (None, 40, 30, 64)   0           add_172[0][0]                    \n",
      "                                                                 conv2d_599[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_541 (BatchN (None, 40, 30, 64)   256         add_173[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_541 (Activation)     (None, 40, 30, 64)   0           batch_normalization_541[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_600 (Conv2D)             (None, 20, 15, 64)   4160        activation_541[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_542 (BatchN (None, 20, 15, 64)   256         conv2d_600[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_542 (Activation)     (None, 20, 15, 64)   0           batch_normalization_542[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_601 (Conv2D)             (None, 20, 15, 64)   36928       activation_542[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_543 (BatchN (None, 20, 15, 64)   256         conv2d_601[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_543 (Activation)     (None, 20, 15, 64)   0           batch_normalization_543[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_603 (Conv2D)             (None, 20, 15, 128)  8320        add_173[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_602 (Conv2D)             (None, 20, 15, 128)  8320        activation_543[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_174 (Add)                   (None, 20, 15, 128)  0           conv2d_603[0][0]                 \n",
      "                                                                 conv2d_602[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_544 (BatchN (None, 20, 15, 128)  512         add_174[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_544 (Activation)     (None, 20, 15, 128)  0           batch_normalization_544[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_604 (Conv2D)             (None, 20, 15, 64)   8256        activation_544[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_545 (BatchN (None, 20, 15, 64)   256         conv2d_604[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_545 (Activation)     (None, 20, 15, 64)   0           batch_normalization_545[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_605 (Conv2D)             (None, 20, 15, 64)   36928       activation_545[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_546 (BatchN (None, 20, 15, 64)   256         conv2d_605[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_546 (Activation)     (None, 20, 15, 64)   0           batch_normalization_546[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_606 (Conv2D)             (None, 20, 15, 128)  8320        activation_546[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_175 (Add)                   (None, 20, 15, 128)  0           add_174[0][0]                    \n",
      "                                                                 conv2d_606[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_547 (BatchN (None, 20, 15, 128)  512         add_175[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_547 (Activation)     (None, 20, 15, 128)  0           batch_normalization_547[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_607 (Conv2D)             (None, 20, 15, 64)   8256        activation_547[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_548 (BatchN (None, 20, 15, 64)   256         conv2d_607[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_548 (Activation)     (None, 20, 15, 64)   0           batch_normalization_548[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_608 (Conv2D)             (None, 20, 15, 64)   36928       activation_548[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_549 (BatchN (None, 20, 15, 64)   256         conv2d_608[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_549 (Activation)     (None, 20, 15, 64)   0           batch_normalization_549[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_609 (Conv2D)             (None, 20, 15, 128)  8320        activation_549[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_176 (Add)                   (None, 20, 15, 128)  0           add_175[0][0]                    \n",
      "                                                                 conv2d_609[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_550 (BatchN (None, 20, 15, 128)  512         add_176[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_550 (Activation)     (None, 20, 15, 128)  0           batch_normalization_550[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_610 (Conv2D)             (None, 10, 8, 128)   16512       activation_550[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_551 (BatchN (None, 10, 8, 128)   512         conv2d_610[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_551 (Activation)     (None, 10, 8, 128)   0           batch_normalization_551[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_611 (Conv2D)             (None, 10, 8, 128)   147584      activation_551[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_552 (BatchN (None, 10, 8, 128)   512         conv2d_611[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_552 (Activation)     (None, 10, 8, 128)   0           batch_normalization_552[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_613 (Conv2D)             (None, 10, 8, 256)   33024       add_176[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_612 (Conv2D)             (None, 10, 8, 256)   33024       activation_552[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_177 (Add)                   (None, 10, 8, 256)   0           conv2d_613[0][0]                 \n",
      "                                                                 conv2d_612[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_553 (BatchN (None, 10, 8, 256)   1024        add_177[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_553 (Activation)     (None, 10, 8, 256)   0           batch_normalization_553[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_614 (Conv2D)             (None, 10, 8, 128)   32896       activation_553[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_554 (BatchN (None, 10, 8, 128)   512         conv2d_614[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_554 (Activation)     (None, 10, 8, 128)   0           batch_normalization_554[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_615 (Conv2D)             (None, 10, 8, 128)   147584      activation_554[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_555 (BatchN (None, 10, 8, 128)   512         conv2d_615[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_555 (Activation)     (None, 10, 8, 128)   0           batch_normalization_555[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_616 (Conv2D)             (None, 10, 8, 256)   33024       activation_555[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_178 (Add)                   (None, 10, 8, 256)   0           add_177[0][0]                    \n",
      "                                                                 conv2d_616[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_556 (BatchN (None, 10, 8, 256)   1024        add_178[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_556 (Activation)     (None, 10, 8, 256)   0           batch_normalization_556[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_617 (Conv2D)             (None, 10, 8, 128)   32896       activation_556[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_557 (BatchN (None, 10, 8, 128)   512         conv2d_617[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_557 (Activation)     (None, 10, 8, 128)   0           batch_normalization_557[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_618 (Conv2D)             (None, 10, 8, 128)   147584      activation_557[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_558 (BatchN (None, 10, 8, 128)   512         conv2d_618[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_558 (Activation)     (None, 10, 8, 128)   0           batch_normalization_558[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_619 (Conv2D)             (None, 10, 8, 256)   33024       activation_558[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_179 (Add)                   (None, 10, 8, 256)   0           add_178[0][0]                    \n",
      "                                                                 conv2d_619[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_559 (BatchN (None, 10, 8, 256)   1024        add_179[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_559 (Activation)     (None, 10, 8, 256)   0           batch_normalization_559[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_19 (AveragePo (None, 1, 1, 256)    0           activation_559[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 256)          0           average_pooling2d_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 10)           2570        flatten_19[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[0.29803923 0.32941177 0.37254903 ... 0.29803923 0.32941177\n",
      "    0.37254903]\n",
      "   [0.05882353 0.05882353 0.07843138 ... 0.05882353 0.05882353\n",
      "    0.07843138]\n",
      "   [0.03137255 0.02745098 0.04313726 ... 0.03137255 0.02745098\n",
      "    0.04313726]\n",
      "   ...\n",
      "   [0.0627451  0.05490196 0.09803922 ... 0.0627451  0.05490196\n",
      "    0.09803922]\n",
      "   [0.05098039 0.05098039 0.08627451 ... 0.05098039 0.05098039\n",
      "    0.08627451]\n",
      "   [0.03137255 0.03529412 0.05098039 ... 0.03137255 0.03529412\n",
      "    0.05098039]]\n",
      "\n",
      "  [[0.34509805 0.3764706  0.41960785 ... 0.34509805 0.3764706\n",
      "    0.41960785]\n",
      "   [0.06666667 0.06666667 0.08627451 ... 0.06666667 0.06666667\n",
      "    0.08627451]\n",
      "   [0.02352941 0.01960784 0.03529412 ... 0.02352941 0.01960784\n",
      "    0.03529412]\n",
      "   ...\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.05882353 0.05098039 0.09411765 ... 0.05882353 0.05098039\n",
      "    0.09411765]\n",
      "   [0.03921569 0.03921569 0.0627451  ... 0.03921569 0.03921569\n",
      "    0.0627451 ]]\n",
      "\n",
      "  [[0.3372549  0.35686275 0.38431373 ... 0.3372549  0.35686275\n",
      "    0.38431373]\n",
      "   [0.05490196 0.05098039 0.06666667 ... 0.05490196 0.05098039\n",
      "    0.06666667]\n",
      "   [0.01960784 0.01568628 0.02745098 ... 0.01960784 0.01568628\n",
      "    0.02745098]\n",
      "   ...\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.04705882 0.04313726 0.07058824 ... 0.04705882 0.04313726\n",
      "    0.07058824]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6156863  0.63529414 0.68235296 ... 0.6156863  0.63529414\n",
      "    0.68235296]\n",
      "   [0.62352943 0.64705884 0.6901961  ... 0.62352943 0.64705884\n",
      "    0.6901961 ]\n",
      "   [0.60784316 0.61960787 0.62352943 ... 0.60784316 0.61960787\n",
      "    0.62352943]\n",
      "   ...\n",
      "   [0.49411765 0.53333336 0.65882355 ... 0.49411765 0.53333336\n",
      "    0.65882355]\n",
      "   [0.5686275  0.6        0.69803923 ... 0.5686275  0.6\n",
      "    0.69803923]\n",
      "   [0.5882353  0.6156863  0.7058824  ... 0.5882353  0.6156863\n",
      "    0.7058824 ]]\n",
      "\n",
      "  [[0.61960787 0.63529414 0.68235296 ... 0.61960787 0.63529414\n",
      "    0.68235296]\n",
      "   [0.61960787 0.6431373  0.69411767 ... 0.61960787 0.6431373\n",
      "    0.69411767]\n",
      "   [0.627451   0.654902   0.7019608  ... 0.627451   0.654902\n",
      "    0.7019608 ]\n",
      "   ...\n",
      "   [0.5254902  0.5647059  0.6745098  ... 0.5254902  0.5647059\n",
      "    0.6745098 ]\n",
      "   [0.56078434 0.5921569  0.69411767 ... 0.56078434 0.5921569\n",
      "    0.69411767]\n",
      "   [0.5921569  0.61960787 0.7058824  ... 0.5921569  0.61960787\n",
      "    0.7058824 ]]\n",
      "\n",
      "  [[0.627451   0.6431373  0.6901961  ... 0.627451   0.6431373\n",
      "    0.6901961 ]\n",
      "   [0.61960787 0.6431373  0.69411767 ... 0.61960787 0.6431373\n",
      "    0.69411767]\n",
      "   [0.6392157  0.6627451  0.70980394 ... 0.6392157  0.6627451\n",
      "    0.70980394]\n",
      "   ...\n",
      "   [0.5529412  0.5882353  0.68235296 ... 0.5529412  0.5882353\n",
      "    0.68235296]\n",
      "   [0.54901963 0.58431375 0.6901961  ... 0.54901963 0.58431375\n",
      "    0.6901961 ]\n",
      "   [0.5882353  0.61960787 0.7058824  ... 0.5882353  0.61960787\n",
      "    0.7058824 ]]]\n",
      "\n",
      "\n",
      " [[[0.11764706 0.37254903 0.4117647  ... 0.11764706 0.37254903\n",
      "    0.4117647 ]\n",
      "   [0.12156863 0.38039216 0.41960785 ... 0.12156863 0.38039216\n",
      "    0.41960785]\n",
      "   [0.11764706 0.3764706  0.41568628 ... 0.11764706 0.3764706\n",
      "    0.41568628]\n",
      "   ...\n",
      "   [0.10196079 0.35686275 0.40392157 ... 0.10196079 0.35686275\n",
      "    0.40392157]\n",
      "   [0.09803922 0.36078432 0.4        ... 0.09803922 0.36078432\n",
      "    0.4       ]\n",
      "   [0.09411765 0.35686275 0.39607844 ... 0.09411765 0.35686275\n",
      "    0.39607844]]\n",
      "\n",
      "  [[0.10588235 0.43529412 0.47843137 ... 0.10588235 0.43529412\n",
      "    0.47843137]\n",
      "   [0.10588235 0.43529412 0.48235294 ... 0.10588235 0.43529412\n",
      "    0.48235294]\n",
      "   [0.10980392 0.43529412 0.48235294 ... 0.10980392 0.43529412\n",
      "    0.48235294]\n",
      "   ...\n",
      "   [0.08627451 0.41960785 0.46666667 ... 0.08627451 0.41960785\n",
      "    0.46666667]\n",
      "   [0.08627451 0.42352942 0.4627451  ... 0.08627451 0.42352942\n",
      "    0.4627451 ]\n",
      "   [0.09019608 0.42352942 0.46666667 ... 0.09019608 0.42352942\n",
      "    0.46666667]]\n",
      "\n",
      "  [[0.09411765 0.4117647  0.45490196 ... 0.09411765 0.4117647\n",
      "    0.45490196]\n",
      "   [0.09803922 0.41960785 0.4627451  ... 0.09803922 0.41960785\n",
      "    0.4627451 ]\n",
      "   [0.10980392 0.42352942 0.46666667 ... 0.10980392 0.42352942\n",
      "    0.46666667]\n",
      "   ...\n",
      "   [0.10196079 0.41960785 0.45882353 ... 0.10196079 0.41960785\n",
      "    0.45882353]\n",
      "   [0.09019608 0.4117647  0.4509804  ... 0.09019608 0.4117647\n",
      "    0.4509804 ]\n",
      "   [0.08627451 0.40784314 0.45490196 ... 0.08627451 0.40784314\n",
      "    0.45490196]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.02745098 0.11372549 0.13333334 ... 0.02745098 0.11372549\n",
      "    0.13333334]\n",
      "   [0.34117648 0.2627451  0.21960784 ... 0.34117648 0.2627451\n",
      "    0.21960784]\n",
      "   [0.7019608  0.5019608  0.46666667 ... 0.7019608  0.5019608\n",
      "    0.46666667]\n",
      "   ...\n",
      "   [0.7647059  0.5176471  0.52156866 ... 0.7647059  0.5176471\n",
      "    0.52156866]\n",
      "   [0.46666667 0.3019608  0.28235295 ... 0.46666667 0.3019608\n",
      "    0.28235295]\n",
      "   [0.19607843 0.13725491 0.13725491 ... 0.19607843 0.13725491\n",
      "    0.13725491]]\n",
      "\n",
      "  [[0.00392157 0.16078432 0.19215687 ... 0.00392157 0.16078432\n",
      "    0.19215687]\n",
      "   [0.28235295 0.23921569 0.20392157 ... 0.28235295 0.23921569\n",
      "    0.20392157]\n",
      "   [0.6627451  0.4627451  0.41960785 ... 0.6627451  0.4627451\n",
      "    0.41960785]\n",
      "   ...\n",
      "   [0.7294118  0.5137255  0.49411765 ... 0.7294118  0.5137255\n",
      "    0.49411765]\n",
      "   [0.38039216 0.25490198 0.22745098 ... 0.38039216 0.25490198\n",
      "    0.22745098]\n",
      "   [0.11764706 0.10588235 0.12156863 ... 0.11764706 0.10588235\n",
      "    0.12156863]]\n",
      "\n",
      "  [[0.01960784 0.21960784 0.25490198 ... 0.01960784 0.21960784\n",
      "    0.25490198]\n",
      "   [0.24705882 0.23529412 0.20784314 ... 0.24705882 0.23529412\n",
      "    0.20784314]\n",
      "   [0.6313726  0.44313726 0.40392157 ... 0.6313726  0.44313726\n",
      "    0.40392157]\n",
      "   ...\n",
      "   [0.6901961  0.48235294 0.45882353 ... 0.6901961  0.48235294\n",
      "    0.45882353]\n",
      "   [0.29803923 0.22745098 0.21176471 ... 0.29803923 0.22745098\n",
      "    0.21176471]\n",
      "   [0.07450981 0.13725491 0.16470589 ... 0.07450981 0.13725491\n",
      "    0.16470589]]]\n",
      "\n",
      "\n",
      " [[[0.29803923 0.2784314  0.28627452 ... 0.29803923 0.2784314\n",
      "    0.28627452]\n",
      "   [0.29411766 0.28627452 0.2901961  ... 0.29411766 0.28627452\n",
      "    0.2901961 ]\n",
      "   [0.29803923 0.2901961  0.2901961  ... 0.29803923 0.2901961\n",
      "    0.2901961 ]\n",
      "   ...\n",
      "   [0.32941177 0.29803923 0.28627452 ... 0.32941177 0.29803923\n",
      "    0.28627452]\n",
      "   [0.33333334 0.30980393 0.29411766 ... 0.33333334 0.30980393\n",
      "    0.29411766]\n",
      "   [0.35686275 0.32941177 0.30980393 ... 0.35686275 0.32941177\n",
      "    0.30980393]]\n",
      "\n",
      "  [[0.28627452 0.2784314  0.28235295 ... 0.28627452 0.2784314\n",
      "    0.28235295]\n",
      "   [0.28235295 0.27058825 0.27450982 ... 0.28235295 0.27058825\n",
      "    0.27450982]\n",
      "   [0.2901961  0.27058825 0.2784314  ... 0.2901961  0.27058825\n",
      "    0.2784314 ]\n",
      "   ...\n",
      "   [0.32156864 0.29803923 0.28627452 ... 0.32156864 0.29803923\n",
      "    0.28627452]\n",
      "   [0.34117648 0.30980393 0.29411766 ... 0.34117648 0.30980393\n",
      "    0.29411766]\n",
      "   [0.3647059  0.34117648 0.31764707 ... 0.3647059  0.34117648\n",
      "    0.31764707]]\n",
      "\n",
      "  [[0.28235295 0.2784314  0.2784314  ... 0.28235295 0.2784314\n",
      "    0.2784314 ]\n",
      "   [0.2784314  0.27058825 0.27450982 ... 0.2784314  0.27058825\n",
      "    0.27450982]\n",
      "   [0.29803923 0.2784314  0.28235295 ... 0.29803923 0.2784314\n",
      "    0.28235295]\n",
      "   ...\n",
      "   [0.3254902  0.29411766 0.28235295 ... 0.3254902  0.29411766\n",
      "    0.28235295]\n",
      "   [0.33333334 0.29803923 0.2901961  ... 0.33333334 0.29803923\n",
      "    0.2901961 ]\n",
      "   [0.35686275 0.33333334 0.31764707 ... 0.35686275 0.33333334\n",
      "    0.31764707]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.5372549  0.45882353 0.4509804  ... 0.5372549  0.45882353\n",
      "    0.4509804 ]\n",
      "   [0.5176471  0.42745098 0.41568628 ... 0.5176471  0.42745098\n",
      "    0.41568628]\n",
      "   [0.5176471  0.42745098 0.41960785 ... 0.5176471  0.42745098\n",
      "    0.41960785]\n",
      "   ...\n",
      "   [0.32156864 0.5058824  0.56078434 ... 0.32156864 0.5058824\n",
      "    0.56078434]\n",
      "   [0.33333334 0.5058824  0.5686275  ... 0.33333334 0.5058824\n",
      "    0.5686275 ]\n",
      "   [0.32941177 0.49019608 0.54509807 ... 0.32941177 0.49019608\n",
      "    0.54509807]]\n",
      "\n",
      "  [[0.5019608  0.44313726 0.43137255 ... 0.5019608  0.44313726\n",
      "    0.43137255]\n",
      "   [0.49411765 0.43137255 0.41960785 ... 0.49411765 0.43137255\n",
      "    0.41960785]\n",
      "   [0.4862745  0.42352942 0.41568628 ... 0.4862745  0.42352942\n",
      "    0.41568628]\n",
      "   ...\n",
      "   [0.33333334 0.52156866 0.5921569  ... 0.33333334 0.52156866\n",
      "    0.5921569 ]\n",
      "   [0.3254902  0.5058824  0.57254905 ... 0.3254902  0.5058824\n",
      "    0.57254905]\n",
      "   [0.33333334 0.49803922 0.5568628  ... 0.33333334 0.49803922\n",
      "    0.5568628 ]]\n",
      "\n",
      "  [[0.49803922 0.43529412 0.41960785 ... 0.49803922 0.43529412\n",
      "    0.41960785]\n",
      "   [0.47843137 0.41568628 0.4        ... 0.47843137 0.41568628\n",
      "    0.4       ]\n",
      "   [0.47058824 0.4117647  0.39607844 ... 0.47058824 0.4117647\n",
      "    0.39607844]\n",
      "   ...\n",
      "   [0.3529412  0.53333336 0.60784316 ... 0.3529412  0.53333336\n",
      "    0.60784316]\n",
      "   [0.34117648 0.5137255  0.5803922  ... 0.34117648 0.5137255\n",
      "    0.5803922 ]\n",
      "   [0.34901962 0.5019608  0.5568628  ... 0.34901962 0.5019608\n",
      "    0.5568628 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157 ... 0.00392157 0.00392157\n",
      "    0.00392157]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.61960787 0.62352943 0.6784314  ... 0.61960787 0.62352943\n",
      "    0.6784314 ]\n",
      "   [0.6784314  0.6862745  0.73333335 ... 0.6784314  0.6862745\n",
      "    0.73333335]\n",
      "   [0.5019608  0.5058824  0.5568628  ... 0.5019608  0.5058824\n",
      "    0.5568628 ]\n",
      "   ...\n",
      "   [0.25490198 0.26666668 0.30588236 ... 0.25490198 0.26666668\n",
      "    0.30588236]\n",
      "   [0.16078432 0.16078432 0.21176471 ... 0.16078432 0.16078432\n",
      "    0.21176471]\n",
      "   [0.15686275 0.15294118 0.2        ... 0.15686275 0.15294118\n",
      "    0.2       ]]\n",
      "\n",
      "  [[0.59607846 0.6039216  0.65882355 ... 0.59607846 0.6039216\n",
      "    0.65882355]\n",
      "   [0.6        0.60784316 0.6627451  ... 0.6        0.60784316\n",
      "    0.6627451 ]\n",
      "   [0.4627451  0.46666667 0.52156866 ... 0.4627451  0.46666667\n",
      "    0.52156866]\n",
      "   ...\n",
      "   [0.16078432 0.16862746 0.19607843 ... 0.16078432 0.16862746\n",
      "    0.19607843]\n",
      "   [0.1254902  0.12156863 0.16470589 ... 0.1254902  0.12156863\n",
      "    0.16470589]\n",
      "   [0.12156863 0.12156863 0.16078432 ... 0.12156863 0.12156863\n",
      "    0.16078432]]\n",
      "\n",
      "  [[0.49411765 0.49019608 0.5568628  ... 0.49411765 0.49019608\n",
      "    0.5568628 ]\n",
      "   [0.5019608  0.49803922 0.5647059  ... 0.5019608  0.49803922\n",
      "    0.5647059 ]\n",
      "   [0.4745098  0.47058824 0.5294118  ... 0.4745098  0.47058824\n",
      "    0.5294118 ]\n",
      "   ...\n",
      "   [0.02745098 0.02352941 0.03137255 ... 0.02745098 0.02352941\n",
      "    0.03137255]\n",
      "   [0.09803922 0.09019608 0.1254902  ... 0.09803922 0.09019608\n",
      "    0.1254902 ]\n",
      "   [0.09411765 0.08627451 0.12156863 ... 0.09411765 0.08627451\n",
      "    0.12156863]]]\n",
      "\n",
      "\n",
      " [[[0.04705882 0.04705882 0.04705882 ... 0.04705882 0.04705882\n",
      "    0.04705882]\n",
      "   [0.01176471 0.01176471 0.01176471 ... 0.01176471 0.01176471\n",
      "    0.01176471]\n",
      "   [0.03529412 0.03529412 0.03529412 ... 0.03529412 0.03529412\n",
      "    0.03529412]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.01176471 0.01176471 0.01176471 ... 0.01176471 0.01176471\n",
      "    0.01176471]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.49803922 0.49803922 0.5176471  ... 0.49803922 0.49803922\n",
      "    0.5176471 ]\n",
      "   [0.5058824  0.5019608  0.52156866 ... 0.5058824  0.5019608\n",
      "    0.52156866]\n",
      "   [0.50980395 0.5058824  0.5254902  ... 0.50980395 0.5058824\n",
      "    0.5254902 ]\n",
      "   ...\n",
      "   [0.5686275  0.57254905 0.5882353  ... 0.5686275  0.57254905\n",
      "    0.5882353 ]\n",
      "   [0.57254905 0.5764706  0.5921569  ... 0.57254905 0.5764706\n",
      "    0.5921569 ]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]]\n",
      "\n",
      "  [[0.50980395 0.50980395 0.5254902  ... 0.50980395 0.50980395\n",
      "    0.5254902 ]\n",
      "   [0.5137255  0.5137255  0.5294118  ... 0.5137255  0.5137255\n",
      "    0.5294118 ]\n",
      "   [0.5176471  0.5176471  0.53333336 ... 0.5176471  0.5176471\n",
      "    0.53333336]\n",
      "   ...\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5803922  0.58431375 0.6        ... 0.5803922  0.58431375\n",
      "    0.6       ]]\n",
      "\n",
      "  [[0.5176471  0.5176471  0.5372549  ... 0.5176471  0.5176471\n",
      "    0.5372549 ]\n",
      "   [0.52156866 0.5254902  0.5411765  ... 0.52156866 0.5254902\n",
      "    0.5411765 ]\n",
      "   [0.5176471  0.52156866 0.5372549  ... 0.5176471  0.52156866\n",
      "    0.5372549 ]\n",
      "   ...\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.5882353  0.5921569  0.60784316 ... 0.5882353  0.5921569\n",
      "    0.60784316]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8901961  0.8392157  0.8039216  ... 0.8901961  0.8392157\n",
      "    0.8039216 ]\n",
      "   [0.91764706 0.8745098  0.8392157  ... 0.91764706 0.8745098\n",
      "    0.8392157 ]\n",
      "   [0.8627451  0.79607844 0.7490196  ... 0.8627451  0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9372549  0.90588236 0.8784314  ... 0.9372549  0.90588236\n",
      "    0.8784314 ]\n",
      "   [0.90588236 0.85882354 0.8235294  ... 0.90588236 0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.87058824 0.8117647  0.7764706  ... 0.87058824 0.8117647\n",
      "    0.7764706 ]]\n",
      "\n",
      "  [[0.88235295 0.8235294  0.78431374 ... 0.88235295 0.8235294\n",
      "    0.78431374]\n",
      "   [0.89411765 0.8392157  0.8        ... 0.89411765 0.8392157\n",
      "    0.8       ]\n",
      "   [0.87058824 0.8039216  0.7607843  ... 0.87058824 0.8039216\n",
      "    0.7607843 ]\n",
      "   ...\n",
      "   [0.92156863 0.8862745  0.8509804  ... 0.92156863 0.8862745\n",
      "    0.8509804 ]\n",
      "   [0.9098039  0.87058824 0.8352941  ... 0.9098039  0.87058824\n",
      "    0.8352941 ]\n",
      "   [0.8745098  0.8156863  0.77254903 ... 0.8745098  0.8156863\n",
      "    0.77254903]]\n",
      "\n",
      "  [[0.8627451  0.7921569  0.7411765  ... 0.8627451  0.7921569\n",
      "    0.7411765 ]\n",
      "   [0.8666667  0.8039216  0.7529412  ... 0.8666667  0.8039216\n",
      "    0.7529412 ]\n",
      "   [0.85882354 0.79607844 0.7490196  ... 0.85882354 0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9098039  0.85882354 0.8235294  ... 0.9098039  0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.90588236 0.85490197 0.8156863  ... 0.90588236 0.85490197\n",
      "    0.8156863 ]\n",
      "   [0.8784314  0.8156863  0.7647059  ... 0.8784314  0.8156863\n",
      "    0.7647059 ]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (69, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (69, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Learning rate:  0.0\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0187 - accuracy: 0.0000e+0033\n",
      "33\n",
      "[[[[243 243 243 ... 243 243 243]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  [[242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  [[242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[243 242 244 ... 243 242 244]\n",
      "   [245 242 248 ... 245 242 248]\n",
      "   [250 250 253 ... 250 250 253]\n",
      "   ...\n",
      "   [214 218 239 ... 214 218 239]\n",
      "   [205 209 233 ... 205 209 233]\n",
      "   [213 215 234 ... 213 215 234]]\n",
      "\n",
      "  [[238 236 241 ... 238 236 241]\n",
      "   [235 230 238 ... 235 230 238]\n",
      "   [240 236 243 ... 240 236 243]\n",
      "   ...\n",
      "   [210 214 238 ... 210 214 238]\n",
      "   [202 206 232 ... 202 206 232]\n",
      "   [206 208 231 ... 206 208 231]]\n",
      "\n",
      "  [[226 219 227 ... 226 219 227]\n",
      "   [221 211 220 ... 221 211 220]\n",
      "   [219 210 222 ... 219 210 222]\n",
      "   ...\n",
      "   [207 212 237 ... 207 212 237]\n",
      "   [207 212 236 ... 207 212 236]\n",
      "   [207 210 233 ... 207 210 233]]]\n",
      "\n",
      "\n",
      " [[[ 93  64  41 ...  93  64  41]\n",
      "   [101  73  50 ... 101  73  50]\n",
      "   [109  83  61 ... 109  83  61]\n",
      "   ...\n",
      "   [ 92  57  35 ...  92  57  35]\n",
      "   [ 88  52  30 ...  88  52  30]\n",
      "   [ 83  47  26 ...  83  47  26]]\n",
      "\n",
      "  [[101  73  50 ... 101  73  50]\n",
      "   [109  82  60 ... 109  82  60]\n",
      "   [116  92  70 ... 116  92  70]\n",
      "   ...\n",
      "   [ 97  62  39 ...  97  62  39]\n",
      "   [ 92  56  34 ...  92  56  34]\n",
      "   [ 87  51  30 ...  87  51  30]]\n",
      "\n",
      "  [[108  82  59 ... 108  82  59]\n",
      "   [115  91  70 ... 115  91  70]\n",
      "   [122 100  81 ... 122 100  81]\n",
      "   ...\n",
      "   [101  66  44 ... 101  66  44]\n",
      "   [ 96  60  38 ...  96  60  38]\n",
      "   [ 91  54  33 ...  91  54  33]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 87  89  90 ...  87  89  90]\n",
      "   [ 98 100 102 ...  98 100 102]\n",
      "   [112 115 118 ... 112 115 118]\n",
      "   ...\n",
      "   [242 187 163 ... 242 187 163]\n",
      "   [230 169 145 ... 230 169 145]\n",
      "   [209 149 124 ... 209 149 124]]\n",
      "\n",
      "  [[ 81  83  83 ...  81  83  83]\n",
      "   [ 87  89  91 ...  87  89  91]\n",
      "   [106 108 111 ... 106 108 111]\n",
      "   ...\n",
      "   [235 180 155 ... 235 180 155]\n",
      "   [224 167 141 ... 224 167 141]\n",
      "   [204 143 118 ... 204 143 118]]\n",
      "\n",
      "  [[ 78  80  80 ...  78  80  80]\n",
      "   [ 79  80  82 ...  79  80  82]\n",
      "   [ 94  96  99 ...  94  96  99]\n",
      "   ...\n",
      "   [229 174 149 ... 229 174 149]\n",
      "   [218 160 135 ... 218 160 135]\n",
      "   [197 137 113 ... 197 137 113]]]\n",
      "\n",
      "\n",
      " [[[ 76  71  73 ...  76  71  73]\n",
      "   [ 75  73  74 ...  75  73  74]\n",
      "   [ 76  74  74 ...  76  74  74]\n",
      "   ...\n",
      "   [ 84  76  73 ...  84  76  73]\n",
      "   [ 85  79  75 ...  85  79  75]\n",
      "   [ 91  84  79 ...  91  84  79]]\n",
      "\n",
      "  [[ 73  71  72 ...  73  71  72]\n",
      "   [ 72  69  70 ...  72  69  70]\n",
      "   [ 74  69  71 ...  74  69  71]\n",
      "   ...\n",
      "   [ 82  76  73 ...  82  76  73]\n",
      "   [ 87  79  75 ...  87  79  75]\n",
      "   [ 93  87  81 ...  93  87  81]]\n",
      "\n",
      "  [[ 72  71  71 ...  72  71  71]\n",
      "   [ 71  69  70 ...  71  69  70]\n",
      "   [ 76  71  72 ...  76  71  72]\n",
      "   ...\n",
      "   [ 83  75  72 ...  83  75  72]\n",
      "   [ 85  76  74 ...  85  76  74]\n",
      "   [ 91  85  81 ...  91  85  81]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[137 117 115 ... 137 117 115]\n",
      "   [132 109 106 ... 132 109 106]\n",
      "   [132 109 107 ... 132 109 107]\n",
      "   ...\n",
      "   [ 82 129 143 ...  82 129 143]\n",
      "   [ 85 129 145 ...  85 129 145]\n",
      "   [ 84 125 139 ...  84 125 139]]\n",
      "\n",
      "  [[128 113 110 ... 128 113 110]\n",
      "   [126 110 107 ... 126 110 107]\n",
      "   [124 108 106 ... 124 108 106]\n",
      "   ...\n",
      "   [ 85 133 151 ...  85 133 151]\n",
      "   [ 83 129 146 ...  83 129 146]\n",
      "   [ 85 127 142 ...  85 127 142]]\n",
      "\n",
      "  [[127 111 107 ... 127 111 107]\n",
      "   [122 106 102 ... 122 106 102]\n",
      "   [120 105 101 ... 120 105 101]\n",
      "   ...\n",
      "   [ 90 136 155 ...  90 136 155]\n",
      "   [ 87 131 148 ...  87 131 148]\n",
      "   [ 89 128 142 ...  89 128 142]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (65, 40, 30, 9)\n",
      "65 train samples\n",
      "32 test samples\n",
      "y_train shape: (65, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_620 (Conv2D)             (None, 40, 30, 16)   1312        input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_560 (BatchN (None, 40, 30, 16)   64          conv2d_620[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_560 (Activation)     (None, 40, 30, 16)   0           batch_normalization_560[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_621 (Conv2D)             (None, 40, 30, 16)   272         activation_560[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_561 (BatchN (None, 40, 30, 16)   64          conv2d_621[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_561 (Activation)     (None, 40, 30, 16)   0           batch_normalization_561[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_622 (Conv2D)             (None, 40, 30, 16)   2320        activation_561[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_562 (BatchN (None, 40, 30, 16)   64          conv2d_622[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_562 (Activation)     (None, 40, 30, 16)   0           batch_normalization_562[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_624 (Conv2D)             (None, 40, 30, 64)   1088        activation_560[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_623 (Conv2D)             (None, 40, 30, 64)   1088        activation_562[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_180 (Add)                   (None, 40, 30, 64)   0           conv2d_624[0][0]                 \n",
      "                                                                 conv2d_623[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_563 (BatchN (None, 40, 30, 64)   256         add_180[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_563 (Activation)     (None, 40, 30, 64)   0           batch_normalization_563[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_625 (Conv2D)             (None, 40, 30, 16)   1040        activation_563[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_564 (BatchN (None, 40, 30, 16)   64          conv2d_625[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_564 (Activation)     (None, 40, 30, 16)   0           batch_normalization_564[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_626 (Conv2D)             (None, 40, 30, 16)   2320        activation_564[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_565 (BatchN (None, 40, 30, 16)   64          conv2d_626[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_565 (Activation)     (None, 40, 30, 16)   0           batch_normalization_565[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_627 (Conv2D)             (None, 40, 30, 64)   1088        activation_565[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_181 (Add)                   (None, 40, 30, 64)   0           add_180[0][0]                    \n",
      "                                                                 conv2d_627[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_566 (BatchN (None, 40, 30, 64)   256         add_181[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_566 (Activation)     (None, 40, 30, 64)   0           batch_normalization_566[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_628 (Conv2D)             (None, 40, 30, 16)   1040        activation_566[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_567 (BatchN (None, 40, 30, 16)   64          conv2d_628[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_567 (Activation)     (None, 40, 30, 16)   0           batch_normalization_567[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_629 (Conv2D)             (None, 40, 30, 16)   2320        activation_567[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_568 (BatchN (None, 40, 30, 16)   64          conv2d_629[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_568 (Activation)     (None, 40, 30, 16)   0           batch_normalization_568[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_630 (Conv2D)             (None, 40, 30, 64)   1088        activation_568[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_182 (Add)                   (None, 40, 30, 64)   0           add_181[0][0]                    \n",
      "                                                                 conv2d_630[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_569 (BatchN (None, 40, 30, 64)   256         add_182[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_569 (Activation)     (None, 40, 30, 64)   0           batch_normalization_569[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_631 (Conv2D)             (None, 20, 15, 64)   4160        activation_569[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_570 (BatchN (None, 20, 15, 64)   256         conv2d_631[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_570 (Activation)     (None, 20, 15, 64)   0           batch_normalization_570[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_632 (Conv2D)             (None, 20, 15, 64)   36928       activation_570[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_571 (BatchN (None, 20, 15, 64)   256         conv2d_632[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_571 (Activation)     (None, 20, 15, 64)   0           batch_normalization_571[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_634 (Conv2D)             (None, 20, 15, 128)  8320        add_182[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_633 (Conv2D)             (None, 20, 15, 128)  8320        activation_571[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_183 (Add)                   (None, 20, 15, 128)  0           conv2d_634[0][0]                 \n",
      "                                                                 conv2d_633[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_572 (BatchN (None, 20, 15, 128)  512         add_183[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_572 (Activation)     (None, 20, 15, 128)  0           batch_normalization_572[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_635 (Conv2D)             (None, 20, 15, 64)   8256        activation_572[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_573 (BatchN (None, 20, 15, 64)   256         conv2d_635[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_573 (Activation)     (None, 20, 15, 64)   0           batch_normalization_573[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_636 (Conv2D)             (None, 20, 15, 64)   36928       activation_573[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_574 (BatchN (None, 20, 15, 64)   256         conv2d_636[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_574 (Activation)     (None, 20, 15, 64)   0           batch_normalization_574[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_637 (Conv2D)             (None, 20, 15, 128)  8320        activation_574[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_184 (Add)                   (None, 20, 15, 128)  0           add_183[0][0]                    \n",
      "                                                                 conv2d_637[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_575 (BatchN (None, 20, 15, 128)  512         add_184[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_575 (Activation)     (None, 20, 15, 128)  0           batch_normalization_575[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_638 (Conv2D)             (None, 20, 15, 64)   8256        activation_575[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_576 (BatchN (None, 20, 15, 64)   256         conv2d_638[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_576 (Activation)     (None, 20, 15, 64)   0           batch_normalization_576[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_639 (Conv2D)             (None, 20, 15, 64)   36928       activation_576[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_577 (BatchN (None, 20, 15, 64)   256         conv2d_639[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_577 (Activation)     (None, 20, 15, 64)   0           batch_normalization_577[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_640 (Conv2D)             (None, 20, 15, 128)  8320        activation_577[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_185 (Add)                   (None, 20, 15, 128)  0           add_184[0][0]                    \n",
      "                                                                 conv2d_640[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_578 (BatchN (None, 20, 15, 128)  512         add_185[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_578 (Activation)     (None, 20, 15, 128)  0           batch_normalization_578[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_641 (Conv2D)             (None, 10, 8, 128)   16512       activation_578[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_579 (BatchN (None, 10, 8, 128)   512         conv2d_641[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_579 (Activation)     (None, 10, 8, 128)   0           batch_normalization_579[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_642 (Conv2D)             (None, 10, 8, 128)   147584      activation_579[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_580 (BatchN (None, 10, 8, 128)   512         conv2d_642[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_580 (Activation)     (None, 10, 8, 128)   0           batch_normalization_580[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_644 (Conv2D)             (None, 10, 8, 256)   33024       add_185[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_643 (Conv2D)             (None, 10, 8, 256)   33024       activation_580[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_186 (Add)                   (None, 10, 8, 256)   0           conv2d_644[0][0]                 \n",
      "                                                                 conv2d_643[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_581 (BatchN (None, 10, 8, 256)   1024        add_186[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_581 (Activation)     (None, 10, 8, 256)   0           batch_normalization_581[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_645 (Conv2D)             (None, 10, 8, 128)   32896       activation_581[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_582 (BatchN (None, 10, 8, 128)   512         conv2d_645[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_582 (Activation)     (None, 10, 8, 128)   0           batch_normalization_582[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_646 (Conv2D)             (None, 10, 8, 128)   147584      activation_582[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_583 (BatchN (None, 10, 8, 128)   512         conv2d_646[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_583 (Activation)     (None, 10, 8, 128)   0           batch_normalization_583[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_647 (Conv2D)             (None, 10, 8, 256)   33024       activation_583[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_187 (Add)                   (None, 10, 8, 256)   0           add_186[0][0]                    \n",
      "                                                                 conv2d_647[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_584 (BatchN (None, 10, 8, 256)   1024        add_187[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_584 (Activation)     (None, 10, 8, 256)   0           batch_normalization_584[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_648 (Conv2D)             (None, 10, 8, 128)   32896       activation_584[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_585 (BatchN (None, 10, 8, 128)   512         conv2d_648[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_585 (Activation)     (None, 10, 8, 128)   0           batch_normalization_585[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_649 (Conv2D)             (None, 10, 8, 128)   147584      activation_585[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_586 (BatchN (None, 10, 8, 128)   512         conv2d_649[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_586 (Activation)     (None, 10, 8, 128)   0           batch_normalization_586[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_650 (Conv2D)             (None, 10, 8, 256)   33024       activation_586[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_188 (Add)                   (None, 10, 8, 256)   0           add_187[0][0]                    \n",
      "                                                                 conv2d_650[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_587 (BatchN (None, 10, 8, 256)   1024        add_188[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_587 (Activation)     (None, 10, 8, 256)   0           batch_normalization_587[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_20 (AveragePo (None, 1, 1, 256)    0           activation_587[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 256)          0           average_pooling2d_20[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 10)           2570        flatten_20[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[0.9529412  0.9529412  0.9529412  ... 0.9529412  0.9529412\n",
      "    0.9529412 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  [[0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  [[0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.9529412  0.9490196  0.95686275 ... 0.9529412  0.9490196\n",
      "    0.95686275]\n",
      "   [0.9607843  0.9490196  0.972549   ... 0.9607843  0.9490196\n",
      "    0.972549  ]\n",
      "   [0.98039216 0.98039216 0.99215686 ... 0.98039216 0.98039216\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [0.8392157  0.85490197 0.9372549  ... 0.8392157  0.85490197\n",
      "    0.9372549 ]\n",
      "   [0.8039216  0.81960785 0.9137255  ... 0.8039216  0.81960785\n",
      "    0.9137255 ]\n",
      "   [0.8352941  0.84313726 0.91764706 ... 0.8352941  0.84313726\n",
      "    0.91764706]]\n",
      "\n",
      "  [[0.93333334 0.9254902  0.94509804 ... 0.93333334 0.9254902\n",
      "    0.94509804]\n",
      "   [0.92156863 0.9019608  0.93333334 ... 0.92156863 0.9019608\n",
      "    0.93333334]\n",
      "   [0.9411765  0.9254902  0.9529412  ... 0.9411765  0.9254902\n",
      "    0.9529412 ]\n",
      "   ...\n",
      "   [0.8235294  0.8392157  0.93333334 ... 0.8235294  0.8392157\n",
      "    0.93333334]\n",
      "   [0.7921569  0.80784315 0.9098039  ... 0.7921569  0.80784315\n",
      "    0.9098039 ]\n",
      "   [0.80784315 0.8156863  0.90588236 ... 0.80784315 0.8156863\n",
      "    0.90588236]]\n",
      "\n",
      "  [[0.8862745  0.85882354 0.8901961  ... 0.8862745  0.85882354\n",
      "    0.8901961 ]\n",
      "   [0.8666667  0.827451   0.8627451  ... 0.8666667  0.827451\n",
      "    0.8627451 ]\n",
      "   [0.85882354 0.8235294  0.87058824 ... 0.85882354 0.8235294\n",
      "    0.87058824]\n",
      "   ...\n",
      "   [0.8117647  0.83137256 0.92941177 ... 0.8117647  0.83137256\n",
      "    0.92941177]\n",
      "   [0.8117647  0.83137256 0.9254902  ... 0.8117647  0.83137256\n",
      "    0.9254902 ]\n",
      "   [0.8117647  0.8235294  0.9137255  ... 0.8117647  0.8235294\n",
      "    0.9137255 ]]]\n",
      "\n",
      "\n",
      " [[[0.3647059  0.2509804  0.16078432 ... 0.3647059  0.2509804\n",
      "    0.16078432]\n",
      "   [0.39607844 0.28627452 0.19607843 ... 0.39607844 0.28627452\n",
      "    0.19607843]\n",
      "   [0.42745098 0.3254902  0.23921569 ... 0.42745098 0.3254902\n",
      "    0.23921569]\n",
      "   ...\n",
      "   [0.36078432 0.22352941 0.13725491 ... 0.36078432 0.22352941\n",
      "    0.13725491]\n",
      "   [0.34509805 0.20392157 0.11764706 ... 0.34509805 0.20392157\n",
      "    0.11764706]\n",
      "   [0.3254902  0.18431373 0.10196079 ... 0.3254902  0.18431373\n",
      "    0.10196079]]\n",
      "\n",
      "  [[0.39607844 0.28627452 0.19607843 ... 0.39607844 0.28627452\n",
      "    0.19607843]\n",
      "   [0.42745098 0.32156864 0.23529412 ... 0.42745098 0.32156864\n",
      "    0.23529412]\n",
      "   [0.45490196 0.36078432 0.27450982 ... 0.45490196 0.36078432\n",
      "    0.27450982]\n",
      "   ...\n",
      "   [0.38039216 0.24313726 0.15294118 ... 0.38039216 0.24313726\n",
      "    0.15294118]\n",
      "   [0.36078432 0.21960784 0.13333334 ... 0.36078432 0.21960784\n",
      "    0.13333334]\n",
      "   [0.34117648 0.2        0.11764706 ... 0.34117648 0.2\n",
      "    0.11764706]]\n",
      "\n",
      "  [[0.42352942 0.32156864 0.23137255 ... 0.42352942 0.32156864\n",
      "    0.23137255]\n",
      "   [0.4509804  0.35686275 0.27450982 ... 0.4509804  0.35686275\n",
      "    0.27450982]\n",
      "   [0.47843137 0.39215687 0.31764707 ... 0.47843137 0.39215687\n",
      "    0.31764707]\n",
      "   ...\n",
      "   [0.39607844 0.25882354 0.17254902 ... 0.39607844 0.25882354\n",
      "    0.17254902]\n",
      "   [0.3764706  0.23529412 0.14901961 ... 0.3764706  0.23529412\n",
      "    0.14901961]\n",
      "   [0.35686275 0.21176471 0.12941177 ... 0.35686275 0.21176471\n",
      "    0.12941177]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.34117648 0.34901962 0.3529412  ... 0.34117648 0.34901962\n",
      "    0.3529412 ]\n",
      "   [0.38431373 0.39215687 0.4        ... 0.38431373 0.39215687\n",
      "    0.4       ]\n",
      "   [0.4392157  0.4509804  0.4627451  ... 0.4392157  0.4509804\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.9490196  0.73333335 0.6392157  ... 0.9490196  0.73333335\n",
      "    0.6392157 ]\n",
      "   [0.9019608  0.6627451  0.5686275  ... 0.9019608  0.6627451\n",
      "    0.5686275 ]\n",
      "   [0.81960785 0.58431375 0.4862745  ... 0.81960785 0.58431375\n",
      "    0.4862745 ]]\n",
      "\n",
      "  [[0.31764707 0.3254902  0.3254902  ... 0.31764707 0.3254902\n",
      "    0.3254902 ]\n",
      "   [0.34117648 0.34901962 0.35686275 ... 0.34117648 0.34901962\n",
      "    0.35686275]\n",
      "   [0.41568628 0.42352942 0.43529412 ... 0.41568628 0.42352942\n",
      "    0.43529412]\n",
      "   ...\n",
      "   [0.92156863 0.7058824  0.60784316 ... 0.92156863 0.7058824\n",
      "    0.60784316]\n",
      "   [0.8784314  0.654902   0.5529412  ... 0.8784314  0.654902\n",
      "    0.5529412 ]\n",
      "   [0.8        0.56078434 0.4627451  ... 0.8        0.56078434\n",
      "    0.4627451 ]]\n",
      "\n",
      "  [[0.30588236 0.3137255  0.3137255  ... 0.30588236 0.3137255\n",
      "    0.3137255 ]\n",
      "   [0.30980393 0.3137255  0.32156864 ... 0.30980393 0.3137255\n",
      "    0.32156864]\n",
      "   [0.36862746 0.3764706  0.3882353  ... 0.36862746 0.3764706\n",
      "    0.3882353 ]\n",
      "   ...\n",
      "   [0.8980392  0.68235296 0.58431375 ... 0.8980392  0.68235296\n",
      "    0.58431375]\n",
      "   [0.85490197 0.627451   0.5294118  ... 0.85490197 0.627451\n",
      "    0.5294118 ]\n",
      "   [0.77254903 0.5372549  0.44313726 ... 0.77254903 0.5372549\n",
      "    0.44313726]]]\n",
      "\n",
      "\n",
      " [[[0.29803923 0.2784314  0.28627452 ... 0.29803923 0.2784314\n",
      "    0.28627452]\n",
      "   [0.29411766 0.28627452 0.2901961  ... 0.29411766 0.28627452\n",
      "    0.2901961 ]\n",
      "   [0.29803923 0.2901961  0.2901961  ... 0.29803923 0.2901961\n",
      "    0.2901961 ]\n",
      "   ...\n",
      "   [0.32941177 0.29803923 0.28627452 ... 0.32941177 0.29803923\n",
      "    0.28627452]\n",
      "   [0.33333334 0.30980393 0.29411766 ... 0.33333334 0.30980393\n",
      "    0.29411766]\n",
      "   [0.35686275 0.32941177 0.30980393 ... 0.35686275 0.32941177\n",
      "    0.30980393]]\n",
      "\n",
      "  [[0.28627452 0.2784314  0.28235295 ... 0.28627452 0.2784314\n",
      "    0.28235295]\n",
      "   [0.28235295 0.27058825 0.27450982 ... 0.28235295 0.27058825\n",
      "    0.27450982]\n",
      "   [0.2901961  0.27058825 0.2784314  ... 0.2901961  0.27058825\n",
      "    0.2784314 ]\n",
      "   ...\n",
      "   [0.32156864 0.29803923 0.28627452 ... 0.32156864 0.29803923\n",
      "    0.28627452]\n",
      "   [0.34117648 0.30980393 0.29411766 ... 0.34117648 0.30980393\n",
      "    0.29411766]\n",
      "   [0.3647059  0.34117648 0.31764707 ... 0.3647059  0.34117648\n",
      "    0.31764707]]\n",
      "\n",
      "  [[0.28235295 0.2784314  0.2784314  ... 0.28235295 0.2784314\n",
      "    0.2784314 ]\n",
      "   [0.2784314  0.27058825 0.27450982 ... 0.2784314  0.27058825\n",
      "    0.27450982]\n",
      "   [0.29803923 0.2784314  0.28235295 ... 0.29803923 0.2784314\n",
      "    0.28235295]\n",
      "   ...\n",
      "   [0.3254902  0.29411766 0.28235295 ... 0.3254902  0.29411766\n",
      "    0.28235295]\n",
      "   [0.33333334 0.29803923 0.2901961  ... 0.33333334 0.29803923\n",
      "    0.2901961 ]\n",
      "   [0.35686275 0.33333334 0.31764707 ... 0.35686275 0.33333334\n",
      "    0.31764707]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.5372549  0.45882353 0.4509804  ... 0.5372549  0.45882353\n",
      "    0.4509804 ]\n",
      "   [0.5176471  0.42745098 0.41568628 ... 0.5176471  0.42745098\n",
      "    0.41568628]\n",
      "   [0.5176471  0.42745098 0.41960785 ... 0.5176471  0.42745098\n",
      "    0.41960785]\n",
      "   ...\n",
      "   [0.32156864 0.5058824  0.56078434 ... 0.32156864 0.5058824\n",
      "    0.56078434]\n",
      "   [0.33333334 0.5058824  0.5686275  ... 0.33333334 0.5058824\n",
      "    0.5686275 ]\n",
      "   [0.32941177 0.49019608 0.54509807 ... 0.32941177 0.49019608\n",
      "    0.54509807]]\n",
      "\n",
      "  [[0.5019608  0.44313726 0.43137255 ... 0.5019608  0.44313726\n",
      "    0.43137255]\n",
      "   [0.49411765 0.43137255 0.41960785 ... 0.49411765 0.43137255\n",
      "    0.41960785]\n",
      "   [0.4862745  0.42352942 0.41568628 ... 0.4862745  0.42352942\n",
      "    0.41568628]\n",
      "   ...\n",
      "   [0.33333334 0.52156866 0.5921569  ... 0.33333334 0.52156866\n",
      "    0.5921569 ]\n",
      "   [0.3254902  0.5058824  0.57254905 ... 0.3254902  0.5058824\n",
      "    0.57254905]\n",
      "   [0.33333334 0.49803922 0.5568628  ... 0.33333334 0.49803922\n",
      "    0.5568628 ]]\n",
      "\n",
      "  [[0.49803922 0.43529412 0.41960785 ... 0.49803922 0.43529412\n",
      "    0.41960785]\n",
      "   [0.47843137 0.41568628 0.4        ... 0.47843137 0.41568628\n",
      "    0.4       ]\n",
      "   [0.47058824 0.4117647  0.39607844 ... 0.47058824 0.4117647\n",
      "    0.39607844]\n",
      "   ...\n",
      "   [0.3529412  0.53333336 0.60784316 ... 0.3529412  0.53333336\n",
      "    0.60784316]\n",
      "   [0.34117648 0.5137255  0.5803922  ... 0.34117648 0.5137255\n",
      "    0.5803922 ]\n",
      "   [0.34901962 0.5019608  0.5568628  ... 0.34901962 0.5019608\n",
      "    0.5568628 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.49803922 0.49803922 0.5176471  ... 0.49803922 0.49803922\n",
      "    0.5176471 ]\n",
      "   [0.5058824  0.5019608  0.52156866 ... 0.5058824  0.5019608\n",
      "    0.52156866]\n",
      "   [0.50980395 0.5058824  0.5254902  ... 0.50980395 0.5058824\n",
      "    0.5254902 ]\n",
      "   ...\n",
      "   [0.5686275  0.57254905 0.5882353  ... 0.5686275  0.57254905\n",
      "    0.5882353 ]\n",
      "   [0.57254905 0.5764706  0.5921569  ... 0.57254905 0.5764706\n",
      "    0.5921569 ]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]]\n",
      "\n",
      "  [[0.50980395 0.50980395 0.5254902  ... 0.50980395 0.50980395\n",
      "    0.5254902 ]\n",
      "   [0.5137255  0.5137255  0.5294118  ... 0.5137255  0.5137255\n",
      "    0.5294118 ]\n",
      "   [0.5176471  0.5176471  0.53333336 ... 0.5176471  0.5176471\n",
      "    0.53333336]\n",
      "   ...\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5803922  0.58431375 0.6        ... 0.5803922  0.58431375\n",
      "    0.6       ]]\n",
      "\n",
      "  [[0.5176471  0.5176471  0.5372549  ... 0.5176471  0.5176471\n",
      "    0.5372549 ]\n",
      "   [0.52156866 0.5254902  0.5411765  ... 0.52156866 0.5254902\n",
      "    0.5411765 ]\n",
      "   [0.5176471  0.52156866 0.5372549  ... 0.5176471  0.52156866\n",
      "    0.5372549 ]\n",
      "   ...\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.5882353  0.5921569  0.60784316 ... 0.5882353  0.5921569\n",
      "    0.60784316]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8901961  0.8392157  0.8039216  ... 0.8901961  0.8392157\n",
      "    0.8039216 ]\n",
      "   [0.91764706 0.8745098  0.8392157  ... 0.91764706 0.8745098\n",
      "    0.8392157 ]\n",
      "   [0.8627451  0.79607844 0.7490196  ... 0.8627451  0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9372549  0.90588236 0.8784314  ... 0.9372549  0.90588236\n",
      "    0.8784314 ]\n",
      "   [0.90588236 0.85882354 0.8235294  ... 0.90588236 0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.87058824 0.8117647  0.7764706  ... 0.87058824 0.8117647\n",
      "    0.7764706 ]]\n",
      "\n",
      "  [[0.88235295 0.8235294  0.78431374 ... 0.88235295 0.8235294\n",
      "    0.78431374]\n",
      "   [0.89411765 0.8392157  0.8        ... 0.89411765 0.8392157\n",
      "    0.8       ]\n",
      "   [0.87058824 0.8039216  0.7607843  ... 0.87058824 0.8039216\n",
      "    0.7607843 ]\n",
      "   ...\n",
      "   [0.92156863 0.8862745  0.8509804  ... 0.92156863 0.8862745\n",
      "    0.8509804 ]\n",
      "   [0.9098039  0.87058824 0.8352941  ... 0.9098039  0.87058824\n",
      "    0.8352941 ]\n",
      "   [0.8745098  0.8156863  0.77254903 ... 0.8745098  0.8156863\n",
      "    0.77254903]]\n",
      "\n",
      "  [[0.8627451  0.7921569  0.7411765  ... 0.8627451  0.7921569\n",
      "    0.7411765 ]\n",
      "   [0.8666667  0.8039216  0.7529412  ... 0.8666667  0.8039216\n",
      "    0.7529412 ]\n",
      "   [0.85882354 0.79607844 0.7490196  ... 0.85882354 0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9098039  0.85882354 0.8235294  ... 0.9098039  0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.90588236 0.85490197 0.8156863  ... 0.90588236 0.85490197\n",
      "    0.8156863 ]\n",
      "   [0.8784314  0.8156863  0.7647059  ... 0.8784314  0.8156863\n",
      "    0.7647059 ]]]\n",
      "\n",
      "\n",
      " [[[0.65882355 0.5411765  0.34901962 ... 0.65882355 0.5411765\n",
      "    0.34901962]\n",
      "   [0.79607844 0.6862745  0.44313726 ... 0.79607844 0.6862745\n",
      "    0.44313726]\n",
      "   [0.84705883 0.7372549  0.4862745  ... 0.84705883 0.7372549\n",
      "    0.4862745 ]\n",
      "   ...\n",
      "   [0.9019608  0.78431374 0.57254905 ... 0.9019608  0.78431374\n",
      "    0.57254905]\n",
      "   [0.8784314  0.77254903 0.5803922  ... 0.8784314  0.77254903\n",
      "    0.5803922 ]\n",
      "   [0.8666667  0.7607843  0.58431375 ... 0.8666667  0.7607843\n",
      "    0.58431375]]\n",
      "\n",
      "  [[0.6313726  0.5058824  0.27450982 ... 0.6313726  0.5058824\n",
      "    0.27450982]\n",
      "   [0.7372549  0.61960787 0.3764706  ... 0.7372549  0.61960787\n",
      "    0.3764706 ]\n",
      "   [0.83137256 0.72156864 0.4627451  ... 0.83137256 0.72156864\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.8784314  0.7607843  0.5568628  ... 0.8784314  0.7607843\n",
      "    0.5568628 ]\n",
      "   [0.85882354 0.7490196  0.56078434 ... 0.85882354 0.7490196\n",
      "    0.56078434]\n",
      "   [0.85490197 0.7529412  0.5647059  ... 0.85490197 0.7529412\n",
      "    0.5647059 ]]\n",
      "\n",
      "  [[0.6        0.46666667 0.22352941 ... 0.6        0.46666667\n",
      "    0.22352941]\n",
      "   [0.6313726  0.49803922 0.27058825 ... 0.6313726  0.49803922\n",
      "    0.27058825]\n",
      "   [0.80784315 0.69803923 0.44705883 ... 0.80784315 0.69803923\n",
      "    0.44705883]\n",
      "   ...\n",
      "   [0.8352941  0.72156864 0.5372549  ... 0.8352941  0.72156864\n",
      "    0.5372549 ]\n",
      "   [0.83137256 0.7254902  0.5411765  ... 0.83137256 0.7254902\n",
      "    0.5411765 ]\n",
      "   [0.8352941  0.73333335 0.5372549  ... 0.8352941  0.73333335\n",
      "    0.5372549 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6039216  0.3764706  0.27058825 ... 0.6039216  0.3764706\n",
      "    0.27058825]\n",
      "   [0.627451   0.40784314 0.31764707 ... 0.627451   0.40784314\n",
      "    0.31764707]\n",
      "   [0.67058825 0.47058824 0.4        ... 0.67058825 0.47058824\n",
      "    0.4       ]\n",
      "   ...\n",
      "   [0.76862746 0.58431375 0.4745098  ... 0.76862746 0.58431375\n",
      "    0.4745098 ]\n",
      "   [0.7058824  0.49803922 0.38431373 ... 0.7058824  0.49803922\n",
      "    0.38431373]\n",
      "   [0.65882355 0.44313726 0.32156864 ... 0.65882355 0.44313726\n",
      "    0.32156864]]\n",
      "\n",
      "  [[0.6        0.38039216 0.26666668 ... 0.6        0.38039216\n",
      "    0.26666668]\n",
      "   [0.63529414 0.41568628 0.32156864 ... 0.63529414 0.41568628\n",
      "    0.32156864]\n",
      "   [0.7372549  0.56078434 0.4627451  ... 0.7372549  0.56078434\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.83137256 0.654902   0.54901963 ... 0.83137256 0.654902\n",
      "    0.54901963]\n",
      "   [0.77254903 0.5647059  0.44313726 ... 0.77254903 0.5647059\n",
      "    0.44313726]\n",
      "   [0.7176471  0.5019608  0.38039216 ... 0.7176471  0.5019608\n",
      "    0.38039216]]\n",
      "\n",
      "  [[0.627451   0.41960785 0.29411766 ... 0.627451   0.41960785\n",
      "    0.29411766]\n",
      "   [0.68235296 0.46666667 0.3882353  ... 0.68235296 0.46666667\n",
      "    0.3882353 ]\n",
      "   [0.8509804  0.7294118  0.49019608 ... 0.8509804  0.7294118\n",
      "    0.49019608]\n",
      "   ...\n",
      "   [0.89411765 0.72156864 0.6117647  ... 0.89411765 0.72156864\n",
      "    0.6117647 ]\n",
      "   [0.8392157  0.627451   0.50980395 ... 0.8392157  0.627451\n",
      "    0.50980395]\n",
      "   [0.76862746 0.56078434 0.43529412 ... 0.76862746 0.56078434\n",
      "    0.43529412]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.99215686 0.99215686 0.99215686 ... 0.99215686 0.99215686\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6509804  0.6117647  0.60784316 ... 0.6509804  0.6117647\n",
      "    0.60784316]\n",
      "   [0.6431373  0.6        0.5921569  ... 0.6431373  0.6\n",
      "    0.5921569 ]\n",
      "   [0.63529414 0.5921569  0.5803922  ... 0.63529414 0.5921569\n",
      "    0.5803922 ]\n",
      "   ...\n",
      "   [0.80784315 0.69803923 0.67058825 ... 0.80784315 0.69803923\n",
      "    0.67058825]\n",
      "   [0.80784315 0.7176471  0.6901961  ... 0.80784315 0.7176471\n",
      "    0.6901961 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]]\n",
      "\n",
      "  [[0.64705884 0.59607846 0.59607846 ... 0.64705884 0.59607846\n",
      "    0.59607846]\n",
      "   [0.63529414 0.5921569  0.58431375 ... 0.63529414 0.5921569\n",
      "    0.58431375]\n",
      "   [0.62352943 0.5803922  0.57254905 ... 0.62352943 0.5803922\n",
      "    0.57254905]\n",
      "   ...\n",
      "   [0.8        0.69803923 0.6745098  ... 0.8        0.69803923\n",
      "    0.6745098 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]\n",
      "   [0.7921569  0.7019608  0.68235296 ... 0.7921569  0.7019608\n",
      "    0.68235296]]\n",
      "\n",
      "  [[0.6431373  0.59607846 0.5921569  ... 0.6431373  0.59607846\n",
      "    0.5921569 ]\n",
      "   [0.6313726  0.5882353  0.5803922  ... 0.6313726  0.5882353\n",
      "    0.5803922 ]\n",
      "   [0.627451   0.57254905 0.5686275  ... 0.627451   0.57254905\n",
      "    0.5686275 ]\n",
      "   ...\n",
      "   [0.7921569  0.6901961  0.6784314  ... 0.7921569  0.6901961\n",
      "    0.6784314 ]\n",
      "   [0.80784315 0.7058824  0.6901961  ... 0.80784315 0.7058824\n",
      "    0.6901961 ]\n",
      "   [0.80784315 0.7176471  0.7019608  ... 0.80784315 0.7176471\n",
      "    0.7019608 ]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (65, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (65, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 2.4396 - accuracy: 0.2462    30\n",
      "30\n",
      "[[[[243 243 243 ... 243 243 243]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  [[242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  [[242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[243 242 244 ... 243 242 244]\n",
      "   [245 242 248 ... 245 242 248]\n",
      "   [250 250 253 ... 250 250 253]\n",
      "   ...\n",
      "   [214 218 239 ... 214 218 239]\n",
      "   [205 209 233 ... 205 209 233]\n",
      "   [213 215 234 ... 213 215 234]]\n",
      "\n",
      "  [[238 236 241 ... 238 236 241]\n",
      "   [235 230 238 ... 235 230 238]\n",
      "   [240 236 243 ... 240 236 243]\n",
      "   ...\n",
      "   [210 214 238 ... 210 214 238]\n",
      "   [202 206 232 ... 202 206 232]\n",
      "   [206 208 231 ... 206 208 231]]\n",
      "\n",
      "  [[226 219 227 ... 226 219 227]\n",
      "   [221 211 220 ... 221 211 220]\n",
      "   [219 210 222 ... 219 210 222]\n",
      "   ...\n",
      "   [207 212 237 ... 207 212 237]\n",
      "   [207 212 236 ... 207 212 236]\n",
      "   [207 210 233 ... 207 210 233]]]\n",
      "\n",
      "\n",
      " [[[  9   9   2 ...   9   9   2]\n",
      "   [  6   6   2 ...   6   6   2]\n",
      "   [  4   3   1 ...   4   3   1]\n",
      "   ...\n",
      "   [  0   1   1 ...   0   1   1]\n",
      "   [  1   2   2 ...   1   2   2]\n",
      "   [  9  10   7 ...   9  10   7]]\n",
      "\n",
      "  [[  9   8   4 ...   9   8   4]\n",
      "   [  9   9   4 ...   9   9   4]\n",
      "   [  9   8   3 ...   9   8   3]\n",
      "   ...\n",
      "   [  1   1   1 ...   1   1   1]\n",
      "   [  4   4   3 ...   4   4   3]\n",
      "   [  4   5   3 ...   4   5   3]]\n",
      "\n",
      "  [[ 26  23  14 ...  26  23  14]\n",
      "   [ 33  27  15 ...  33  27  15]\n",
      "   [ 45  37  24 ...  45  37  24]\n",
      "   ...\n",
      "   [  2   3   2 ...   2   3   2]\n",
      "   [  2   2   2 ...   2   2   2]\n",
      "   [  4   4   3 ...   4   4   3]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[188 178 161 ... 188 178 161]\n",
      "   [190 180 162 ... 190 180 162]\n",
      "   [187 177 158 ... 187 177 158]\n",
      "   ...\n",
      "   [150 142 124 ... 150 142 124]\n",
      "   [145 135 118 ... 145 135 118]\n",
      "   [145 135 117 ... 145 135 117]]\n",
      "\n",
      "  [[197 186 170 ... 197 186 170]\n",
      "   [194 184 168 ... 194 184 168]\n",
      "   [192 181 164 ... 192 181 164]\n",
      "   ...\n",
      "   [148 139 120 ... 148 139 120]\n",
      "   [149 138 119 ... 149 138 119]\n",
      "   [147 136 117 ... 147 136 117]]\n",
      "\n",
      "  [[201 191 174 ... 201 191 174]\n",
      "   [198 188 172 ... 198 188 172]\n",
      "   [194 183 167 ... 194 183 167]\n",
      "   ...\n",
      "   [150 139 120 ... 150 139 120]\n",
      "   [152 141 121 ... 152 141 121]\n",
      "   [152 142 124 ... 152 142 124]]]\n",
      "\n",
      "\n",
      " [[[ 93  64  41 ...  93  64  41]\n",
      "   [101  73  50 ... 101  73  50]\n",
      "   [109  83  61 ... 109  83  61]\n",
      "   ...\n",
      "   [ 92  57  35 ...  92  57  35]\n",
      "   [ 88  52  30 ...  88  52  30]\n",
      "   [ 83  47  26 ...  83  47  26]]\n",
      "\n",
      "  [[101  73  50 ... 101  73  50]\n",
      "   [109  82  60 ... 109  82  60]\n",
      "   [116  92  70 ... 116  92  70]\n",
      "   ...\n",
      "   [ 97  62  39 ...  97  62  39]\n",
      "   [ 92  56  34 ...  92  56  34]\n",
      "   [ 87  51  30 ...  87  51  30]]\n",
      "\n",
      "  [[108  82  59 ... 108  82  59]\n",
      "   [115  91  70 ... 115  91  70]\n",
      "   [122 100  81 ... 122 100  81]\n",
      "   ...\n",
      "   [101  66  44 ... 101  66  44]\n",
      "   [ 96  60  38 ...  96  60  38]\n",
      "   [ 91  54  33 ...  91  54  33]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 87  89  90 ...  87  89  90]\n",
      "   [ 98 100 102 ...  98 100 102]\n",
      "   [112 115 118 ... 112 115 118]\n",
      "   ...\n",
      "   [242 187 163 ... 242 187 163]\n",
      "   [230 169 145 ... 230 169 145]\n",
      "   [209 149 124 ... 209 149 124]]\n",
      "\n",
      "  [[ 81  83  83 ...  81  83  83]\n",
      "   [ 87  89  91 ...  87  89  91]\n",
      "   [106 108 111 ... 106 108 111]\n",
      "   ...\n",
      "   [235 180 155 ... 235 180 155]\n",
      "   [224 167 141 ... 224 167 141]\n",
      "   [204 143 118 ... 204 143 118]]\n",
      "\n",
      "  [[ 78  80  80 ...  78  80  80]\n",
      "   [ 79  80  82 ...  79  80  82]\n",
      "   [ 94  96  99 ...  94  96  99]\n",
      "   ...\n",
      "   [229 174 149 ... 229 174 149]\n",
      "   [218 160 135 ... 218 160 135]\n",
      "   [197 137 113 ... 197 137 113]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (70, 40, 30, 9)\n",
      "70 train samples\n",
      "30 test samples\n",
      "y_train shape: (70, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_651 (Conv2D)             (None, 40, 30, 16)   1312        input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_588 (BatchN (None, 40, 30, 16)   64          conv2d_651[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_588 (Activation)     (None, 40, 30, 16)   0           batch_normalization_588[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_652 (Conv2D)             (None, 40, 30, 16)   272         activation_588[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_589 (BatchN (None, 40, 30, 16)   64          conv2d_652[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_589 (Activation)     (None, 40, 30, 16)   0           batch_normalization_589[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_653 (Conv2D)             (None, 40, 30, 16)   2320        activation_589[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_590 (BatchN (None, 40, 30, 16)   64          conv2d_653[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_590 (Activation)     (None, 40, 30, 16)   0           batch_normalization_590[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_655 (Conv2D)             (None, 40, 30, 64)   1088        activation_588[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_654 (Conv2D)             (None, 40, 30, 64)   1088        activation_590[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_189 (Add)                   (None, 40, 30, 64)   0           conv2d_655[0][0]                 \n",
      "                                                                 conv2d_654[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_591 (BatchN (None, 40, 30, 64)   256         add_189[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_591 (Activation)     (None, 40, 30, 64)   0           batch_normalization_591[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_656 (Conv2D)             (None, 40, 30, 16)   1040        activation_591[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_592 (BatchN (None, 40, 30, 16)   64          conv2d_656[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_592 (Activation)     (None, 40, 30, 16)   0           batch_normalization_592[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_657 (Conv2D)             (None, 40, 30, 16)   2320        activation_592[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_593 (BatchN (None, 40, 30, 16)   64          conv2d_657[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_593 (Activation)     (None, 40, 30, 16)   0           batch_normalization_593[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_658 (Conv2D)             (None, 40, 30, 64)   1088        activation_593[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_190 (Add)                   (None, 40, 30, 64)   0           add_189[0][0]                    \n",
      "                                                                 conv2d_658[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_594 (BatchN (None, 40, 30, 64)   256         add_190[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_594 (Activation)     (None, 40, 30, 64)   0           batch_normalization_594[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_659 (Conv2D)             (None, 40, 30, 16)   1040        activation_594[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_595 (BatchN (None, 40, 30, 16)   64          conv2d_659[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_595 (Activation)     (None, 40, 30, 16)   0           batch_normalization_595[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_660 (Conv2D)             (None, 40, 30, 16)   2320        activation_595[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_596 (BatchN (None, 40, 30, 16)   64          conv2d_660[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_596 (Activation)     (None, 40, 30, 16)   0           batch_normalization_596[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_661 (Conv2D)             (None, 40, 30, 64)   1088        activation_596[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_191 (Add)                   (None, 40, 30, 64)   0           add_190[0][0]                    \n",
      "                                                                 conv2d_661[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_597 (BatchN (None, 40, 30, 64)   256         add_191[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_597 (Activation)     (None, 40, 30, 64)   0           batch_normalization_597[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_662 (Conv2D)             (None, 20, 15, 64)   4160        activation_597[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_598 (BatchN (None, 20, 15, 64)   256         conv2d_662[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_598 (Activation)     (None, 20, 15, 64)   0           batch_normalization_598[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_663 (Conv2D)             (None, 20, 15, 64)   36928       activation_598[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_599 (BatchN (None, 20, 15, 64)   256         conv2d_663[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_599 (Activation)     (None, 20, 15, 64)   0           batch_normalization_599[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_665 (Conv2D)             (None, 20, 15, 128)  8320        add_191[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_664 (Conv2D)             (None, 20, 15, 128)  8320        activation_599[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_192 (Add)                   (None, 20, 15, 128)  0           conv2d_665[0][0]                 \n",
      "                                                                 conv2d_664[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_600 (BatchN (None, 20, 15, 128)  512         add_192[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_600 (Activation)     (None, 20, 15, 128)  0           batch_normalization_600[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_666 (Conv2D)             (None, 20, 15, 64)   8256        activation_600[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_601 (BatchN (None, 20, 15, 64)   256         conv2d_666[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_601 (Activation)     (None, 20, 15, 64)   0           batch_normalization_601[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_667 (Conv2D)             (None, 20, 15, 64)   36928       activation_601[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_602 (BatchN (None, 20, 15, 64)   256         conv2d_667[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_602 (Activation)     (None, 20, 15, 64)   0           batch_normalization_602[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_668 (Conv2D)             (None, 20, 15, 128)  8320        activation_602[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_193 (Add)                   (None, 20, 15, 128)  0           add_192[0][0]                    \n",
      "                                                                 conv2d_668[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_603 (BatchN (None, 20, 15, 128)  512         add_193[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_603 (Activation)     (None, 20, 15, 128)  0           batch_normalization_603[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_669 (Conv2D)             (None, 20, 15, 64)   8256        activation_603[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_604 (BatchN (None, 20, 15, 64)   256         conv2d_669[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_604 (Activation)     (None, 20, 15, 64)   0           batch_normalization_604[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_670 (Conv2D)             (None, 20, 15, 64)   36928       activation_604[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_605 (BatchN (None, 20, 15, 64)   256         conv2d_670[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_605 (Activation)     (None, 20, 15, 64)   0           batch_normalization_605[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_671 (Conv2D)             (None, 20, 15, 128)  8320        activation_605[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_194 (Add)                   (None, 20, 15, 128)  0           add_193[0][0]                    \n",
      "                                                                 conv2d_671[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_606 (BatchN (None, 20, 15, 128)  512         add_194[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_606 (Activation)     (None, 20, 15, 128)  0           batch_normalization_606[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_672 (Conv2D)             (None, 10, 8, 128)   16512       activation_606[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_607 (BatchN (None, 10, 8, 128)   512         conv2d_672[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_607 (Activation)     (None, 10, 8, 128)   0           batch_normalization_607[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_673 (Conv2D)             (None, 10, 8, 128)   147584      activation_607[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_608 (BatchN (None, 10, 8, 128)   512         conv2d_673[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_608 (Activation)     (None, 10, 8, 128)   0           batch_normalization_608[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_675 (Conv2D)             (None, 10, 8, 256)   33024       add_194[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_674 (Conv2D)             (None, 10, 8, 256)   33024       activation_608[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_195 (Add)                   (None, 10, 8, 256)   0           conv2d_675[0][0]                 \n",
      "                                                                 conv2d_674[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_609 (BatchN (None, 10, 8, 256)   1024        add_195[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_609 (Activation)     (None, 10, 8, 256)   0           batch_normalization_609[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_676 (Conv2D)             (None, 10, 8, 128)   32896       activation_609[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_610 (BatchN (None, 10, 8, 128)   512         conv2d_676[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_610 (Activation)     (None, 10, 8, 128)   0           batch_normalization_610[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_677 (Conv2D)             (None, 10, 8, 128)   147584      activation_610[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_611 (BatchN (None, 10, 8, 128)   512         conv2d_677[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_611 (Activation)     (None, 10, 8, 128)   0           batch_normalization_611[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_678 (Conv2D)             (None, 10, 8, 256)   33024       activation_611[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_196 (Add)                   (None, 10, 8, 256)   0           add_195[0][0]                    \n",
      "                                                                 conv2d_678[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_612 (BatchN (None, 10, 8, 256)   1024        add_196[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_612 (Activation)     (None, 10, 8, 256)   0           batch_normalization_612[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_679 (Conv2D)             (None, 10, 8, 128)   32896       activation_612[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_613 (BatchN (None, 10, 8, 128)   512         conv2d_679[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_613 (Activation)     (None, 10, 8, 128)   0           batch_normalization_613[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_680 (Conv2D)             (None, 10, 8, 128)   147584      activation_613[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_614 (BatchN (None, 10, 8, 128)   512         conv2d_680[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_614 (Activation)     (None, 10, 8, 128)   0           batch_normalization_614[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_681 (Conv2D)             (None, 10, 8, 256)   33024       activation_614[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_197 (Add)                   (None, 10, 8, 256)   0           add_196[0][0]                    \n",
      "                                                                 conv2d_681[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_615 (BatchN (None, 10, 8, 256)   1024        add_197[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_615 (Activation)     (None, 10, 8, 256)   0           batch_normalization_615[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_21 (AveragePo (None, 1, 1, 256)    0           activation_615[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 256)          0           average_pooling2d_21[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 10)           2570        flatten_21[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[0.9529412  0.9529412  0.9529412  ... 0.9529412  0.9529412\n",
      "    0.9529412 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  [[0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  [[0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.9529412  0.9490196  0.95686275 ... 0.9529412  0.9490196\n",
      "    0.95686275]\n",
      "   [0.9607843  0.9490196  0.972549   ... 0.9607843  0.9490196\n",
      "    0.972549  ]\n",
      "   [0.98039216 0.98039216 0.99215686 ... 0.98039216 0.98039216\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [0.8392157  0.85490197 0.9372549  ... 0.8392157  0.85490197\n",
      "    0.9372549 ]\n",
      "   [0.8039216  0.81960785 0.9137255  ... 0.8039216  0.81960785\n",
      "    0.9137255 ]\n",
      "   [0.8352941  0.84313726 0.91764706 ... 0.8352941  0.84313726\n",
      "    0.91764706]]\n",
      "\n",
      "  [[0.93333334 0.9254902  0.94509804 ... 0.93333334 0.9254902\n",
      "    0.94509804]\n",
      "   [0.92156863 0.9019608  0.93333334 ... 0.92156863 0.9019608\n",
      "    0.93333334]\n",
      "   [0.9411765  0.9254902  0.9529412  ... 0.9411765  0.9254902\n",
      "    0.9529412 ]\n",
      "   ...\n",
      "   [0.8235294  0.8392157  0.93333334 ... 0.8235294  0.8392157\n",
      "    0.93333334]\n",
      "   [0.7921569  0.80784315 0.9098039  ... 0.7921569  0.80784315\n",
      "    0.9098039 ]\n",
      "   [0.80784315 0.8156863  0.90588236 ... 0.80784315 0.8156863\n",
      "    0.90588236]]\n",
      "\n",
      "  [[0.8862745  0.85882354 0.8901961  ... 0.8862745  0.85882354\n",
      "    0.8901961 ]\n",
      "   [0.8666667  0.827451   0.8627451  ... 0.8666667  0.827451\n",
      "    0.8627451 ]\n",
      "   [0.85882354 0.8235294  0.87058824 ... 0.85882354 0.8235294\n",
      "    0.87058824]\n",
      "   ...\n",
      "   [0.8117647  0.83137256 0.92941177 ... 0.8117647  0.83137256\n",
      "    0.92941177]\n",
      "   [0.8117647  0.83137256 0.9254902  ... 0.8117647  0.83137256\n",
      "    0.9254902 ]\n",
      "   [0.8117647  0.8235294  0.9137255  ... 0.8117647  0.8235294\n",
      "    0.9137255 ]]]\n",
      "\n",
      "\n",
      " [[[0.03529412 0.03529412 0.00784314 ... 0.03529412 0.03529412\n",
      "    0.00784314]\n",
      "   [0.02352941 0.02352941 0.00784314 ... 0.02352941 0.02352941\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01176471 0.00392157 ... 0.01568628 0.01176471\n",
      "    0.00392157]\n",
      "   ...\n",
      "   [0.         0.00392157 0.00392157 ... 0.         0.00392157\n",
      "    0.00392157]\n",
      "   [0.00392157 0.00784314 0.00784314 ... 0.00392157 0.00784314\n",
      "    0.00784314]\n",
      "   [0.03529412 0.03921569 0.02745098 ... 0.03529412 0.03921569\n",
      "    0.02745098]]\n",
      "\n",
      "  [[0.03529412 0.03137255 0.01568628 ... 0.03529412 0.03137255\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03529412 0.01568628 ... 0.03529412 0.03529412\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03137255 0.01176471 ... 0.03529412 0.03137255\n",
      "    0.01176471]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157 ... 0.00392157 0.00392157\n",
      "    0.00392157]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]\n",
      "   [0.01568628 0.01960784 0.01176471 ... 0.01568628 0.01960784\n",
      "    0.01176471]]\n",
      "\n",
      "  [[0.10196079 0.09019608 0.05490196 ... 0.10196079 0.09019608\n",
      "    0.05490196]\n",
      "   [0.12941177 0.10588235 0.05882353 ... 0.12941177 0.10588235\n",
      "    0.05882353]\n",
      "   [0.1764706  0.14509805 0.09411765 ... 0.1764706  0.14509805\n",
      "    0.09411765]\n",
      "   ...\n",
      "   [0.00784314 0.01176471 0.00784314 ... 0.00784314 0.01176471\n",
      "    0.00784314]\n",
      "   [0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.7372549  0.69803923 0.6313726  ... 0.7372549  0.69803923\n",
      "    0.6313726 ]\n",
      "   [0.74509805 0.7058824  0.63529414 ... 0.74509805 0.7058824\n",
      "    0.63529414]\n",
      "   [0.73333335 0.69411767 0.61960787 ... 0.73333335 0.69411767\n",
      "    0.61960787]\n",
      "   ...\n",
      "   [0.5882353  0.5568628  0.4862745  ... 0.5882353  0.5568628\n",
      "    0.4862745 ]\n",
      "   [0.5686275  0.5294118  0.4627451  ... 0.5686275  0.5294118\n",
      "    0.4627451 ]\n",
      "   [0.5686275  0.5294118  0.45882353 ... 0.5686275  0.5294118\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.77254903 0.7294118  0.6666667  ... 0.77254903 0.7294118\n",
      "    0.6666667 ]\n",
      "   [0.7607843  0.72156864 0.65882355 ... 0.7607843  0.72156864\n",
      "    0.65882355]\n",
      "   [0.7529412  0.70980394 0.6431373  ... 0.7529412  0.70980394\n",
      "    0.6431373 ]\n",
      "   ...\n",
      "   [0.5803922  0.54509807 0.47058824 ... 0.5803922  0.54509807\n",
      "    0.47058824]\n",
      "   [0.58431375 0.5411765  0.46666667 ... 0.58431375 0.5411765\n",
      "    0.46666667]\n",
      "   [0.5764706  0.53333336 0.45882353 ... 0.5764706  0.53333336\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.7882353  0.7490196  0.68235296 ... 0.7882353  0.7490196\n",
      "    0.68235296]\n",
      "   [0.7764706  0.7372549  0.6745098  ... 0.7764706  0.7372549\n",
      "    0.6745098 ]\n",
      "   [0.7607843  0.7176471  0.654902   ... 0.7607843  0.7176471\n",
      "    0.654902  ]\n",
      "   ...\n",
      "   [0.5882353  0.54509807 0.47058824 ... 0.5882353  0.54509807\n",
      "    0.47058824]\n",
      "   [0.59607846 0.5529412  0.4745098  ... 0.59607846 0.5529412\n",
      "    0.4745098 ]\n",
      "   [0.59607846 0.5568628  0.4862745  ... 0.59607846 0.5568628\n",
      "    0.4862745 ]]]\n",
      "\n",
      "\n",
      " [[[0.3647059  0.2509804  0.16078432 ... 0.3647059  0.2509804\n",
      "    0.16078432]\n",
      "   [0.39607844 0.28627452 0.19607843 ... 0.39607844 0.28627452\n",
      "    0.19607843]\n",
      "   [0.42745098 0.3254902  0.23921569 ... 0.42745098 0.3254902\n",
      "    0.23921569]\n",
      "   ...\n",
      "   [0.36078432 0.22352941 0.13725491 ... 0.36078432 0.22352941\n",
      "    0.13725491]\n",
      "   [0.34509805 0.20392157 0.11764706 ... 0.34509805 0.20392157\n",
      "    0.11764706]\n",
      "   [0.3254902  0.18431373 0.10196079 ... 0.3254902  0.18431373\n",
      "    0.10196079]]\n",
      "\n",
      "  [[0.39607844 0.28627452 0.19607843 ... 0.39607844 0.28627452\n",
      "    0.19607843]\n",
      "   [0.42745098 0.32156864 0.23529412 ... 0.42745098 0.32156864\n",
      "    0.23529412]\n",
      "   [0.45490196 0.36078432 0.27450982 ... 0.45490196 0.36078432\n",
      "    0.27450982]\n",
      "   ...\n",
      "   [0.38039216 0.24313726 0.15294118 ... 0.38039216 0.24313726\n",
      "    0.15294118]\n",
      "   [0.36078432 0.21960784 0.13333334 ... 0.36078432 0.21960784\n",
      "    0.13333334]\n",
      "   [0.34117648 0.2        0.11764706 ... 0.34117648 0.2\n",
      "    0.11764706]]\n",
      "\n",
      "  [[0.42352942 0.32156864 0.23137255 ... 0.42352942 0.32156864\n",
      "    0.23137255]\n",
      "   [0.4509804  0.35686275 0.27450982 ... 0.4509804  0.35686275\n",
      "    0.27450982]\n",
      "   [0.47843137 0.39215687 0.31764707 ... 0.47843137 0.39215687\n",
      "    0.31764707]\n",
      "   ...\n",
      "   [0.39607844 0.25882354 0.17254902 ... 0.39607844 0.25882354\n",
      "    0.17254902]\n",
      "   [0.3764706  0.23529412 0.14901961 ... 0.3764706  0.23529412\n",
      "    0.14901961]\n",
      "   [0.35686275 0.21176471 0.12941177 ... 0.35686275 0.21176471\n",
      "    0.12941177]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.34117648 0.34901962 0.3529412  ... 0.34117648 0.34901962\n",
      "    0.3529412 ]\n",
      "   [0.38431373 0.39215687 0.4        ... 0.38431373 0.39215687\n",
      "    0.4       ]\n",
      "   [0.4392157  0.4509804  0.4627451  ... 0.4392157  0.4509804\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.9490196  0.73333335 0.6392157  ... 0.9490196  0.73333335\n",
      "    0.6392157 ]\n",
      "   [0.9019608  0.6627451  0.5686275  ... 0.9019608  0.6627451\n",
      "    0.5686275 ]\n",
      "   [0.81960785 0.58431375 0.4862745  ... 0.81960785 0.58431375\n",
      "    0.4862745 ]]\n",
      "\n",
      "  [[0.31764707 0.3254902  0.3254902  ... 0.31764707 0.3254902\n",
      "    0.3254902 ]\n",
      "   [0.34117648 0.34901962 0.35686275 ... 0.34117648 0.34901962\n",
      "    0.35686275]\n",
      "   [0.41568628 0.42352942 0.43529412 ... 0.41568628 0.42352942\n",
      "    0.43529412]\n",
      "   ...\n",
      "   [0.92156863 0.7058824  0.60784316 ... 0.92156863 0.7058824\n",
      "    0.60784316]\n",
      "   [0.8784314  0.654902   0.5529412  ... 0.8784314  0.654902\n",
      "    0.5529412 ]\n",
      "   [0.8        0.56078434 0.4627451  ... 0.8        0.56078434\n",
      "    0.4627451 ]]\n",
      "\n",
      "  [[0.30588236 0.3137255  0.3137255  ... 0.30588236 0.3137255\n",
      "    0.3137255 ]\n",
      "   [0.30980393 0.3137255  0.32156864 ... 0.30980393 0.3137255\n",
      "    0.32156864]\n",
      "   [0.36862746 0.3764706  0.3882353  ... 0.36862746 0.3764706\n",
      "    0.3882353 ]\n",
      "   ...\n",
      "   [0.8980392  0.68235296 0.58431375 ... 0.8980392  0.68235296\n",
      "    0.58431375]\n",
      "   [0.85490197 0.627451   0.5294118  ... 0.85490197 0.627451\n",
      "    0.5294118 ]\n",
      "   [0.77254903 0.5372549  0.44313726 ... 0.77254903 0.5372549\n",
      "    0.44313726]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157 ... 0.00392157 0.00392157\n",
      "    0.00392157]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.61960787 0.62352943 0.6784314  ... 0.61960787 0.62352943\n",
      "    0.6784314 ]\n",
      "   [0.6784314  0.6862745  0.73333335 ... 0.6784314  0.6862745\n",
      "    0.73333335]\n",
      "   [0.5019608  0.5058824  0.5568628  ... 0.5019608  0.5058824\n",
      "    0.5568628 ]\n",
      "   ...\n",
      "   [0.25490198 0.26666668 0.30588236 ... 0.25490198 0.26666668\n",
      "    0.30588236]\n",
      "   [0.16078432 0.16078432 0.21176471 ... 0.16078432 0.16078432\n",
      "    0.21176471]\n",
      "   [0.15686275 0.15294118 0.2        ... 0.15686275 0.15294118\n",
      "    0.2       ]]\n",
      "\n",
      "  [[0.59607846 0.6039216  0.65882355 ... 0.59607846 0.6039216\n",
      "    0.65882355]\n",
      "   [0.6        0.60784316 0.6627451  ... 0.6        0.60784316\n",
      "    0.6627451 ]\n",
      "   [0.4627451  0.46666667 0.52156866 ... 0.4627451  0.46666667\n",
      "    0.52156866]\n",
      "   ...\n",
      "   [0.16078432 0.16862746 0.19607843 ... 0.16078432 0.16862746\n",
      "    0.19607843]\n",
      "   [0.1254902  0.12156863 0.16470589 ... 0.1254902  0.12156863\n",
      "    0.16470589]\n",
      "   [0.12156863 0.12156863 0.16078432 ... 0.12156863 0.12156863\n",
      "    0.16078432]]\n",
      "\n",
      "  [[0.49411765 0.49019608 0.5568628  ... 0.49411765 0.49019608\n",
      "    0.5568628 ]\n",
      "   [0.5019608  0.49803922 0.5647059  ... 0.5019608  0.49803922\n",
      "    0.5647059 ]\n",
      "   [0.4745098  0.47058824 0.5294118  ... 0.4745098  0.47058824\n",
      "    0.5294118 ]\n",
      "   ...\n",
      "   [0.02745098 0.02352941 0.03137255 ... 0.02745098 0.02352941\n",
      "    0.03137255]\n",
      "   [0.09803922 0.09019608 0.1254902  ... 0.09803922 0.09019608\n",
      "    0.1254902 ]\n",
      "   [0.09411765 0.08627451 0.12156863 ... 0.09411765 0.08627451\n",
      "    0.12156863]]]\n",
      "\n",
      "\n",
      " [[[0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   ...\n",
      "   [0.8980392  0.9098039  0.9372549  ... 0.8980392  0.9098039\n",
      "    0.9372549 ]\n",
      "   [0.8901961  0.9019608  0.92941177 ... 0.8901961  0.9019608\n",
      "    0.92941177]\n",
      "   [0.8862745  0.8980392  0.9254902  ... 0.8862745  0.8980392\n",
      "    0.9254902 ]]\n",
      "\n",
      "  [[0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   ...\n",
      "   [0.8901961  0.9019608  0.92941177 ... 0.8901961  0.9019608\n",
      "    0.92941177]\n",
      "   [0.8862745  0.89411765 0.92156863 ... 0.8862745  0.89411765\n",
      "    0.92156863]\n",
      "   [0.8862745  0.8901961  0.92156863 ... 0.8862745  0.8901961\n",
      "    0.92156863]]\n",
      "\n",
      "  [[0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.9647059  ... 0.96862745 0.96862745\n",
      "    0.9647059 ]\n",
      "   [0.972549   0.972549   0.9647059  ... 0.972549   0.972549\n",
      "    0.9647059 ]\n",
      "   ...\n",
      "   [0.8862745  0.8901961  0.92156863 ... 0.8862745  0.8901961\n",
      "    0.92156863]\n",
      "   [0.8862745  0.8862745  0.91764706 ... 0.8862745  0.8862745\n",
      "    0.91764706]\n",
      "   [0.88235295 0.88235295 0.9137255  ... 0.88235295 0.88235295\n",
      "    0.9137255 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.827451   0.8117647  0.8352941  ... 0.827451   0.8117647\n",
      "    0.8352941 ]\n",
      "   [0.8352941  0.8156863  0.8352941  ... 0.8352941  0.8156863\n",
      "    0.8352941 ]\n",
      "   [0.8666667  0.8509804  0.8666667  ... 0.8666667  0.8509804\n",
      "    0.8666667 ]\n",
      "   ...\n",
      "   [0.7490196  0.74509805 0.76862746 ... 0.7490196  0.74509805\n",
      "    0.76862746]\n",
      "   [0.7647059  0.75686276 0.7921569  ... 0.7647059  0.75686276\n",
      "    0.7921569 ]\n",
      "   [0.78431374 0.77254903 0.81960785 ... 0.78431374 0.77254903\n",
      "    0.81960785]]\n",
      "\n",
      "  [[0.84705883 0.827451   0.8509804  ... 0.84705883 0.827451\n",
      "    0.8509804 ]\n",
      "   [0.8509804  0.83137256 0.84705883 ... 0.8509804  0.83137256\n",
      "    0.84705883]\n",
      "   [0.88235295 0.8627451  0.8784314  ... 0.88235295 0.8627451\n",
      "    0.8784314 ]\n",
      "   ...\n",
      "   [0.7372549  0.73333335 0.75686276 ... 0.7372549  0.73333335\n",
      "    0.75686276]\n",
      "   [0.75686276 0.7490196  0.7764706  ... 0.75686276 0.7490196\n",
      "    0.7764706 ]\n",
      "   [0.77254903 0.7647059  0.8039216  ... 0.77254903 0.7647059\n",
      "    0.8039216 ]]\n",
      "\n",
      "  [[0.8509804  0.8352941  0.85490197 ... 0.8509804  0.8352941\n",
      "    0.85490197]\n",
      "   [0.8666667  0.84705883 0.8627451  ... 0.8666667  0.84705883\n",
      "    0.8627451 ]\n",
      "   [0.8901961  0.87058824 0.8862745  ... 0.8901961  0.87058824\n",
      "    0.8862745 ]\n",
      "   ...\n",
      "   [0.7254902  0.7254902  0.7490196  ... 0.7254902  0.7254902\n",
      "    0.7490196 ]\n",
      "   [0.7411765  0.7372549  0.7647059  ... 0.7411765  0.7372549\n",
      "    0.7647059 ]\n",
      "   [0.7607843  0.7529412  0.78431374 ... 0.7607843  0.7529412\n",
      "    0.78431374]]]\n",
      "\n",
      "\n",
      " [[[0.49803922 0.49803922 0.5176471  ... 0.49803922 0.49803922\n",
      "    0.5176471 ]\n",
      "   [0.5058824  0.5019608  0.52156866 ... 0.5058824  0.5019608\n",
      "    0.52156866]\n",
      "   [0.50980395 0.5058824  0.5254902  ... 0.50980395 0.5058824\n",
      "    0.5254902 ]\n",
      "   ...\n",
      "   [0.5686275  0.57254905 0.5882353  ... 0.5686275  0.57254905\n",
      "    0.5882353 ]\n",
      "   [0.57254905 0.5764706  0.5921569  ... 0.57254905 0.5764706\n",
      "    0.5921569 ]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]]\n",
      "\n",
      "  [[0.50980395 0.50980395 0.5254902  ... 0.50980395 0.50980395\n",
      "    0.5254902 ]\n",
      "   [0.5137255  0.5137255  0.5294118  ... 0.5137255  0.5137255\n",
      "    0.5294118 ]\n",
      "   [0.5176471  0.5176471  0.53333336 ... 0.5176471  0.5176471\n",
      "    0.53333336]\n",
      "   ...\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5803922  0.58431375 0.6        ... 0.5803922  0.58431375\n",
      "    0.6       ]]\n",
      "\n",
      "  [[0.5176471  0.5176471  0.5372549  ... 0.5176471  0.5176471\n",
      "    0.5372549 ]\n",
      "   [0.52156866 0.5254902  0.5411765  ... 0.52156866 0.5254902\n",
      "    0.5411765 ]\n",
      "   [0.5176471  0.52156866 0.5372549  ... 0.5176471  0.52156866\n",
      "    0.5372549 ]\n",
      "   ...\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.5882353  0.5921569  0.60784316 ... 0.5882353  0.5921569\n",
      "    0.60784316]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8901961  0.8392157  0.8039216  ... 0.8901961  0.8392157\n",
      "    0.8039216 ]\n",
      "   [0.91764706 0.8745098  0.8392157  ... 0.91764706 0.8745098\n",
      "    0.8392157 ]\n",
      "   [0.8627451  0.79607844 0.7490196  ... 0.8627451  0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9372549  0.90588236 0.8784314  ... 0.9372549  0.90588236\n",
      "    0.8784314 ]\n",
      "   [0.90588236 0.85882354 0.8235294  ... 0.90588236 0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.87058824 0.8117647  0.7764706  ... 0.87058824 0.8117647\n",
      "    0.7764706 ]]\n",
      "\n",
      "  [[0.88235295 0.8235294  0.78431374 ... 0.88235295 0.8235294\n",
      "    0.78431374]\n",
      "   [0.89411765 0.8392157  0.8        ... 0.89411765 0.8392157\n",
      "    0.8       ]\n",
      "   [0.87058824 0.8039216  0.7607843  ... 0.87058824 0.8039216\n",
      "    0.7607843 ]\n",
      "   ...\n",
      "   [0.92156863 0.8862745  0.8509804  ... 0.92156863 0.8862745\n",
      "    0.8509804 ]\n",
      "   [0.9098039  0.87058824 0.8352941  ... 0.9098039  0.87058824\n",
      "    0.8352941 ]\n",
      "   [0.8745098  0.8156863  0.77254903 ... 0.8745098  0.8156863\n",
      "    0.77254903]]\n",
      "\n",
      "  [[0.8627451  0.7921569  0.7411765  ... 0.8627451  0.7921569\n",
      "    0.7411765 ]\n",
      "   [0.8666667  0.8039216  0.7529412  ... 0.8666667  0.8039216\n",
      "    0.7529412 ]\n",
      "   [0.85882354 0.79607844 0.7490196  ... 0.85882354 0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9098039  0.85882354 0.8235294  ... 0.9098039  0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.90588236 0.85490197 0.8156863  ... 0.90588236 0.85490197\n",
      "    0.8156863 ]\n",
      "   [0.8784314  0.8156863  0.7647059  ... 0.8784314  0.8156863\n",
      "    0.7647059 ]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (70, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (70, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 207 calls to <function Model.make_train_function.<locals>.train_function at 0x000001801A03D280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 6s 3s/step - loss: 2.9723 - accuracy: 0.0429 - val_loss: 2.9140 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/100\n",
      "Learning rate:  1e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.9680 - accuracy: 0.0429 - val_loss: 2.9261 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/100\n",
      "Learning rate:  2e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.9530 - accuracy: 0.0714 - val_loss: 2.9221 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/100\n",
      "Learning rate:  3e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.9356 - accuracy: 0.0286 - val_loss: 2.9017 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/100\n",
      "Learning rate:  4e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.9079 - accuracy: 0.0429 - val_loss: 2.8756 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/100\n",
      "Learning rate:  5e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.8741 - accuracy: 0.1000 - val_loss: 2.8427 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/100\n",
      "Learning rate:  6e-05\n",
      "2/2 [==============================] - 2s 362ms/step - loss: 2.8389 - accuracy: 0.0857 - val_loss: 2.8005 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/100\n",
      "Learning rate:  7.000000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.7718 - accuracy: 0.1714 - val_loss: 2.7561 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/100\n",
      "Learning rate:  8e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.7080 - accuracy: 0.1857 - val_loss: 2.7011 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/100\n",
      "Learning rate:  8.999999999999999e-05\n",
      "2/2 [==============================] - 2s 413ms/step - loss: 2.6625 - accuracy: 0.2571 - val_loss: 2.6424 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/100\n",
      "Learning rate:  0.0001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.5629 - accuracy: 0.4714 - val_loss: 2.5773 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/100\n",
      "Learning rate:  0.00011\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.5025 - accuracy: 0.5714 - val_loss: 2.5002 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/100\n",
      "Learning rate:  0.00012\n",
      "2/2 [==============================] - 2s 380ms/step - loss: 2.4391 - accuracy: 0.5857 - val_loss: 2.4128 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/100\n",
      "Learning rate:  0.00013000000000000002\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.3118 - accuracy: 0.6857 - val_loss: 2.3154 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/100\n",
      "Learning rate:  0.00014000000000000001\n",
      "2/2 [==============================] - 2s 375ms/step - loss: 2.2590 - accuracy: 0.8286 - val_loss: 2.1972 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/100\n",
      "Learning rate:  0.00015\n",
      "2/2 [==============================] - 2s 385ms/step - loss: 2.1672 - accuracy: 0.7571 - val_loss: 2.0740 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/100\n",
      "Learning rate:  0.00016\n",
      "2/2 [==============================] - 2s 376ms/step - loss: 2.0748 - accuracy: 0.8286 - val_loss: 1.9376 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/100\n",
      "Learning rate:  0.00017\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.9602 - accuracy: 0.7857 - val_loss: 1.8111 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/100\n",
      "Learning rate:  0.00017999999999999998\n",
      "2/2 [==============================] - 2s 425ms/step - loss: 1.9034 - accuracy: 0.8286 - val_loss: 1.6758 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/100\n",
      "Learning rate:  0.00019\n",
      "2/2 [==============================] - 2s 389ms/step - loss: 1.7962 - accuracy: 0.8143 - val_loss: 1.5269 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 21/100\n",
      "Learning rate:  0.0002\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.6775 - accuracy: 0.8429 - val_loss: 1.3929 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 22/100\n",
      "Learning rate:  0.00021\n",
      "2/2 [==============================] - 2s 497ms/step - loss: 1.6403 - accuracy: 0.8429 - val_loss: 1.2607 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 23/100\n",
      "Learning rate:  0.00022\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.5243 - accuracy: 0.8286 - val_loss: 1.1321 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 24/100\n",
      "Learning rate:  0.00023\n",
      "2/2 [==============================] - 2s 522ms/step - loss: 1.4736 - accuracy: 0.8286 - val_loss: 1.0185 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 25/100\n",
      "Learning rate:  0.00024\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.4105 - accuracy: 0.8286 - val_loss: 0.9360 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 26/100\n",
      "Learning rate:  0.00025\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.3720 - accuracy: 0.8286 - val_loss: 0.9028 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 27/100\n",
      "Learning rate:  0.00026000000000000003\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.2519 - accuracy: 0.8571 - val_loss: 0.8761 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 28/100\n",
      "Learning rate:  0.00027\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.2255 - accuracy: 0.8714 - val_loss: 0.8609 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 29/100\n",
      "Learning rate:  0.00028000000000000003\n",
      "2/2 [==============================] - 2s 399ms/step - loss: 1.2189 - accuracy: 0.8571 - val_loss: 0.8541 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 30/100\n",
      "Learning rate:  0.00029\n",
      "2/2 [==============================] - 2s 419ms/step - loss: 1.1455 - accuracy: 0.8429 - val_loss: 0.8502 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 31/100\n",
      "Learning rate:  0.0003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 462ms/step - loss: 1.0819 - accuracy: 0.8714 - val_loss: 0.8528 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 32/100\n",
      "Learning rate:  0.00031\n",
      "2/2 [==============================] - 2s 434ms/step - loss: 1.1055 - accuracy: 0.8714 - val_loss: 0.8689 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 33/100\n",
      "Learning rate:  0.00032\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0407 - accuracy: 0.8571 - val_loss: 0.8789 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 34/100\n",
      "Learning rate:  0.00033\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0170 - accuracy: 0.8857 - val_loss: 0.8682 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 35/100\n",
      "Learning rate:  0.00034\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9516 - accuracy: 0.9286 - val_loss: 0.8527 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 36/100\n",
      "Learning rate:  0.00035\n",
      "2/2 [==============================] - 2s 405ms/step - loss: 0.9394 - accuracy: 0.9429 - val_loss: 0.8349 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 37/100\n",
      "Learning rate:  0.00035999999999999997\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9574 - accuracy: 0.8857 - val_loss: 0.8024 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 38/100\n",
      "Learning rate:  0.00037\n",
      "2/2 [==============================] - 2s 403ms/step - loss: 0.9151 - accuracy: 0.9000 - val_loss: 0.7766 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 39/100\n",
      "Learning rate:  0.00038\n",
      "2/2 [==============================] - 2s 384ms/step - loss: 0.9084 - accuracy: 0.9143 - val_loss: 0.7495 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 40/100\n",
      "Learning rate:  0.00039000000000000005\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8647 - accuracy: 0.9286 - val_loss: 0.7302 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 41/100\n",
      "Learning rate:  0.0004\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8547 - accuracy: 0.9429 - val_loss: 0.7028 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 42/100\n",
      "Learning rate:  0.00041\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8990 - accuracy: 0.9143 - val_loss: 0.7018 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 43/100\n",
      "Learning rate:  0.00042\n",
      "2/2 [==============================] - 2s 388ms/step - loss: 0.8477 - accuracy: 0.9286 - val_loss: 0.7009 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 44/100\n",
      "Learning rate:  0.00043\n",
      "2/2 [==============================] - 2s 373ms/step - loss: 0.7965 - accuracy: 0.9571 - val_loss: 0.7035 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 45/100\n",
      "Learning rate:  0.00044\n",
      "2/2 [==============================] - 2s 378ms/step - loss: 0.8005 - accuracy: 0.9429 - val_loss: 0.7050 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 46/100\n",
      "Learning rate:  0.00045000000000000004\n",
      "2/2 [==============================] - 2s 398ms/step - loss: 0.7965 - accuracy: 0.9286 - val_loss: 0.7109 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 47/100\n",
      "Learning rate:  0.00046\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7938 - accuracy: 0.9429 - val_loss: 0.7129 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 48/100\n",
      "Learning rate:  0.00047\n",
      "2/2 [==============================] - 2s 381ms/step - loss: 0.7862 - accuracy: 0.9286 - val_loss: 0.7041 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 49/100\n",
      "Learning rate:  0.00048\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7815 - accuracy: 0.9286 - val_loss: 0.7134 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 50/100\n",
      "Learning rate:  0.00049\n",
      "2/2 [==============================] - 2s 396ms/step - loss: 0.7457 - accuracy: 0.9571 - val_loss: 0.7107 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 51/100\n",
      "Learning rate:  0.0005\n",
      "2/2 [==============================] - 2s 389ms/step - loss: 0.7544 - accuracy: 0.9714 - val_loss: 0.7054 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 52/100\n",
      "Learning rate:  0.00051\n",
      "2/2 [==============================] - 2s 410ms/step - loss: 0.8037 - accuracy: 0.9286 - val_loss: 0.6767 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 53/100\n",
      "Learning rate:  0.0005200000000000001\n",
      "2/2 [==============================] - 2s 421ms/step - loss: 0.7360 - accuracy: 0.9857 - val_loss: 0.6465 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 54/100\n",
      "Learning rate:  0.0005300000000000001\n",
      "2/2 [==============================] - 2s 393ms/step - loss: 0.8545 - accuracy: 0.9000 - val_loss: 0.6289 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 55/100\n",
      "Learning rate:  0.00054\n",
      "2/2 [==============================] - 2s 406ms/step - loss: 0.9331 - accuracy: 0.9000 - val_loss: 0.6145 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 56/100\n",
      "Learning rate:  0.00055\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9187 - accuracy: 0.9143 - val_loss: 0.6056 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 57/100\n",
      "Learning rate:  0.0005600000000000001\n",
      "2/2 [==============================] - 2s 373ms/step - loss: 0.8901 - accuracy: 0.9143 - val_loss: 0.6031 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 58/100\n",
      "Learning rate:  0.00057\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8501 - accuracy: 0.9000 - val_loss: 0.6021 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 59/100\n",
      "Learning rate:  0.00058\n",
      "2/2 [==============================] - 2s 381ms/step - loss: 0.8370 - accuracy: 0.9000 - val_loss: 0.6028 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 60/100\n",
      "Learning rate:  0.00059\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8635 - accuracy: 0.9143 - val_loss: 0.6043 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 61/100\n",
      "Learning rate:  0.0006\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8580 - accuracy: 0.9000 - val_loss: 0.6042 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 62/100\n",
      "Learning rate:  0.00061\n",
      "2/2 [==============================] - 2s 411ms/step - loss: 0.8494 - accuracy: 0.9429 - val_loss: 0.6028 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 63/100\n",
      "Learning rate:  0.00062\n",
      "2/2 [==============================] - 2s 386ms/step - loss: 0.8120 - accuracy: 0.9000 - val_loss: 0.6022 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 64/100\n",
      "Learning rate:  0.00063\n",
      "2/2 [==============================] - 2s 382ms/step - loss: 0.7382 - accuracy: 0.9714 - val_loss: 0.6019 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 65/100\n",
      "Learning rate:  0.00064\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7491 - accuracy: 0.9429 - val_loss: 0.6004 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 66/100\n",
      "Learning rate:  0.0006500000000000001\n",
      "2/2 [==============================] - 2s 393ms/step - loss: 0.7319 - accuracy: 0.9571 - val_loss: 0.5991 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 67/100\n",
      "Learning rate:  0.00066\n",
      "2/2 [==============================] - 2s 400ms/step - loss: 0.7823 - accuracy: 0.9571 - val_loss: 0.5977 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 68/100\n",
      "Learning rate:  0.00067\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7121 - accuracy: 0.9571 - val_loss: 0.5965 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 69/100\n",
      "Learning rate:  0.00068\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7260 - accuracy: 0.9571 - val_loss: 0.5956 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 70/100\n",
      "Learning rate:  0.00069\n",
      "2/2 [==============================] - 2s 390ms/step - loss: 0.8177 - accuracy: 0.9286 - val_loss: 0.5950 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 71/100\n",
      "Learning rate:  0.0007\n",
      "2/2 [==============================] - 2s 375ms/step - loss: 0.7092 - accuracy: 0.9714 - val_loss: 0.5947 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 72/100\n",
      "Learning rate:  0.00071\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7854 - accuracy: 0.9143 - val_loss: 0.5939 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 73/100\n",
      "Learning rate:  0.0007199999999999999\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7769 - accuracy: 0.9429 - val_loss: 0.5932 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 74/100\n",
      "Learning rate:  0.00073\n",
      "2/2 [==============================] - 2s 382ms/step - loss: 0.7668 - accuracy: 0.9286 - val_loss: 0.5926 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 75/100\n",
      "Learning rate:  0.00074\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7237 - accuracy: 0.9714 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 76/100\n",
      "Learning rate:  0.00075\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6962 - accuracy: 0.9571 - val_loss: 0.5914 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 77/100\n",
      "Learning rate:  0.00076\n",
      "2/2 [==============================] - 2s 484ms/step - loss: 0.7024 - accuracy: 0.9714 - val_loss: 0.5908 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 78/100\n",
      "Learning rate:  0.0007700000000000001\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.6908 - accuracy: 0.9857 - val_loss: 0.5901 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 79/100\n",
      "Learning rate:  0.0007800000000000001\n",
      "2/2 [==============================] - 3s 440ms/step - loss: 0.7851 - accuracy: 0.9429 - val_loss: 0.5895 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 80/100\n",
      "Learning rate:  0.00079\n",
      "2/2 [==============================] - 3s 487ms/step - loss: 0.6857 - accuracy: 1.0000 - val_loss: 0.5889 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 81/100\n",
      "Learning rate:  0.0008\n",
      "2/2 [==============================] - 2s 390ms/step - loss: 0.6953 - accuracy: 0.9714 - val_loss: 0.5882 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 82/100\n",
      "Learning rate:  8.1e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6818 - accuracy: 0.9714 - val_loss: 0.5882 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 83/100\n",
      "Learning rate:  8.2e-05\n",
      "2/2 [==============================] - 3s 446ms/step - loss: 0.6795 - accuracy: 0.9857 - val_loss: 0.5881 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 84/100\n",
      "Learning rate:  8.3e-05\n",
      "2/2 [==============================] - 3s 560ms/step - loss: 0.7423 - accuracy: 0.9857 - val_loss: 0.5880 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 85/100\n",
      "Learning rate:  8.4e-05\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.6776 - accuracy: 0.9714 - val_loss: 0.5880 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 86/100\n",
      "Learning rate:  8.5e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6989 - accuracy: 0.9857 - val_loss: 0.5879 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 87/100\n",
      "Learning rate:  8.6e-05\n",
      "2/2 [==============================] - 2s 441ms/step - loss: 0.6830 - accuracy: 0.9714 - val_loss: 0.5878 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 88/100\n",
      "Learning rate:  8.7e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6714 - accuracy: 1.0000 - val_loss: 0.5877 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 89/100\n",
      "Learning rate:  8.800000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6676 - accuracy: 1.0000 - val_loss: 0.5877 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 90/100\n",
      "Learning rate:  8.900000000000001e-05\n",
      "2/2 [==============================] - 2s 410ms/step - loss: 0.6771 - accuracy: 1.0000 - val_loss: 0.5876 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 91/100\n",
      "Learning rate:  9e-05\n",
      "2/2 [==============================] - 2s 482ms/step - loss: 0.6746 - accuracy: 0.9714 - val_loss: 0.5875 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 92/100\n",
      "Learning rate:  9.1e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6682 - accuracy: 0.9857 - val_loss: 0.5874 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 93/100\n",
      "Learning rate:  9.200000000000001e-05\n",
      "2/2 [==============================] - 2s 374ms/step - loss: 0.6695 - accuracy: 0.9857 - val_loss: 0.5874 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 94/100\n",
      "Learning rate:  9.300000000000001e-05\n",
      "2/2 [==============================] - 2s 385ms/step - loss: 0.6600 - accuracy: 0.9857 - val_loss: 0.5873 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 95/100\n",
      "Learning rate:  9.4e-05\n",
      "2/2 [==============================] - 2s 386ms/step - loss: 0.6565 - accuracy: 1.0000 - val_loss: 0.5872 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 96/100\n",
      "Learning rate:  9.5e-05\n",
      "2/2 [==============================] - 2s 437ms/step - loss: 0.6605 - accuracy: 1.0000 - val_loss: 0.5871 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 97/100\n",
      "Learning rate:  9.6e-05\n",
      "2/2 [==============================] - 2s 390ms/step - loss: 0.6463 - accuracy: 1.0000 - val_loss: 0.5871 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100\n",
      "Learning rate:  9.7e-05\n",
      "2/2 [==============================] - 2s 395ms/step - loss: 0.6487 - accuracy: 1.0000 - val_loss: 0.5870 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 99/100\n",
      "Learning rate:  9.8e-05\n",
      "2/2 [==============================] - 2s 430ms/step - loss: 0.6560 - accuracy: 1.0000 - val_loss: 0.5869 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 100/100\n",
      "Learning rate:  9.900000000000001e-05\n",
      "2/2 [==============================] - 2s 387ms/step - loss: 0.6587 - accuracy: 0.9857 - val_loss: 0.5868 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 1.4189 - accuracy: 0.7000\n",
      "Test loss: 1.4188671112060547\n",
      "Test accuracy: 0.699999988079071\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision:  0.7625\n",
      "Recall:  0.7\n",
      "F1 Score:  0.7123384253819036\n",
      "29\n",
      "29\n",
      "[[[[ 76  84  95 ...  76  84  95]\n",
      "   [ 15  15  20 ...  15  15  20]\n",
      "   [  8   7  11 ...   8   7  11]\n",
      "   ...\n",
      "   [ 16  14  25 ...  16  14  25]\n",
      "   [ 13  13  22 ...  13  13  22]\n",
      "   [  8   9  13 ...   8   9  13]]\n",
      "\n",
      "  [[ 88  96 107 ...  88  96 107]\n",
      "   [ 17  17  22 ...  17  17  22]\n",
      "   [  6   5   9 ...   6   5   9]\n",
      "   ...\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 15  13  24 ...  15  13  24]\n",
      "   [ 10  10  16 ...  10  10  16]]\n",
      "\n",
      "  [[ 86  91  98 ...  86  91  98]\n",
      "   [ 14  13  17 ...  14  13  17]\n",
      "   [  5   4   7 ...   5   4   7]\n",
      "   ...\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 12  11  18 ...  12  11  18]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[157 162 174 ... 157 162 174]\n",
      "   [159 165 176 ... 159 165 176]\n",
      "   [155 158 159 ... 155 158 159]\n",
      "   ...\n",
      "   [126 136 168 ... 126 136 168]\n",
      "   [145 153 178 ... 145 153 178]\n",
      "   [150 157 180 ... 150 157 180]]\n",
      "\n",
      "  [[158 162 174 ... 158 162 174]\n",
      "   [158 164 177 ... 158 164 177]\n",
      "   [160 167 179 ... 160 167 179]\n",
      "   ...\n",
      "   [134 144 172 ... 134 144 172]\n",
      "   [143 151 177 ... 143 151 177]\n",
      "   [151 158 180 ... 151 158 180]]\n",
      "\n",
      "  [[160 164 176 ... 160 164 176]\n",
      "   [158 164 177 ... 158 164 177]\n",
      "   [163 169 181 ... 163 169 181]\n",
      "   ...\n",
      "   [141 150 174 ... 141 150 174]\n",
      "   [140 149 176 ... 140 149 176]\n",
      "   [150 158 180 ... 150 158 180]]]\n",
      "\n",
      "\n",
      " [[[  9   9   2 ...   9   9   2]\n",
      "   [  6   6   2 ...   6   6   2]\n",
      "   [  4   3   1 ...   4   3   1]\n",
      "   ...\n",
      "   [  0   1   1 ...   0   1   1]\n",
      "   [  1   2   2 ...   1   2   2]\n",
      "   [  9  10   7 ...   9  10   7]]\n",
      "\n",
      "  [[  9   8   4 ...   9   8   4]\n",
      "   [  9   9   4 ...   9   9   4]\n",
      "   [  9   8   3 ...   9   8   3]\n",
      "   ...\n",
      "   [  1   1   1 ...   1   1   1]\n",
      "   [  4   4   3 ...   4   4   3]\n",
      "   [  4   5   3 ...   4   5   3]]\n",
      "\n",
      "  [[ 26  23  14 ...  26  23  14]\n",
      "   [ 33  27  15 ...  33  27  15]\n",
      "   [ 45  37  24 ...  45  37  24]\n",
      "   ...\n",
      "   [  2   3   2 ...   2   3   2]\n",
      "   [  2   2   2 ...   2   2   2]\n",
      "   [  4   4   3 ...   4   4   3]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[188 178 161 ... 188 178 161]\n",
      "   [190 180 162 ... 190 180 162]\n",
      "   [187 177 158 ... 187 177 158]\n",
      "   ...\n",
      "   [150 142 124 ... 150 142 124]\n",
      "   [145 135 118 ... 145 135 118]\n",
      "   [145 135 117 ... 145 135 117]]\n",
      "\n",
      "  [[197 186 170 ... 197 186 170]\n",
      "   [194 184 168 ... 194 184 168]\n",
      "   [192 181 164 ... 192 181 164]\n",
      "   ...\n",
      "   [148 139 120 ... 148 139 120]\n",
      "   [149 138 119 ... 149 138 119]\n",
      "   [147 136 117 ... 147 136 117]]\n",
      "\n",
      "  [[201 191 174 ... 201 191 174]\n",
      "   [198 188 172 ... 198 188 172]\n",
      "   [194 183 167 ... 194 183 167]\n",
      "   ...\n",
      "   [150 139 120 ... 150 139 120]\n",
      "   [152 141 121 ... 152 141 121]\n",
      "   [152 142 124 ... 152 142 124]]]\n",
      "\n",
      "\n",
      " [[[255 255 255 ... 255 255 255]\n",
      "   [243 246 242 ... 243 246 242]\n",
      "   [214 220 205 ... 214 220 205]\n",
      "   ...\n",
      "   [106 120  98 ... 106 120  98]\n",
      "   [210 215 205 ... 210 215 205]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [246 248 245 ... 246 248 245]\n",
      "   [217 226 213 ... 217 226 213]\n",
      "   ...\n",
      "   [108 123  99 ... 108 123  99]\n",
      "   [211 216 206 ... 211 216 206]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [248 251 248 ... 248 251 248]\n",
      "   [221 232 221 ... 221 232 221]\n",
      "   ...\n",
      "   [110 125 100 ... 110 125 100]\n",
      "   [212 217 206 ... 212 217 206]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [249 250 249 ... 249 250 249]\n",
      "   [232 229 221 ... 232 229 221]\n",
      "   ...\n",
      "   [ 86 101  95 ...  86 101  95]\n",
      "   [204 210 208 ... 204 210 208]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [250 250 249 ... 250 250 249]\n",
      "   [234 236 231 ... 234 236 231]\n",
      "   ...\n",
      "   [102 111 101 ... 102 111 101]\n",
      "   [202 208 207 ... 202 208 207]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [250 250 249 ... 250 250 249]\n",
      "   [237 240 235 ... 237 240 235]\n",
      "   ...\n",
      "   [128 125 110 ... 128 125 110]\n",
      "   [199 206 205 ... 199 206 205]\n",
      "   [255 255 255 ... 255 255 255]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (73, 40, 30, 9)\n",
      "73 train samples\n",
      "28 test samples\n",
      "y_train shape: (73, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_682 (Conv2D)             (None, 40, 30, 16)   1312        input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_616 (BatchN (None, 40, 30, 16)   64          conv2d_682[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_616 (Activation)     (None, 40, 30, 16)   0           batch_normalization_616[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_683 (Conv2D)             (None, 40, 30, 16)   272         activation_616[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_617 (BatchN (None, 40, 30, 16)   64          conv2d_683[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_617 (Activation)     (None, 40, 30, 16)   0           batch_normalization_617[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_684 (Conv2D)             (None, 40, 30, 16)   2320        activation_617[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_618 (BatchN (None, 40, 30, 16)   64          conv2d_684[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_618 (Activation)     (None, 40, 30, 16)   0           batch_normalization_618[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_686 (Conv2D)             (None, 40, 30, 64)   1088        activation_616[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_685 (Conv2D)             (None, 40, 30, 64)   1088        activation_618[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_198 (Add)                   (None, 40, 30, 64)   0           conv2d_686[0][0]                 \n",
      "                                                                 conv2d_685[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_619 (BatchN (None, 40, 30, 64)   256         add_198[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_619 (Activation)     (None, 40, 30, 64)   0           batch_normalization_619[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_687 (Conv2D)             (None, 40, 30, 16)   1040        activation_619[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_620 (BatchN (None, 40, 30, 16)   64          conv2d_687[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_620 (Activation)     (None, 40, 30, 16)   0           batch_normalization_620[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_688 (Conv2D)             (None, 40, 30, 16)   2320        activation_620[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_621 (BatchN (None, 40, 30, 16)   64          conv2d_688[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_621 (Activation)     (None, 40, 30, 16)   0           batch_normalization_621[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_689 (Conv2D)             (None, 40, 30, 64)   1088        activation_621[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_199 (Add)                   (None, 40, 30, 64)   0           add_198[0][0]                    \n",
      "                                                                 conv2d_689[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_622 (BatchN (None, 40, 30, 64)   256         add_199[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_622 (Activation)     (None, 40, 30, 64)   0           batch_normalization_622[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_690 (Conv2D)             (None, 40, 30, 16)   1040        activation_622[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_623 (BatchN (None, 40, 30, 16)   64          conv2d_690[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_623 (Activation)     (None, 40, 30, 16)   0           batch_normalization_623[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_691 (Conv2D)             (None, 40, 30, 16)   2320        activation_623[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_624 (BatchN (None, 40, 30, 16)   64          conv2d_691[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_624 (Activation)     (None, 40, 30, 16)   0           batch_normalization_624[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_692 (Conv2D)             (None, 40, 30, 64)   1088        activation_624[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_200 (Add)                   (None, 40, 30, 64)   0           add_199[0][0]                    \n",
      "                                                                 conv2d_692[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_625 (BatchN (None, 40, 30, 64)   256         add_200[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_625 (Activation)     (None, 40, 30, 64)   0           batch_normalization_625[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_693 (Conv2D)             (None, 20, 15, 64)   4160        activation_625[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_626 (BatchN (None, 20, 15, 64)   256         conv2d_693[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_626 (Activation)     (None, 20, 15, 64)   0           batch_normalization_626[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_694 (Conv2D)             (None, 20, 15, 64)   36928       activation_626[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_627 (BatchN (None, 20, 15, 64)   256         conv2d_694[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_627 (Activation)     (None, 20, 15, 64)   0           batch_normalization_627[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_696 (Conv2D)             (None, 20, 15, 128)  8320        add_200[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_695 (Conv2D)             (None, 20, 15, 128)  8320        activation_627[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_201 (Add)                   (None, 20, 15, 128)  0           conv2d_696[0][0]                 \n",
      "                                                                 conv2d_695[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_628 (BatchN (None, 20, 15, 128)  512         add_201[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_628 (Activation)     (None, 20, 15, 128)  0           batch_normalization_628[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_697 (Conv2D)             (None, 20, 15, 64)   8256        activation_628[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_629 (BatchN (None, 20, 15, 64)   256         conv2d_697[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_629 (Activation)     (None, 20, 15, 64)   0           batch_normalization_629[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_698 (Conv2D)             (None, 20, 15, 64)   36928       activation_629[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_630 (BatchN (None, 20, 15, 64)   256         conv2d_698[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_630 (Activation)     (None, 20, 15, 64)   0           batch_normalization_630[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_699 (Conv2D)             (None, 20, 15, 128)  8320        activation_630[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_202 (Add)                   (None, 20, 15, 128)  0           add_201[0][0]                    \n",
      "                                                                 conv2d_699[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_631 (BatchN (None, 20, 15, 128)  512         add_202[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_631 (Activation)     (None, 20, 15, 128)  0           batch_normalization_631[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_700 (Conv2D)             (None, 20, 15, 64)   8256        activation_631[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_632 (BatchN (None, 20, 15, 64)   256         conv2d_700[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_632 (Activation)     (None, 20, 15, 64)   0           batch_normalization_632[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_701 (Conv2D)             (None, 20, 15, 64)   36928       activation_632[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_633 (BatchN (None, 20, 15, 64)   256         conv2d_701[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_633 (Activation)     (None, 20, 15, 64)   0           batch_normalization_633[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_702 (Conv2D)             (None, 20, 15, 128)  8320        activation_633[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_203 (Add)                   (None, 20, 15, 128)  0           add_202[0][0]                    \n",
      "                                                                 conv2d_702[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_634 (BatchN (None, 20, 15, 128)  512         add_203[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_634 (Activation)     (None, 20, 15, 128)  0           batch_normalization_634[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_703 (Conv2D)             (None, 10, 8, 128)   16512       activation_634[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_635 (BatchN (None, 10, 8, 128)   512         conv2d_703[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_635 (Activation)     (None, 10, 8, 128)   0           batch_normalization_635[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_704 (Conv2D)             (None, 10, 8, 128)   147584      activation_635[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_636 (BatchN (None, 10, 8, 128)   512         conv2d_704[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_636 (Activation)     (None, 10, 8, 128)   0           batch_normalization_636[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_706 (Conv2D)             (None, 10, 8, 256)   33024       add_203[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_705 (Conv2D)             (None, 10, 8, 256)   33024       activation_636[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_204 (Add)                   (None, 10, 8, 256)   0           conv2d_706[0][0]                 \n",
      "                                                                 conv2d_705[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_637 (BatchN (None, 10, 8, 256)   1024        add_204[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_637 (Activation)     (None, 10, 8, 256)   0           batch_normalization_637[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_707 (Conv2D)             (None, 10, 8, 128)   32896       activation_637[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_638 (BatchN (None, 10, 8, 128)   512         conv2d_707[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_638 (Activation)     (None, 10, 8, 128)   0           batch_normalization_638[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_708 (Conv2D)             (None, 10, 8, 128)   147584      activation_638[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_639 (BatchN (None, 10, 8, 128)   512         conv2d_708[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_639 (Activation)     (None, 10, 8, 128)   0           batch_normalization_639[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_709 (Conv2D)             (None, 10, 8, 256)   33024       activation_639[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_205 (Add)                   (None, 10, 8, 256)   0           add_204[0][0]                    \n",
      "                                                                 conv2d_709[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_640 (BatchN (None, 10, 8, 256)   1024        add_205[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_640 (Activation)     (None, 10, 8, 256)   0           batch_normalization_640[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_710 (Conv2D)             (None, 10, 8, 128)   32896       activation_640[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_641 (BatchN (None, 10, 8, 128)   512         conv2d_710[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_641 (Activation)     (None, 10, 8, 128)   0           batch_normalization_641[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_711 (Conv2D)             (None, 10, 8, 128)   147584      activation_641[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_642 (BatchN (None, 10, 8, 128)   512         conv2d_711[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_642 (Activation)     (None, 10, 8, 128)   0           batch_normalization_642[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_712 (Conv2D)             (None, 10, 8, 256)   33024       activation_642[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_206 (Add)                   (None, 10, 8, 256)   0           add_205[0][0]                    \n",
      "                                                                 conv2d_712[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_643 (BatchN (None, 10, 8, 256)   1024        add_206[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_643 (Activation)     (None, 10, 8, 256)   0           batch_normalization_643[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_22 (AveragePo (None, 1, 1, 256)    0           activation_643[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 256)          0           average_pooling2d_22[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 10)           2570        flatten_22[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[0.29803923 0.32941177 0.37254903 ... 0.29803923 0.32941177\n",
      "    0.37254903]\n",
      "   [0.05882353 0.05882353 0.07843138 ... 0.05882353 0.05882353\n",
      "    0.07843138]\n",
      "   [0.03137255 0.02745098 0.04313726 ... 0.03137255 0.02745098\n",
      "    0.04313726]\n",
      "   ...\n",
      "   [0.0627451  0.05490196 0.09803922 ... 0.0627451  0.05490196\n",
      "    0.09803922]\n",
      "   [0.05098039 0.05098039 0.08627451 ... 0.05098039 0.05098039\n",
      "    0.08627451]\n",
      "   [0.03137255 0.03529412 0.05098039 ... 0.03137255 0.03529412\n",
      "    0.05098039]]\n",
      "\n",
      "  [[0.34509805 0.3764706  0.41960785 ... 0.34509805 0.3764706\n",
      "    0.41960785]\n",
      "   [0.06666667 0.06666667 0.08627451 ... 0.06666667 0.06666667\n",
      "    0.08627451]\n",
      "   [0.02352941 0.01960784 0.03529412 ... 0.02352941 0.01960784\n",
      "    0.03529412]\n",
      "   ...\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.05882353 0.05098039 0.09411765 ... 0.05882353 0.05098039\n",
      "    0.09411765]\n",
      "   [0.03921569 0.03921569 0.0627451  ... 0.03921569 0.03921569\n",
      "    0.0627451 ]]\n",
      "\n",
      "  [[0.3372549  0.35686275 0.38431373 ... 0.3372549  0.35686275\n",
      "    0.38431373]\n",
      "   [0.05490196 0.05098039 0.06666667 ... 0.05490196 0.05098039\n",
      "    0.06666667]\n",
      "   [0.01960784 0.01568628 0.02745098 ... 0.01960784 0.01568628\n",
      "    0.02745098]\n",
      "   ...\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.04705882 0.04313726 0.07058824 ... 0.04705882 0.04313726\n",
      "    0.07058824]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6156863  0.63529414 0.68235296 ... 0.6156863  0.63529414\n",
      "    0.68235296]\n",
      "   [0.62352943 0.64705884 0.6901961  ... 0.62352943 0.64705884\n",
      "    0.6901961 ]\n",
      "   [0.60784316 0.61960787 0.62352943 ... 0.60784316 0.61960787\n",
      "    0.62352943]\n",
      "   ...\n",
      "   [0.49411765 0.53333336 0.65882355 ... 0.49411765 0.53333336\n",
      "    0.65882355]\n",
      "   [0.5686275  0.6        0.69803923 ... 0.5686275  0.6\n",
      "    0.69803923]\n",
      "   [0.5882353  0.6156863  0.7058824  ... 0.5882353  0.6156863\n",
      "    0.7058824 ]]\n",
      "\n",
      "  [[0.61960787 0.63529414 0.68235296 ... 0.61960787 0.63529414\n",
      "    0.68235296]\n",
      "   [0.61960787 0.6431373  0.69411767 ... 0.61960787 0.6431373\n",
      "    0.69411767]\n",
      "   [0.627451   0.654902   0.7019608  ... 0.627451   0.654902\n",
      "    0.7019608 ]\n",
      "   ...\n",
      "   [0.5254902  0.5647059  0.6745098  ... 0.5254902  0.5647059\n",
      "    0.6745098 ]\n",
      "   [0.56078434 0.5921569  0.69411767 ... 0.56078434 0.5921569\n",
      "    0.69411767]\n",
      "   [0.5921569  0.61960787 0.7058824  ... 0.5921569  0.61960787\n",
      "    0.7058824 ]]\n",
      "\n",
      "  [[0.627451   0.6431373  0.6901961  ... 0.627451   0.6431373\n",
      "    0.6901961 ]\n",
      "   [0.61960787 0.6431373  0.69411767 ... 0.61960787 0.6431373\n",
      "    0.69411767]\n",
      "   [0.6392157  0.6627451  0.70980394 ... 0.6392157  0.6627451\n",
      "    0.70980394]\n",
      "   ...\n",
      "   [0.5529412  0.5882353  0.68235296 ... 0.5529412  0.5882353\n",
      "    0.68235296]\n",
      "   [0.54901963 0.58431375 0.6901961  ... 0.54901963 0.58431375\n",
      "    0.6901961 ]\n",
      "   [0.5882353  0.61960787 0.7058824  ... 0.5882353  0.61960787\n",
      "    0.7058824 ]]]\n",
      "\n",
      "\n",
      " [[[0.03529412 0.03529412 0.00784314 ... 0.03529412 0.03529412\n",
      "    0.00784314]\n",
      "   [0.02352941 0.02352941 0.00784314 ... 0.02352941 0.02352941\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01176471 0.00392157 ... 0.01568628 0.01176471\n",
      "    0.00392157]\n",
      "   ...\n",
      "   [0.         0.00392157 0.00392157 ... 0.         0.00392157\n",
      "    0.00392157]\n",
      "   [0.00392157 0.00784314 0.00784314 ... 0.00392157 0.00784314\n",
      "    0.00784314]\n",
      "   [0.03529412 0.03921569 0.02745098 ... 0.03529412 0.03921569\n",
      "    0.02745098]]\n",
      "\n",
      "  [[0.03529412 0.03137255 0.01568628 ... 0.03529412 0.03137255\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03529412 0.01568628 ... 0.03529412 0.03529412\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03137255 0.01176471 ... 0.03529412 0.03137255\n",
      "    0.01176471]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157 ... 0.00392157 0.00392157\n",
      "    0.00392157]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]\n",
      "   [0.01568628 0.01960784 0.01176471 ... 0.01568628 0.01960784\n",
      "    0.01176471]]\n",
      "\n",
      "  [[0.10196079 0.09019608 0.05490196 ... 0.10196079 0.09019608\n",
      "    0.05490196]\n",
      "   [0.12941177 0.10588235 0.05882353 ... 0.12941177 0.10588235\n",
      "    0.05882353]\n",
      "   [0.1764706  0.14509805 0.09411765 ... 0.1764706  0.14509805\n",
      "    0.09411765]\n",
      "   ...\n",
      "   [0.00784314 0.01176471 0.00784314 ... 0.00784314 0.01176471\n",
      "    0.00784314]\n",
      "   [0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.7372549  0.69803923 0.6313726  ... 0.7372549  0.69803923\n",
      "    0.6313726 ]\n",
      "   [0.74509805 0.7058824  0.63529414 ... 0.74509805 0.7058824\n",
      "    0.63529414]\n",
      "   [0.73333335 0.69411767 0.61960787 ... 0.73333335 0.69411767\n",
      "    0.61960787]\n",
      "   ...\n",
      "   [0.5882353  0.5568628  0.4862745  ... 0.5882353  0.5568628\n",
      "    0.4862745 ]\n",
      "   [0.5686275  0.5294118  0.4627451  ... 0.5686275  0.5294118\n",
      "    0.4627451 ]\n",
      "   [0.5686275  0.5294118  0.45882353 ... 0.5686275  0.5294118\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.77254903 0.7294118  0.6666667  ... 0.77254903 0.7294118\n",
      "    0.6666667 ]\n",
      "   [0.7607843  0.72156864 0.65882355 ... 0.7607843  0.72156864\n",
      "    0.65882355]\n",
      "   [0.7529412  0.70980394 0.6431373  ... 0.7529412  0.70980394\n",
      "    0.6431373 ]\n",
      "   ...\n",
      "   [0.5803922  0.54509807 0.47058824 ... 0.5803922  0.54509807\n",
      "    0.47058824]\n",
      "   [0.58431375 0.5411765  0.46666667 ... 0.58431375 0.5411765\n",
      "    0.46666667]\n",
      "   [0.5764706  0.53333336 0.45882353 ... 0.5764706  0.53333336\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.7882353  0.7490196  0.68235296 ... 0.7882353  0.7490196\n",
      "    0.68235296]\n",
      "   [0.7764706  0.7372549  0.6745098  ... 0.7764706  0.7372549\n",
      "    0.6745098 ]\n",
      "   [0.7607843  0.7176471  0.654902   ... 0.7607843  0.7176471\n",
      "    0.654902  ]\n",
      "   ...\n",
      "   [0.5882353  0.54509807 0.47058824 ... 0.5882353  0.54509807\n",
      "    0.47058824]\n",
      "   [0.59607846 0.5529412  0.4745098  ... 0.59607846 0.5529412\n",
      "    0.4745098 ]\n",
      "   [0.59607846 0.5568628  0.4862745  ... 0.59607846 0.5568628\n",
      "    0.4862745 ]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9529412  0.9647059  0.9490196  ... 0.9529412  0.9647059\n",
      "    0.9490196 ]\n",
      "   [0.8392157  0.8627451  0.8039216  ... 0.8392157  0.8627451\n",
      "    0.8039216 ]\n",
      "   ...\n",
      "   [0.41568628 0.47058824 0.38431373 ... 0.41568628 0.47058824\n",
      "    0.38431373]\n",
      "   [0.8235294  0.84313726 0.8039216  ... 0.8235294  0.84313726\n",
      "    0.8039216 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9647059  0.972549   0.9607843  ... 0.9647059  0.972549\n",
      "    0.9607843 ]\n",
      "   [0.8509804  0.8862745  0.8352941  ... 0.8509804  0.8862745\n",
      "    0.8352941 ]\n",
      "   ...\n",
      "   [0.42352942 0.48235294 0.3882353  ... 0.42352942 0.48235294\n",
      "    0.3882353 ]\n",
      "   [0.827451   0.84705883 0.80784315 ... 0.827451   0.84705883\n",
      "    0.80784315]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.972549   0.9843137  0.972549   ... 0.972549   0.9843137\n",
      "    0.972549  ]\n",
      "   [0.8666667  0.9098039  0.8666667  ... 0.8666667  0.9098039\n",
      "    0.8666667 ]\n",
      "   ...\n",
      "   [0.43137255 0.49019608 0.39215687 ... 0.43137255 0.49019608\n",
      "    0.39215687]\n",
      "   [0.83137256 0.8509804  0.80784315 ... 0.83137256 0.8509804\n",
      "    0.80784315]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9764706  0.98039216 0.9764706  ... 0.9764706  0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.9098039  0.8980392  0.8666667  ... 0.9098039  0.8980392\n",
      "    0.8666667 ]\n",
      "   ...\n",
      "   [0.3372549  0.39607844 0.37254903 ... 0.3372549  0.39607844\n",
      "    0.37254903]\n",
      "   [0.8        0.8235294  0.8156863  ... 0.8        0.8235294\n",
      "    0.8156863 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.98039216 0.98039216 0.9764706  ... 0.98039216 0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.91764706 0.9254902  0.90588236 ... 0.91764706 0.9254902\n",
      "    0.90588236]\n",
      "   ...\n",
      "   [0.4        0.43529412 0.39607844 ... 0.4        0.43529412\n",
      "    0.39607844]\n",
      "   [0.7921569  0.8156863  0.8117647  ... 0.7921569  0.8156863\n",
      "    0.8117647 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.98039216 0.98039216 0.9764706  ... 0.98039216 0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.92941177 0.9411765  0.92156863 ... 0.92941177 0.9411765\n",
      "    0.92156863]\n",
      "   ...\n",
      "   [0.5019608  0.49019608 0.43137255 ... 0.5019608  0.49019608\n",
      "    0.43137255]\n",
      "   [0.78039217 0.80784315 0.8039216  ... 0.78039217 0.80784315\n",
      "    0.8039216 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   ...\n",
      "   [0.8980392  0.9098039  0.9372549  ... 0.8980392  0.9098039\n",
      "    0.9372549 ]\n",
      "   [0.8901961  0.9019608  0.92941177 ... 0.8901961  0.9019608\n",
      "    0.92941177]\n",
      "   [0.8862745  0.8980392  0.9254902  ... 0.8862745  0.8980392\n",
      "    0.9254902 ]]\n",
      "\n",
      "  [[0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   ...\n",
      "   [0.8901961  0.9019608  0.92941177 ... 0.8901961  0.9019608\n",
      "    0.92941177]\n",
      "   [0.8862745  0.89411765 0.92156863 ... 0.8862745  0.89411765\n",
      "    0.92156863]\n",
      "   [0.8862745  0.8901961  0.92156863 ... 0.8862745  0.8901961\n",
      "    0.92156863]]\n",
      "\n",
      "  [[0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.9647059  ... 0.96862745 0.96862745\n",
      "    0.9647059 ]\n",
      "   [0.972549   0.972549   0.9647059  ... 0.972549   0.972549\n",
      "    0.9647059 ]\n",
      "   ...\n",
      "   [0.8862745  0.8901961  0.92156863 ... 0.8862745  0.8901961\n",
      "    0.92156863]\n",
      "   [0.8862745  0.8862745  0.91764706 ... 0.8862745  0.8862745\n",
      "    0.91764706]\n",
      "   [0.88235295 0.88235295 0.9137255  ... 0.88235295 0.88235295\n",
      "    0.9137255 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.827451   0.8117647  0.8352941  ... 0.827451   0.8117647\n",
      "    0.8352941 ]\n",
      "   [0.8352941  0.8156863  0.8352941  ... 0.8352941  0.8156863\n",
      "    0.8352941 ]\n",
      "   [0.8666667  0.8509804  0.8666667  ... 0.8666667  0.8509804\n",
      "    0.8666667 ]\n",
      "   ...\n",
      "   [0.7490196  0.74509805 0.76862746 ... 0.7490196  0.74509805\n",
      "    0.76862746]\n",
      "   [0.7647059  0.75686276 0.7921569  ... 0.7647059  0.75686276\n",
      "    0.7921569 ]\n",
      "   [0.78431374 0.77254903 0.81960785 ... 0.78431374 0.77254903\n",
      "    0.81960785]]\n",
      "\n",
      "  [[0.84705883 0.827451   0.8509804  ... 0.84705883 0.827451\n",
      "    0.8509804 ]\n",
      "   [0.8509804  0.83137256 0.84705883 ... 0.8509804  0.83137256\n",
      "    0.84705883]\n",
      "   [0.88235295 0.8627451  0.8784314  ... 0.88235295 0.8627451\n",
      "    0.8784314 ]\n",
      "   ...\n",
      "   [0.7372549  0.73333335 0.75686276 ... 0.7372549  0.73333335\n",
      "    0.75686276]\n",
      "   [0.75686276 0.7490196  0.7764706  ... 0.75686276 0.7490196\n",
      "    0.7764706 ]\n",
      "   [0.77254903 0.7647059  0.8039216  ... 0.77254903 0.7647059\n",
      "    0.8039216 ]]\n",
      "\n",
      "  [[0.8509804  0.8352941  0.85490197 ... 0.8509804  0.8352941\n",
      "    0.85490197]\n",
      "   [0.8666667  0.84705883 0.8627451  ... 0.8666667  0.84705883\n",
      "    0.8627451 ]\n",
      "   [0.8901961  0.87058824 0.8862745  ... 0.8901961  0.87058824\n",
      "    0.8862745 ]\n",
      "   ...\n",
      "   [0.7254902  0.7254902  0.7490196  ... 0.7254902  0.7254902\n",
      "    0.7490196 ]\n",
      "   [0.7411765  0.7372549  0.7647059  ... 0.7411765  0.7372549\n",
      "    0.7647059 ]\n",
      "   [0.7607843  0.7529412  0.78431374 ... 0.7607843  0.7529412\n",
      "    0.78431374]]]\n",
      "\n",
      "\n",
      " [[[0.65882355 0.5411765  0.34901962 ... 0.65882355 0.5411765\n",
      "    0.34901962]\n",
      "   [0.79607844 0.6862745  0.44313726 ... 0.79607844 0.6862745\n",
      "    0.44313726]\n",
      "   [0.84705883 0.7372549  0.4862745  ... 0.84705883 0.7372549\n",
      "    0.4862745 ]\n",
      "   ...\n",
      "   [0.9019608  0.78431374 0.57254905 ... 0.9019608  0.78431374\n",
      "    0.57254905]\n",
      "   [0.8784314  0.77254903 0.5803922  ... 0.8784314  0.77254903\n",
      "    0.5803922 ]\n",
      "   [0.8666667  0.7607843  0.58431375 ... 0.8666667  0.7607843\n",
      "    0.58431375]]\n",
      "\n",
      "  [[0.6313726  0.5058824  0.27450982 ... 0.6313726  0.5058824\n",
      "    0.27450982]\n",
      "   [0.7372549  0.61960787 0.3764706  ... 0.7372549  0.61960787\n",
      "    0.3764706 ]\n",
      "   [0.83137256 0.72156864 0.4627451  ... 0.83137256 0.72156864\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.8784314  0.7607843  0.5568628  ... 0.8784314  0.7607843\n",
      "    0.5568628 ]\n",
      "   [0.85882354 0.7490196  0.56078434 ... 0.85882354 0.7490196\n",
      "    0.56078434]\n",
      "   [0.85490197 0.7529412  0.5647059  ... 0.85490197 0.7529412\n",
      "    0.5647059 ]]\n",
      "\n",
      "  [[0.6        0.46666667 0.22352941 ... 0.6        0.46666667\n",
      "    0.22352941]\n",
      "   [0.6313726  0.49803922 0.27058825 ... 0.6313726  0.49803922\n",
      "    0.27058825]\n",
      "   [0.80784315 0.69803923 0.44705883 ... 0.80784315 0.69803923\n",
      "    0.44705883]\n",
      "   ...\n",
      "   [0.8352941  0.72156864 0.5372549  ... 0.8352941  0.72156864\n",
      "    0.5372549 ]\n",
      "   [0.83137256 0.7254902  0.5411765  ... 0.83137256 0.7254902\n",
      "    0.5411765 ]\n",
      "   [0.8352941  0.73333335 0.5372549  ... 0.8352941  0.73333335\n",
      "    0.5372549 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6039216  0.3764706  0.27058825 ... 0.6039216  0.3764706\n",
      "    0.27058825]\n",
      "   [0.627451   0.40784314 0.31764707 ... 0.627451   0.40784314\n",
      "    0.31764707]\n",
      "   [0.67058825 0.47058824 0.4        ... 0.67058825 0.47058824\n",
      "    0.4       ]\n",
      "   ...\n",
      "   [0.76862746 0.58431375 0.4745098  ... 0.76862746 0.58431375\n",
      "    0.4745098 ]\n",
      "   [0.7058824  0.49803922 0.38431373 ... 0.7058824  0.49803922\n",
      "    0.38431373]\n",
      "   [0.65882355 0.44313726 0.32156864 ... 0.65882355 0.44313726\n",
      "    0.32156864]]\n",
      "\n",
      "  [[0.6        0.38039216 0.26666668 ... 0.6        0.38039216\n",
      "    0.26666668]\n",
      "   [0.63529414 0.41568628 0.32156864 ... 0.63529414 0.41568628\n",
      "    0.32156864]\n",
      "   [0.7372549  0.56078434 0.4627451  ... 0.7372549  0.56078434\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.83137256 0.654902   0.54901963 ... 0.83137256 0.654902\n",
      "    0.54901963]\n",
      "   [0.77254903 0.5647059  0.44313726 ... 0.77254903 0.5647059\n",
      "    0.44313726]\n",
      "   [0.7176471  0.5019608  0.38039216 ... 0.7176471  0.5019608\n",
      "    0.38039216]]\n",
      "\n",
      "  [[0.627451   0.41960785 0.29411766 ... 0.627451   0.41960785\n",
      "    0.29411766]\n",
      "   [0.68235296 0.46666667 0.3882353  ... 0.68235296 0.46666667\n",
      "    0.3882353 ]\n",
      "   [0.8509804  0.7294118  0.49019608 ... 0.8509804  0.7294118\n",
      "    0.49019608]\n",
      "   ...\n",
      "   [0.89411765 0.72156864 0.6117647  ... 0.89411765 0.72156864\n",
      "    0.6117647 ]\n",
      "   [0.8392157  0.627451   0.50980395 ... 0.8392157  0.627451\n",
      "    0.50980395]\n",
      "   [0.76862746 0.56078434 0.43529412 ... 0.76862746 0.56078434\n",
      "    0.43529412]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.99215686 0.99215686 0.99215686 ... 0.99215686 0.99215686\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6509804  0.6117647  0.60784316 ... 0.6509804  0.6117647\n",
      "    0.60784316]\n",
      "   [0.6431373  0.6        0.5921569  ... 0.6431373  0.6\n",
      "    0.5921569 ]\n",
      "   [0.63529414 0.5921569  0.5803922  ... 0.63529414 0.5921569\n",
      "    0.5803922 ]\n",
      "   ...\n",
      "   [0.80784315 0.69803923 0.67058825 ... 0.80784315 0.69803923\n",
      "    0.67058825]\n",
      "   [0.80784315 0.7176471  0.6901961  ... 0.80784315 0.7176471\n",
      "    0.6901961 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]]\n",
      "\n",
      "  [[0.64705884 0.59607846 0.59607846 ... 0.64705884 0.59607846\n",
      "    0.59607846]\n",
      "   [0.63529414 0.5921569  0.58431375 ... 0.63529414 0.5921569\n",
      "    0.58431375]\n",
      "   [0.62352943 0.5803922  0.57254905 ... 0.62352943 0.5803922\n",
      "    0.57254905]\n",
      "   ...\n",
      "   [0.8        0.69803923 0.6745098  ... 0.8        0.69803923\n",
      "    0.6745098 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]\n",
      "   [0.7921569  0.7019608  0.68235296 ... 0.7921569  0.7019608\n",
      "    0.68235296]]\n",
      "\n",
      "  [[0.6431373  0.59607846 0.5921569  ... 0.6431373  0.59607846\n",
      "    0.5921569 ]\n",
      "   [0.6313726  0.5882353  0.5803922  ... 0.6313726  0.5882353\n",
      "    0.5803922 ]\n",
      "   [0.627451   0.57254905 0.5686275  ... 0.627451   0.57254905\n",
      "    0.5686275 ]\n",
      "   ...\n",
      "   [0.7921569  0.6901961  0.6784314  ... 0.7921569  0.6901961\n",
      "    0.6784314 ]\n",
      "   [0.80784315 0.7058824  0.6901961  ... 0.80784315 0.7058824\n",
      "    0.6901961 ]\n",
      "   [0.80784315 0.7176471  0.7019608  ... 0.80784315 0.7176471\n",
      "    0.7019608 ]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (73, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (73, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 3.1219 - accuracy: 0.0000e+0030\n",
      "30\n",
      "[[[[243 243 243 ... 243 243 243]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  [[242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  [[242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[243 242 244 ... 243 242 244]\n",
      "   [245 242 248 ... 245 242 248]\n",
      "   [250 250 253 ... 250 250 253]\n",
      "   ...\n",
      "   [214 218 239 ... 214 218 239]\n",
      "   [205 209 233 ... 205 209 233]\n",
      "   [213 215 234 ... 213 215 234]]\n",
      "\n",
      "  [[238 236 241 ... 238 236 241]\n",
      "   [235 230 238 ... 235 230 238]\n",
      "   [240 236 243 ... 240 236 243]\n",
      "   ...\n",
      "   [210 214 238 ... 210 214 238]\n",
      "   [202 206 232 ... 202 206 232]\n",
      "   [206 208 231 ... 206 208 231]]\n",
      "\n",
      "  [[226 219 227 ... 226 219 227]\n",
      "   [221 211 220 ... 221 211 220]\n",
      "   [219 210 222 ... 219 210 222]\n",
      "   ...\n",
      "   [207 212 237 ... 207 212 237]\n",
      "   [207 212 236 ... 207 212 236]\n",
      "   [207 210 233 ... 207 210 233]]]\n",
      "\n",
      "\n",
      " [[[255 255 255 ... 255 255 255]\n",
      "   [243 246 242 ... 243 246 242]\n",
      "   [214 220 205 ... 214 220 205]\n",
      "   ...\n",
      "   [106 120  98 ... 106 120  98]\n",
      "   [210 215 205 ... 210 215 205]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [246 248 245 ... 246 248 245]\n",
      "   [217 226 213 ... 217 226 213]\n",
      "   ...\n",
      "   [108 123  99 ... 108 123  99]\n",
      "   [211 216 206 ... 211 216 206]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [248 251 248 ... 248 251 248]\n",
      "   [221 232 221 ... 221 232 221]\n",
      "   ...\n",
      "   [110 125 100 ... 110 125 100]\n",
      "   [212 217 206 ... 212 217 206]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [249 250 249 ... 249 250 249]\n",
      "   [232 229 221 ... 232 229 221]\n",
      "   ...\n",
      "   [ 86 101  95 ...  86 101  95]\n",
      "   [204 210 208 ... 204 210 208]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [250 250 249 ... 250 250 249]\n",
      "   [234 236 231 ... 234 236 231]\n",
      "   ...\n",
      "   [102 111 101 ... 102 111 101]\n",
      "   [202 208 207 ... 202 208 207]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [250 250 249 ... 250 250 249]\n",
      "   [237 240 235 ... 237 240 235]\n",
      "   ...\n",
      "   [128 125 110 ... 128 125 110]\n",
      "   [199 206 205 ... 199 206 205]\n",
      "   [255 255 255 ... 255 255 255]]]\n",
      "\n",
      "\n",
      " [[[198 203 206 ... 198 203 206]\n",
      "   [192 196 197 ... 192 196 197]\n",
      "   [154 154 149 ... 154 154 149]\n",
      "   ...\n",
      "   [201 208 206 ... 201 208 206]\n",
      "   [199 207 205 ... 199 207 205]\n",
      "   [192 202 201 ... 192 202 201]]\n",
      "\n",
      "  [[200 204 207 ... 200 204 207]\n",
      "   [184 187 188 ... 184 187 188]\n",
      "   [109 107 101 ... 109 107 101]\n",
      "   ...\n",
      "   [199 206 206 ... 199 206 206]\n",
      "   [201 211 209 ... 201 211 209]\n",
      "   [196 207 205 ... 196 207 205]]\n",
      "\n",
      "  [[207 211 215 ... 207 211 215]\n",
      "   [168 171 170 ... 168 171 170]\n",
      "   [ 62  58  50 ...  62  58  50]\n",
      "   ...\n",
      "   [193 200 200 ... 193 200 200]\n",
      "   [198 207 206 ... 198 207 206]\n",
      "   [198 209 207 ... 198 209 207]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[217 216 217 ... 217 216 217]\n",
      "   [217 214 212 ... 217 214 212]\n",
      "   [223 219 217 ... 223 219 217]\n",
      "   ...\n",
      "   [165 114 111 ... 165 114 111]\n",
      "   [174 125 121 ... 174 125 121]\n",
      "   [175 127 123 ... 175 127 123]]\n",
      "\n",
      "  [[218 217 217 ... 218 217 217]\n",
      "   [218 216 217 ... 218 216 217]\n",
      "   [222 220 218 ... 222 220 218]\n",
      "   ...\n",
      "   [163 113 109 ... 163 113 109]\n",
      "   [171 123 119 ... 171 123 119]\n",
      "   [171 124 119 ... 171 124 119]]\n",
      "\n",
      "  [[216 216 216 ... 216 216 216]\n",
      "   [219 218 217 ... 219 218 217]\n",
      "   [222 221 220 ... 222 221 220]\n",
      "   ...\n",
      "   [162 113 109 ... 162 113 109]\n",
      "   [167 119 115 ... 167 119 115]\n",
      "   [166 120 115 ... 166 120 115]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (70, 40, 30, 9)\n",
      "70 train samples\n",
      "30 test samples\n",
      "y_train shape: (70, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_24 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_713 (Conv2D)             (None, 40, 30, 16)   1312        input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_644 (BatchN (None, 40, 30, 16)   64          conv2d_713[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_644 (Activation)     (None, 40, 30, 16)   0           batch_normalization_644[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_714 (Conv2D)             (None, 40, 30, 16)   272         activation_644[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_645 (BatchN (None, 40, 30, 16)   64          conv2d_714[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_645 (Activation)     (None, 40, 30, 16)   0           batch_normalization_645[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_715 (Conv2D)             (None, 40, 30, 16)   2320        activation_645[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_646 (BatchN (None, 40, 30, 16)   64          conv2d_715[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_646 (Activation)     (None, 40, 30, 16)   0           batch_normalization_646[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_717 (Conv2D)             (None, 40, 30, 64)   1088        activation_644[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_716 (Conv2D)             (None, 40, 30, 64)   1088        activation_646[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_207 (Add)                   (None, 40, 30, 64)   0           conv2d_717[0][0]                 \n",
      "                                                                 conv2d_716[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_647 (BatchN (None, 40, 30, 64)   256         add_207[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_647 (Activation)     (None, 40, 30, 64)   0           batch_normalization_647[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_718 (Conv2D)             (None, 40, 30, 16)   1040        activation_647[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_648 (BatchN (None, 40, 30, 16)   64          conv2d_718[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_648 (Activation)     (None, 40, 30, 16)   0           batch_normalization_648[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_719 (Conv2D)             (None, 40, 30, 16)   2320        activation_648[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_649 (BatchN (None, 40, 30, 16)   64          conv2d_719[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_649 (Activation)     (None, 40, 30, 16)   0           batch_normalization_649[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_720 (Conv2D)             (None, 40, 30, 64)   1088        activation_649[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_208 (Add)                   (None, 40, 30, 64)   0           add_207[0][0]                    \n",
      "                                                                 conv2d_720[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_650 (BatchN (None, 40, 30, 64)   256         add_208[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_650 (Activation)     (None, 40, 30, 64)   0           batch_normalization_650[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_721 (Conv2D)             (None, 40, 30, 16)   1040        activation_650[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_651 (BatchN (None, 40, 30, 16)   64          conv2d_721[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_651 (Activation)     (None, 40, 30, 16)   0           batch_normalization_651[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_722 (Conv2D)             (None, 40, 30, 16)   2320        activation_651[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_652 (BatchN (None, 40, 30, 16)   64          conv2d_722[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_652 (Activation)     (None, 40, 30, 16)   0           batch_normalization_652[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_723 (Conv2D)             (None, 40, 30, 64)   1088        activation_652[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_209 (Add)                   (None, 40, 30, 64)   0           add_208[0][0]                    \n",
      "                                                                 conv2d_723[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_653 (BatchN (None, 40, 30, 64)   256         add_209[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_653 (Activation)     (None, 40, 30, 64)   0           batch_normalization_653[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_724 (Conv2D)             (None, 20, 15, 64)   4160        activation_653[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_654 (BatchN (None, 20, 15, 64)   256         conv2d_724[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_654 (Activation)     (None, 20, 15, 64)   0           batch_normalization_654[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_725 (Conv2D)             (None, 20, 15, 64)   36928       activation_654[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_655 (BatchN (None, 20, 15, 64)   256         conv2d_725[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_655 (Activation)     (None, 20, 15, 64)   0           batch_normalization_655[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_727 (Conv2D)             (None, 20, 15, 128)  8320        add_209[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_726 (Conv2D)             (None, 20, 15, 128)  8320        activation_655[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_210 (Add)                   (None, 20, 15, 128)  0           conv2d_727[0][0]                 \n",
      "                                                                 conv2d_726[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_656 (BatchN (None, 20, 15, 128)  512         add_210[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_656 (Activation)     (None, 20, 15, 128)  0           batch_normalization_656[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_728 (Conv2D)             (None, 20, 15, 64)   8256        activation_656[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_657 (BatchN (None, 20, 15, 64)   256         conv2d_728[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_657 (Activation)     (None, 20, 15, 64)   0           batch_normalization_657[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_729 (Conv2D)             (None, 20, 15, 64)   36928       activation_657[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_658 (BatchN (None, 20, 15, 64)   256         conv2d_729[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_658 (Activation)     (None, 20, 15, 64)   0           batch_normalization_658[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_730 (Conv2D)             (None, 20, 15, 128)  8320        activation_658[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_211 (Add)                   (None, 20, 15, 128)  0           add_210[0][0]                    \n",
      "                                                                 conv2d_730[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_659 (BatchN (None, 20, 15, 128)  512         add_211[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_659 (Activation)     (None, 20, 15, 128)  0           batch_normalization_659[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_731 (Conv2D)             (None, 20, 15, 64)   8256        activation_659[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_660 (BatchN (None, 20, 15, 64)   256         conv2d_731[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_660 (Activation)     (None, 20, 15, 64)   0           batch_normalization_660[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_732 (Conv2D)             (None, 20, 15, 64)   36928       activation_660[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_661 (BatchN (None, 20, 15, 64)   256         conv2d_732[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_661 (Activation)     (None, 20, 15, 64)   0           batch_normalization_661[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_733 (Conv2D)             (None, 20, 15, 128)  8320        activation_661[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_212 (Add)                   (None, 20, 15, 128)  0           add_211[0][0]                    \n",
      "                                                                 conv2d_733[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_662 (BatchN (None, 20, 15, 128)  512         add_212[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_662 (Activation)     (None, 20, 15, 128)  0           batch_normalization_662[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_734 (Conv2D)             (None, 10, 8, 128)   16512       activation_662[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_663 (BatchN (None, 10, 8, 128)   512         conv2d_734[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_663 (Activation)     (None, 10, 8, 128)   0           batch_normalization_663[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_735 (Conv2D)             (None, 10, 8, 128)   147584      activation_663[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_664 (BatchN (None, 10, 8, 128)   512         conv2d_735[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_664 (Activation)     (None, 10, 8, 128)   0           batch_normalization_664[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_737 (Conv2D)             (None, 10, 8, 256)   33024       add_212[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_736 (Conv2D)             (None, 10, 8, 256)   33024       activation_664[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_213 (Add)                   (None, 10, 8, 256)   0           conv2d_737[0][0]                 \n",
      "                                                                 conv2d_736[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_665 (BatchN (None, 10, 8, 256)   1024        add_213[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_665 (Activation)     (None, 10, 8, 256)   0           batch_normalization_665[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_738 (Conv2D)             (None, 10, 8, 128)   32896       activation_665[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_666 (BatchN (None, 10, 8, 128)   512         conv2d_738[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_666 (Activation)     (None, 10, 8, 128)   0           batch_normalization_666[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_739 (Conv2D)             (None, 10, 8, 128)   147584      activation_666[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_667 (BatchN (None, 10, 8, 128)   512         conv2d_739[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_667 (Activation)     (None, 10, 8, 128)   0           batch_normalization_667[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_740 (Conv2D)             (None, 10, 8, 256)   33024       activation_667[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_214 (Add)                   (None, 10, 8, 256)   0           add_213[0][0]                    \n",
      "                                                                 conv2d_740[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_668 (BatchN (None, 10, 8, 256)   1024        add_214[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_668 (Activation)     (None, 10, 8, 256)   0           batch_normalization_668[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_741 (Conv2D)             (None, 10, 8, 128)   32896       activation_668[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_669 (BatchN (None, 10, 8, 128)   512         conv2d_741[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_669 (Activation)     (None, 10, 8, 128)   0           batch_normalization_669[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_742 (Conv2D)             (None, 10, 8, 128)   147584      activation_669[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_670 (BatchN (None, 10, 8, 128)   512         conv2d_742[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_670 (Activation)     (None, 10, 8, 128)   0           batch_normalization_670[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_743 (Conv2D)             (None, 10, 8, 256)   33024       activation_670[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_215 (Add)                   (None, 10, 8, 256)   0           add_214[0][0]                    \n",
      "                                                                 conv2d_743[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_671 (BatchN (None, 10, 8, 256)   1024        add_215[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_671 (Activation)     (None, 10, 8, 256)   0           batch_normalization_671[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_23 (AveragePo (None, 1, 1, 256)    0           activation_671[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 256)          0           average_pooling2d_23[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 10)           2570        flatten_23[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[0.9529412  0.9529412  0.9529412  ... 0.9529412  0.9529412\n",
      "    0.9529412 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  [[0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  [[0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.9529412  0.9490196  0.95686275 ... 0.9529412  0.9490196\n",
      "    0.95686275]\n",
      "   [0.9607843  0.9490196  0.972549   ... 0.9607843  0.9490196\n",
      "    0.972549  ]\n",
      "   [0.98039216 0.98039216 0.99215686 ... 0.98039216 0.98039216\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [0.8392157  0.85490197 0.9372549  ... 0.8392157  0.85490197\n",
      "    0.9372549 ]\n",
      "   [0.8039216  0.81960785 0.9137255  ... 0.8039216  0.81960785\n",
      "    0.9137255 ]\n",
      "   [0.8352941  0.84313726 0.91764706 ... 0.8352941  0.84313726\n",
      "    0.91764706]]\n",
      "\n",
      "  [[0.93333334 0.9254902  0.94509804 ... 0.93333334 0.9254902\n",
      "    0.94509804]\n",
      "   [0.92156863 0.9019608  0.93333334 ... 0.92156863 0.9019608\n",
      "    0.93333334]\n",
      "   [0.9411765  0.9254902  0.9529412  ... 0.9411765  0.9254902\n",
      "    0.9529412 ]\n",
      "   ...\n",
      "   [0.8235294  0.8392157  0.93333334 ... 0.8235294  0.8392157\n",
      "    0.93333334]\n",
      "   [0.7921569  0.80784315 0.9098039  ... 0.7921569  0.80784315\n",
      "    0.9098039 ]\n",
      "   [0.80784315 0.8156863  0.90588236 ... 0.80784315 0.8156863\n",
      "    0.90588236]]\n",
      "\n",
      "  [[0.8862745  0.85882354 0.8901961  ... 0.8862745  0.85882354\n",
      "    0.8901961 ]\n",
      "   [0.8666667  0.827451   0.8627451  ... 0.8666667  0.827451\n",
      "    0.8627451 ]\n",
      "   [0.85882354 0.8235294  0.87058824 ... 0.85882354 0.8235294\n",
      "    0.87058824]\n",
      "   ...\n",
      "   [0.8117647  0.83137256 0.92941177 ... 0.8117647  0.83137256\n",
      "    0.92941177]\n",
      "   [0.8117647  0.83137256 0.9254902  ... 0.8117647  0.83137256\n",
      "    0.9254902 ]\n",
      "   [0.8117647  0.8235294  0.9137255  ... 0.8117647  0.8235294\n",
      "    0.9137255 ]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9529412  0.9647059  0.9490196  ... 0.9529412  0.9647059\n",
      "    0.9490196 ]\n",
      "   [0.8392157  0.8627451  0.8039216  ... 0.8392157  0.8627451\n",
      "    0.8039216 ]\n",
      "   ...\n",
      "   [0.41568628 0.47058824 0.38431373 ... 0.41568628 0.47058824\n",
      "    0.38431373]\n",
      "   [0.8235294  0.84313726 0.8039216  ... 0.8235294  0.84313726\n",
      "    0.8039216 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9647059  0.972549   0.9607843  ... 0.9647059  0.972549\n",
      "    0.9607843 ]\n",
      "   [0.8509804  0.8862745  0.8352941  ... 0.8509804  0.8862745\n",
      "    0.8352941 ]\n",
      "   ...\n",
      "   [0.42352942 0.48235294 0.3882353  ... 0.42352942 0.48235294\n",
      "    0.3882353 ]\n",
      "   [0.827451   0.84705883 0.80784315 ... 0.827451   0.84705883\n",
      "    0.80784315]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.972549   0.9843137  0.972549   ... 0.972549   0.9843137\n",
      "    0.972549  ]\n",
      "   [0.8666667  0.9098039  0.8666667  ... 0.8666667  0.9098039\n",
      "    0.8666667 ]\n",
      "   ...\n",
      "   [0.43137255 0.49019608 0.39215687 ... 0.43137255 0.49019608\n",
      "    0.39215687]\n",
      "   [0.83137256 0.8509804  0.80784315 ... 0.83137256 0.8509804\n",
      "    0.80784315]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9764706  0.98039216 0.9764706  ... 0.9764706  0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.9098039  0.8980392  0.8666667  ... 0.9098039  0.8980392\n",
      "    0.8666667 ]\n",
      "   ...\n",
      "   [0.3372549  0.39607844 0.37254903 ... 0.3372549  0.39607844\n",
      "    0.37254903]\n",
      "   [0.8        0.8235294  0.8156863  ... 0.8        0.8235294\n",
      "    0.8156863 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.98039216 0.98039216 0.9764706  ... 0.98039216 0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.91764706 0.9254902  0.90588236 ... 0.91764706 0.9254902\n",
      "    0.90588236]\n",
      "   ...\n",
      "   [0.4        0.43529412 0.39607844 ... 0.4        0.43529412\n",
      "    0.39607844]\n",
      "   [0.7921569  0.8156863  0.8117647  ... 0.7921569  0.8156863\n",
      "    0.8117647 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.98039216 0.98039216 0.9764706  ... 0.98039216 0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.92941177 0.9411765  0.92156863 ... 0.92941177 0.9411765\n",
      "    0.92156863]\n",
      "   ...\n",
      "   [0.5019608  0.49019608 0.43137255 ... 0.5019608  0.49019608\n",
      "    0.43137255]\n",
      "   [0.78039217 0.80784315 0.8039216  ... 0.78039217 0.80784315\n",
      "    0.8039216 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]]\n",
      "\n",
      "\n",
      " [[[0.7764706  0.79607844 0.80784315 ... 0.7764706  0.79607844\n",
      "    0.80784315]\n",
      "   [0.7529412  0.76862746 0.77254903 ... 0.7529412  0.76862746\n",
      "    0.77254903]\n",
      "   [0.6039216  0.6039216  0.58431375 ... 0.6039216  0.6039216\n",
      "    0.58431375]\n",
      "   ...\n",
      "   [0.7882353  0.8156863  0.80784315 ... 0.7882353  0.8156863\n",
      "    0.80784315]\n",
      "   [0.78039217 0.8117647  0.8039216  ... 0.78039217 0.8117647\n",
      "    0.8039216 ]\n",
      "   [0.7529412  0.7921569  0.7882353  ... 0.7529412  0.7921569\n",
      "    0.7882353 ]]\n",
      "\n",
      "  [[0.78431374 0.8        0.8117647  ... 0.78431374 0.8\n",
      "    0.8117647 ]\n",
      "   [0.72156864 0.73333335 0.7372549  ... 0.72156864 0.73333335\n",
      "    0.7372549 ]\n",
      "   [0.42745098 0.41960785 0.39607844 ... 0.42745098 0.41960785\n",
      "    0.39607844]\n",
      "   ...\n",
      "   [0.78039217 0.80784315 0.80784315 ... 0.78039217 0.80784315\n",
      "    0.80784315]\n",
      "   [0.7882353  0.827451   0.81960785 ... 0.7882353  0.827451\n",
      "    0.81960785]\n",
      "   [0.76862746 0.8117647  0.8039216  ... 0.76862746 0.8117647\n",
      "    0.8039216 ]]\n",
      "\n",
      "  [[0.8117647  0.827451   0.84313726 ... 0.8117647  0.827451\n",
      "    0.84313726]\n",
      "   [0.65882355 0.67058825 0.6666667  ... 0.65882355 0.67058825\n",
      "    0.6666667 ]\n",
      "   [0.24313726 0.22745098 0.19607843 ... 0.24313726 0.22745098\n",
      "    0.19607843]\n",
      "   ...\n",
      "   [0.75686276 0.78431374 0.78431374 ... 0.75686276 0.78431374\n",
      "    0.78431374]\n",
      "   [0.7764706  0.8117647  0.80784315 ... 0.7764706  0.8117647\n",
      "    0.80784315]\n",
      "   [0.7764706  0.81960785 0.8117647  ... 0.7764706  0.81960785\n",
      "    0.8117647 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8509804  0.84705883 0.8509804  ... 0.8509804  0.84705883\n",
      "    0.8509804 ]\n",
      "   [0.8509804  0.8392157  0.83137256 ... 0.8509804  0.8392157\n",
      "    0.83137256]\n",
      "   [0.8745098  0.85882354 0.8509804  ... 0.8745098  0.85882354\n",
      "    0.8509804 ]\n",
      "   ...\n",
      "   [0.64705884 0.44705883 0.43529412 ... 0.64705884 0.44705883\n",
      "    0.43529412]\n",
      "   [0.68235296 0.49019608 0.4745098  ... 0.68235296 0.49019608\n",
      "    0.4745098 ]\n",
      "   [0.6862745  0.49803922 0.48235294 ... 0.6862745  0.49803922\n",
      "    0.48235294]]\n",
      "\n",
      "  [[0.85490197 0.8509804  0.8509804  ... 0.85490197 0.8509804\n",
      "    0.8509804 ]\n",
      "   [0.85490197 0.84705883 0.8509804  ... 0.85490197 0.84705883\n",
      "    0.8509804 ]\n",
      "   [0.87058824 0.8627451  0.85490197 ... 0.87058824 0.8627451\n",
      "    0.85490197]\n",
      "   ...\n",
      "   [0.6392157  0.44313726 0.42745098 ... 0.6392157  0.44313726\n",
      "    0.42745098]\n",
      "   [0.67058825 0.48235294 0.46666667 ... 0.67058825 0.48235294\n",
      "    0.46666667]\n",
      "   [0.67058825 0.4862745  0.46666667 ... 0.67058825 0.4862745\n",
      "    0.46666667]]\n",
      "\n",
      "  [[0.84705883 0.84705883 0.84705883 ... 0.84705883 0.84705883\n",
      "    0.84705883]\n",
      "   [0.85882354 0.85490197 0.8509804  ... 0.85882354 0.85490197\n",
      "    0.8509804 ]\n",
      "   [0.87058824 0.8666667  0.8627451  ... 0.87058824 0.8666667\n",
      "    0.8627451 ]\n",
      "   ...\n",
      "   [0.63529414 0.44313726 0.42745098 ... 0.63529414 0.44313726\n",
      "    0.42745098]\n",
      "   [0.654902   0.46666667 0.4509804  ... 0.654902   0.46666667\n",
      "    0.4509804 ]\n",
      "   [0.6509804  0.47058824 0.4509804  ... 0.6509804  0.47058824\n",
      "    0.4509804 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.04705882 0.04705882 0.04705882 ... 0.04705882 0.04705882\n",
      "    0.04705882]\n",
      "   [0.01176471 0.01176471 0.01176471 ... 0.01176471 0.01176471\n",
      "    0.01176471]\n",
      "   [0.03529412 0.03529412 0.03529412 ... 0.03529412 0.03529412\n",
      "    0.03529412]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.01176471 0.01176471 0.01176471 ... 0.01176471 0.01176471\n",
      "    0.01176471]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.04705882 0.04705882 0.04705882 ... 0.04705882 0.04705882\n",
      "    0.04705882]\n",
      "   [0.01176471 0.01176471 0.01176471 ... 0.01176471 0.01176471\n",
      "    0.01176471]\n",
      "   [0.03529412 0.03529412 0.03529412 ... 0.03529412 0.03529412\n",
      "    0.03529412]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.01176471 0.01176471 0.01176471 ... 0.01176471 0.01176471\n",
      "    0.01176471]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.49803922 0.49803922 0.5176471  ... 0.49803922 0.49803922\n",
      "    0.5176471 ]\n",
      "   [0.5058824  0.5019608  0.52156866 ... 0.5058824  0.5019608\n",
      "    0.52156866]\n",
      "   [0.50980395 0.5058824  0.5254902  ... 0.50980395 0.5058824\n",
      "    0.5254902 ]\n",
      "   ...\n",
      "   [0.5686275  0.57254905 0.5882353  ... 0.5686275  0.57254905\n",
      "    0.5882353 ]\n",
      "   [0.57254905 0.5764706  0.5921569  ... 0.57254905 0.5764706\n",
      "    0.5921569 ]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]]\n",
      "\n",
      "  [[0.50980395 0.50980395 0.5254902  ... 0.50980395 0.50980395\n",
      "    0.5254902 ]\n",
      "   [0.5137255  0.5137255  0.5294118  ... 0.5137255  0.5137255\n",
      "    0.5294118 ]\n",
      "   [0.5176471  0.5176471  0.53333336 ... 0.5176471  0.5176471\n",
      "    0.53333336]\n",
      "   ...\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5803922  0.58431375 0.6        ... 0.5803922  0.58431375\n",
      "    0.6       ]]\n",
      "\n",
      "  [[0.5176471  0.5176471  0.5372549  ... 0.5176471  0.5176471\n",
      "    0.5372549 ]\n",
      "   [0.52156866 0.5254902  0.5411765  ... 0.52156866 0.5254902\n",
      "    0.5411765 ]\n",
      "   [0.5176471  0.52156866 0.5372549  ... 0.5176471  0.52156866\n",
      "    0.5372549 ]\n",
      "   ...\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.5882353  0.5921569  0.60784316 ... 0.5882353  0.5921569\n",
      "    0.60784316]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8901961  0.8392157  0.8039216  ... 0.8901961  0.8392157\n",
      "    0.8039216 ]\n",
      "   [0.91764706 0.8745098  0.8392157  ... 0.91764706 0.8745098\n",
      "    0.8392157 ]\n",
      "   [0.8627451  0.79607844 0.7490196  ... 0.8627451  0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9372549  0.90588236 0.8784314  ... 0.9372549  0.90588236\n",
      "    0.8784314 ]\n",
      "   [0.90588236 0.85882354 0.8235294  ... 0.90588236 0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.87058824 0.8117647  0.7764706  ... 0.87058824 0.8117647\n",
      "    0.7764706 ]]\n",
      "\n",
      "  [[0.88235295 0.8235294  0.78431374 ... 0.88235295 0.8235294\n",
      "    0.78431374]\n",
      "   [0.89411765 0.8392157  0.8        ... 0.89411765 0.8392157\n",
      "    0.8       ]\n",
      "   [0.87058824 0.8039216  0.7607843  ... 0.87058824 0.8039216\n",
      "    0.7607843 ]\n",
      "   ...\n",
      "   [0.92156863 0.8862745  0.8509804  ... 0.92156863 0.8862745\n",
      "    0.8509804 ]\n",
      "   [0.9098039  0.87058824 0.8352941  ... 0.9098039  0.87058824\n",
      "    0.8352941 ]\n",
      "   [0.8745098  0.8156863  0.77254903 ... 0.8745098  0.8156863\n",
      "    0.77254903]]\n",
      "\n",
      "  [[0.8627451  0.7921569  0.7411765  ... 0.8627451  0.7921569\n",
      "    0.7411765 ]\n",
      "   [0.8666667  0.8039216  0.7529412  ... 0.8666667  0.8039216\n",
      "    0.7529412 ]\n",
      "   [0.85882354 0.79607844 0.7490196  ... 0.85882354 0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9098039  0.85882354 0.8235294  ... 0.9098039  0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.90588236 0.85490197 0.8156863  ... 0.90588236 0.85490197\n",
      "    0.8156863 ]\n",
      "   [0.8784314  0.8156863  0.7647059  ... 0.8784314  0.8156863\n",
      "    0.7647059 ]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (70, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (70, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 6s 1s/step - loss: 3.7744 - accuracy: 0.0000e+00 - val_loss: 2.9364 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/100\n",
      "Learning rate:  1e-05\n",
      "2/2 [==============================] - 2s 403ms/step - loss: 3.7535 - accuracy: 0.0000e+00 - val_loss: 2.9515 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/100\n",
      "Learning rate:  2e-05\n",
      "2/2 [==============================] - 2s 404ms/step - loss: 3.7471 - accuracy: 0.0000e+00 - val_loss: 2.9499 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/100\n",
      "Learning rate:  3e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 3.7289 - accuracy: 0.0000e+00 - val_loss: 2.9381 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/100\n",
      "Learning rate:  4e-05\n",
      "2/2 [==============================] - 2s 412ms/step - loss: 3.6908 - accuracy: 0.0000e+00 - val_loss: 2.9159 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/100\n",
      "Learning rate:  5e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 3.6479 - accuracy: 0.0000e+00 - val_loss: 2.8986 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/100\n",
      "Learning rate:  6e-05\n",
      "2/2 [==============================] - 2s 392ms/step - loss: 3.6190 - accuracy: 0.0000e+00 - val_loss: 2.8674 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/100\n",
      "Learning rate:  7.000000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 3.5530 - accuracy: 0.0000e+00 - val_loss: 2.8440 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/100\n",
      "Learning rate:  8e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 3.4895 - accuracy: 0.0000e+00 - val_loss: 2.8212 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/100\n",
      "Learning rate:  8.999999999999999e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 3.4202 - accuracy: 0.0000e+00 - val_loss: 2.7829 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/100\n",
      "Learning rate:  0.0001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 3.3418 - accuracy: 0.0000e+00 - val_loss: 2.7343 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/100\n",
      "Learning rate:  0.00011\n",
      "2/2 [==============================] - 2s 413ms/step - loss: 3.2927 - accuracy: 0.0000e+00 - val_loss: 2.6922 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/100\n",
      "Learning rate:  0.00012\n",
      "2/2 [==============================] - 2s 2s/step - loss: 3.1788 - accuracy: 0.0143 - val_loss: 2.6355 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/100\n",
      "Learning rate:  0.00013000000000000002\n",
      "2/2 [==============================] - 2s 2s/step - loss: 3.0999 - accuracy: 0.0571 - val_loss: 2.5881 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/100\n",
      "Learning rate:  0.00014000000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.9798 - accuracy: 0.1000 - val_loss: 2.5366 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/100\n",
      "Learning rate:  0.00015\n",
      "2/2 [==============================] - 3s 3s/step - loss: 2.8714 - accuracy: 0.1571 - val_loss: 2.4718 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/100\n",
      "Learning rate:  0.00016\n",
      "2/2 [==============================] - 3s 543ms/step - loss: 2.8292 - accuracy: 0.1857 - val_loss: 2.4091 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/100\n",
      "Learning rate:  0.00017\n",
      "2/2 [==============================] - 3s 482ms/step - loss: 2.6864 - accuracy: 0.4000 - val_loss: 2.3479 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/100\n",
      "Learning rate:  0.00017999999999999998\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.5795 - accuracy: 0.5000 - val_loss: 2.2767 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/100\n",
      "Learning rate:  0.00019\n",
      "2/2 [==============================] - 3s 2s/step - loss: 2.4278 - accuracy: 0.6143 - val_loss: 2.2091 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 21/100\n",
      "Learning rate:  0.0002\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.3017 - accuracy: 0.6286 - val_loss: 2.1483 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 22/100\n",
      "Learning rate:  0.00021\n",
      "2/2 [==============================] - 2s 516ms/step - loss: 2.2113 - accuracy: 0.6714 - val_loss: 2.0890 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 23/100\n",
      "Learning rate:  0.00022\n",
      "2/2 [==============================] - 2s 399ms/step - loss: 2.1402 - accuracy: 0.6714 - val_loss: 2.0157 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 24/100\n",
      "Learning rate:  0.00023\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.9559 - accuracy: 0.7000 - val_loss: 1.9694 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 25/100\n",
      "Learning rate:  0.00024\n",
      "2/2 [==============================] - 2s 321ms/step - loss: 1.9347 - accuracy: 0.6714 - val_loss: 1.9474 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 26/100\n",
      "Learning rate:  0.00025\n",
      "2/2 [==============================] - 2s 318ms/step - loss: 1.8290 - accuracy: 0.7429 - val_loss: 1.9106 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 27/100\n",
      "Learning rate:  0.00026000000000000003\n",
      "2/2 [==============================] - 2s 1s/step - loss: 1.6088 - accuracy: 0.7857 - val_loss: 1.8407 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 28/100\n",
      "Learning rate:  0.00027\n",
      "2/2 [==============================] - 2s 306ms/step - loss: 1.5756 - accuracy: 0.7714 - val_loss: 1.7415 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 29/100\n",
      "Learning rate:  0.00028000000000000003\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.5823 - accuracy: 0.7429 - val_loss: 1.7018 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 30/100\n",
      "Learning rate:  0.00029\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.4498 - accuracy: 0.7714 - val_loss: 1.6536 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 31/100\n",
      "Learning rate:  0.0003\n",
      "2/2 [==============================] - 2s 399ms/step - loss: 1.4059 - accuracy: 0.8000 - val_loss: 1.6701 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 32/100\n",
      "Learning rate:  0.00031\n",
      "2/2 [==============================] - 2s 381ms/step - loss: 1.3422 - accuracy: 0.8000 - val_loss: 1.6844 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 33/100\n",
      "Learning rate:  0.00032\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.2687 - accuracy: 0.7714 - val_loss: 1.6013 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 34/100\n",
      "Learning rate:  0.00033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 376ms/step - loss: 1.2426 - accuracy: 0.8000 - val_loss: 1.5884 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 35/100\n",
      "Learning rate:  0.00034\n",
      "2/2 [==============================] - 2s 420ms/step - loss: 1.1916 - accuracy: 0.8571 - val_loss: 1.6723 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 36/100\n",
      "Learning rate:  0.00035\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.1219 - accuracy: 0.8571 - val_loss: 1.8342 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 37/100\n",
      "Learning rate:  0.00035999999999999997\n",
      "2/2 [==============================] - 3s 540ms/step - loss: 1.1125 - accuracy: 0.8571 - val_loss: 2.2488 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 38/100\n",
      "Learning rate:  0.00037\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0495 - accuracy: 0.8857 - val_loss: 2.9678 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 39/100\n",
      "Learning rate:  0.00038\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9848 - accuracy: 0.9143 - val_loss: 3.6685 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 40/100\n",
      "Learning rate:  0.00039000000000000005\n",
      "2/2 [==============================] - 2s 437ms/step - loss: 0.9683 - accuracy: 0.9000 - val_loss: 4.9054 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 41/100\n",
      "Learning rate:  0.0004\n",
      "2/2 [==============================] - 2s 454ms/step - loss: 0.9962 - accuracy: 0.8857 - val_loss: 6.3740 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 42/100\n",
      "Learning rate:  0.00041\n",
      "2/2 [==============================] - 3s 464ms/step - loss: 0.9436 - accuracy: 0.9000 - val_loss: 7.1378 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 43/100\n",
      "Learning rate:  0.00042\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9477 - accuracy: 0.8857 - val_loss: 6.3980 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 44/100\n",
      "Learning rate:  0.00043\n",
      "2/2 [==============================] - 2s 478ms/step - loss: 0.9769 - accuracy: 0.8571 - val_loss: 5.0712 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 45/100\n",
      "Learning rate:  0.00044\n",
      "2/2 [==============================] - 2s 568ms/step - loss: 0.8655 - accuracy: 0.9143 - val_loss: 3.5804 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 46/100\n",
      "Learning rate:  0.00045000000000000004\n",
      "2/2 [==============================] - 2s 453ms/step - loss: 0.9240 - accuracy: 0.8714 - val_loss: 2.7036 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 47/100\n",
      "Learning rate:  0.00046\n",
      "2/2 [==============================] - 2s 469ms/step - loss: 0.9078 - accuracy: 0.8571 - val_loss: 2.3499 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 48/100\n",
      "Learning rate:  0.00047\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8253 - accuracy: 0.9143 - val_loss: 2.2313 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 49/100\n",
      "Learning rate:  0.00048\n",
      "2/2 [==============================] - 2s 554ms/step - loss: 0.8279 - accuracy: 0.9571 - val_loss: 2.6492 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 50/100\n",
      "Learning rate:  0.00049\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8079 - accuracy: 0.9429 - val_loss: 3.5058 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 51/100\n",
      "Learning rate:  0.0005\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7700 - accuracy: 0.9857 - val_loss: 5.1968 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 52/100\n",
      "Learning rate:  0.00051\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8279 - accuracy: 0.9286 - val_loss: 6.1564 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 53/100\n",
      "Learning rate:  0.0005200000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7719 - accuracy: 0.9714 - val_loss: 7.2450 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 54/100\n",
      "Learning rate:  0.0005300000000000001\n",
      "2/2 [==============================] - 2s 418ms/step - loss: 0.8234 - accuracy: 0.9429 - val_loss: 8.3529 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 55/100\n",
      "Learning rate:  0.00054\n",
      "2/2 [==============================] - 2s 409ms/step - loss: 0.8051 - accuracy: 0.9571 - val_loss: 8.3367 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 56/100\n",
      "Learning rate:  0.00055\n",
      "2/2 [==============================] - 2s 402ms/step - loss: 0.8328 - accuracy: 0.9143 - val_loss: 7.3948 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 57/100\n",
      "Learning rate:  0.0005600000000000001\n",
      "2/2 [==============================] - 2s 389ms/step - loss: 0.7954 - accuracy: 0.9571 - val_loss: 5.2679 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 58/100\n",
      "Learning rate:  0.00057\n",
      "2/2 [==============================] - 2s 396ms/step - loss: 0.8042 - accuracy: 0.9286 - val_loss: 2.5562 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 59/100\n",
      "Learning rate:  0.00058\n",
      "2/2 [==============================] - 2s 405ms/step - loss: 0.9325 - accuracy: 0.8857 - val_loss: 1.1565 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 60/100\n",
      "Learning rate:  0.00059\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7964 - accuracy: 0.9429 - val_loss: 0.8233 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 61/100\n",
      "Learning rate:  0.0006\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8343 - accuracy: 0.9000 - val_loss: 0.7674 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 62/100\n",
      "Learning rate:  0.00061\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7513 - accuracy: 0.9571 - val_loss: 0.7959 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 63/100\n",
      "Learning rate:  0.00062\n",
      "2/2 [==============================] - 2s 401ms/step - loss: 0.7242 - accuracy: 0.9857 - val_loss: 0.9029 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 64/100\n",
      "Learning rate:  0.00063\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8124 - accuracy: 0.9143 - val_loss: 1.1003 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 65/100\n",
      "Learning rate:  0.00064\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7917 - accuracy: 0.9571 - val_loss: 1.2160 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 66/100\n",
      "Learning rate:  0.0006500000000000001\n",
      "2/2 [==============================] - 2s 492ms/step - loss: 0.8872 - accuracy: 0.9143 - val_loss: 1.6538 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 67/100\n",
      "Learning rate:  0.00066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 2s/step - loss: 0.8178 - accuracy: 0.9000 - val_loss: 2.3342 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 68/100\n",
      "Learning rate:  0.00067\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8179 - accuracy: 0.9000 - val_loss: 2.0172 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 69/100\n",
      "Learning rate:  0.00068\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.8094 - accuracy: 0.9429 - val_loss: 1.7063 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 70/100\n",
      "Learning rate:  0.00069\n",
      "2/2 [==============================] - 3s 457ms/step - loss: 0.7612 - accuracy: 0.9714 - val_loss: 1.1209 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 71/100\n",
      "Learning rate:  0.0007\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7984 - accuracy: 0.9286 - val_loss: 0.8977 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 72/100\n",
      "Learning rate:  0.00071\n",
      "2/2 [==============================] - 2s 421ms/step - loss: 0.8362 - accuracy: 0.9429 - val_loss: 1.0714 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 73/100\n",
      "Learning rate:  0.0007199999999999999\n",
      "2/2 [==============================] - 2s 397ms/step - loss: 0.7777 - accuracy: 0.9286 - val_loss: 2.0153 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 74/100\n",
      "Learning rate:  0.00073\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7567 - accuracy: 0.9429 - val_loss: 2.2783 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 75/100\n",
      "Learning rate:  0.00074\n",
      "2/2 [==============================] - 2s 444ms/step - loss: 0.7785 - accuracy: 0.9286 - val_loss: 1.7441 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 76/100\n",
      "Learning rate:  0.00075\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7606 - accuracy: 0.9429 - val_loss: 0.9581 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 77/100\n",
      "Learning rate:  0.00076\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7601 - accuracy: 0.9429 - val_loss: 0.7926 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 78/100\n",
      "Learning rate:  0.0007700000000000001\n",
      "2/2 [==============================] - 2s 512ms/step - loss: 0.7547 - accuracy: 0.9571 - val_loss: 0.6968 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 79/100\n",
      "Learning rate:  0.0007800000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7363 - accuracy: 0.9714 - val_loss: 0.6595 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 80/100\n",
      "Learning rate:  0.00079\n",
      "2/2 [==============================] - 2s 398ms/step - loss: 0.7077 - accuracy: 0.9857 - val_loss: 0.6379 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 81/100\n",
      "Learning rate:  0.0008\n",
      "2/2 [==============================] - 3s 738ms/step - loss: 0.7030 - accuracy: 0.9857 - val_loss: 0.6373 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 82/100\n",
      "Learning rate:  8.1e-05\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.7095 - accuracy: 0.9857 - val_loss: 0.6413 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 83/100\n",
      "Learning rate:  8.2e-05\n",
      "2/2 [==============================] - 3s 524ms/step - loss: 0.7444 - accuracy: 0.9571 - val_loss: 0.6505 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 84/100\n",
      "Learning rate:  8.3e-05\n",
      "2/2 [==============================] - 3s 540ms/step - loss: 0.7053 - accuracy: 0.9714 - val_loss: 0.6616 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 85/100\n",
      "Learning rate:  8.4e-05\n",
      "2/2 [==============================] - 3s 522ms/step - loss: 0.8553 - accuracy: 0.9286 - val_loss: 0.6746 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 86/100\n",
      "Learning rate:  8.5e-05\n",
      "2/2 [==============================] - 2s 478ms/step - loss: 0.7122 - accuracy: 0.9714 - val_loss: 0.6988 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 87/100\n",
      "Learning rate:  8.6e-05\n",
      "2/2 [==============================] - 3s 532ms/step - loss: 0.7030 - accuracy: 0.9571 - val_loss: 0.7253 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 88/100\n",
      "Learning rate:  8.7e-05\n",
      "2/2 [==============================] - 3s 446ms/step - loss: 0.7394 - accuracy: 0.9714 - val_loss: 0.7583 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 89/100\n",
      "Learning rate:  8.800000000000001e-05\n",
      "2/2 [==============================] - 3s 524ms/step - loss: 0.6968 - accuracy: 0.9857 - val_loss: 0.8089 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 90/100\n",
      "Learning rate:  8.900000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6967 - accuracy: 0.9714 - val_loss: 0.8565 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 91/100\n",
      "Learning rate:  9e-05\n",
      "2/2 [==============================] - 2s 454ms/step - loss: 0.6977 - accuracy: 0.9857 - val_loss: 0.8954 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 92/100\n",
      "Learning rate:  9.1e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6944 - accuracy: 0.9857 - val_loss: 0.9272 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 93/100\n",
      "Learning rate:  9.200000000000001e-05\n",
      "2/2 [==============================] - 3s 525ms/step - loss: 0.7321 - accuracy: 0.9714 - val_loss: 0.9684 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 94/100\n",
      "Learning rate:  9.300000000000001e-05\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.6879 - accuracy: 0.9714 - val_loss: 1.0330 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 95/100\n",
      "Learning rate:  9.4e-05\n",
      "2/2 [==============================] - 3s 406ms/step - loss: 0.6940 - accuracy: 0.9714 - val_loss: 1.1023 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 96/100\n",
      "Learning rate:  9.5e-05\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.6835 - accuracy: 0.9714 - val_loss: 1.1827 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 97/100\n",
      "Learning rate:  9.6e-05\n",
      "2/2 [==============================] - 3s 572ms/step - loss: 0.7082 - accuracy: 0.9714 - val_loss: 1.2420 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 98/100\n",
      "Learning rate:  9.7e-05\n",
      "2/2 [==============================] - 3s 525ms/step - loss: 0.7161 - accuracy: 0.9714 - val_loss: 1.2777 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 99/100\n",
      "Learning rate:  9.8e-05\n",
      "2/2 [==============================] - 3s 497ms/step - loss: 0.7009 - accuracy: 0.9857 - val_loss: 1.3307 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 100/100\n",
      "Learning rate:  9.900000000000001e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 481ms/step - loss: 0.7586 - accuracy: 0.9571 - val_loss: 1.4027 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 1.3157 - accuracy: 0.8333\n",
      "Test loss: 1.315671443939209\n",
      "Test accuracy: 0.8333333134651184\n",
      "[1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision:  0.8928571428571428\n",
      "Recall:  0.8333333333333334\n",
      "F1 Score:  0.8401880141010576\n",
      "30\n",
      "30\n",
      "[[[[ 30  95 105 ...  30  95 105]\n",
      "   [ 31  97 107 ...  31  97 107]\n",
      "   [ 30  96 106 ...  30  96 106]\n",
      "   ...\n",
      "   [ 26  91 103 ...  26  91 103]\n",
      "   [ 25  92 102 ...  25  92 102]\n",
      "   [ 24  91 101 ...  24  91 101]]\n",
      "\n",
      "  [[ 27 111 122 ...  27 111 122]\n",
      "   [ 27 111 123 ...  27 111 123]\n",
      "   [ 28 111 123 ...  28 111 123]\n",
      "   ...\n",
      "   [ 22 107 119 ...  22 107 119]\n",
      "   [ 22 108 118 ...  22 108 118]\n",
      "   [ 23 108 119 ...  23 108 119]]\n",
      "\n",
      "  [[ 24 105 116 ...  24 105 116]\n",
      "   [ 25 107 118 ...  25 107 118]\n",
      "   [ 28 108 119 ...  28 108 119]\n",
      "   ...\n",
      "   [ 26 107 117 ...  26 107 117]\n",
      "   [ 23 105 115 ...  23 105 115]\n",
      "   [ 22 104 116 ...  22 104 116]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  7  29  34 ...   7  29  34]\n",
      "   [ 87  67  56 ...  87  67  56]\n",
      "   [179 128 119 ... 179 128 119]\n",
      "   ...\n",
      "   [195 132 133 ... 195 132 133]\n",
      "   [119  77  72 ... 119  77  72]\n",
      "   [ 50  35  35 ...  50  35  35]]\n",
      "\n",
      "  [[  1  41  49 ...   1  41  49]\n",
      "   [ 72  61  52 ...  72  61  52]\n",
      "   [169 118 107 ... 169 118 107]\n",
      "   ...\n",
      "   [186 131 126 ... 186 131 126]\n",
      "   [ 97  65  58 ...  97  65  58]\n",
      "   [ 30  27  31 ...  30  27  31]]\n",
      "\n",
      "  [[  5  56  65 ...   5  56  65]\n",
      "   [ 63  60  53 ...  63  60  53]\n",
      "   [161 113 103 ... 161 113 103]\n",
      "   ...\n",
      "   [176 123 117 ... 176 123 117]\n",
      "   [ 76  58  54 ...  76  58  54]\n",
      "   [ 19  35  42 ...  19  35  42]]]\n",
      "\n",
      "\n",
      " [[[255 255 255 ... 255 255 255]\n",
      "   [243 246 242 ... 243 246 242]\n",
      "   [214 220 205 ... 214 220 205]\n",
      "   ...\n",
      "   [106 120  98 ... 106 120  98]\n",
      "   [210 215 205 ... 210 215 205]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [246 248 245 ... 246 248 245]\n",
      "   [217 226 213 ... 217 226 213]\n",
      "   ...\n",
      "   [108 123  99 ... 108 123  99]\n",
      "   [211 216 206 ... 211 216 206]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [248 251 248 ... 248 251 248]\n",
      "   [221 232 221 ... 221 232 221]\n",
      "   ...\n",
      "   [110 125 100 ... 110 125 100]\n",
      "   [212 217 206 ... 212 217 206]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [249 250 249 ... 249 250 249]\n",
      "   [232 229 221 ... 232 229 221]\n",
      "   ...\n",
      "   [ 86 101  95 ...  86 101  95]\n",
      "   [204 210 208 ... 204 210 208]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [250 250 249 ... 250 250 249]\n",
      "   [234 236 231 ... 234 236 231]\n",
      "   ...\n",
      "   [102 111 101 ... 102 111 101]\n",
      "   [202 208 207 ... 202 208 207]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [250 250 249 ... 250 250 249]\n",
      "   [237 240 235 ... 237 240 235]\n",
      "   ...\n",
      "   [128 125 110 ... 128 125 110]\n",
      "   [199 206 205 ... 199 206 205]\n",
      "   [255 255 255 ... 255 255 255]]]\n",
      "\n",
      "\n",
      " [[[ 93  64  41 ...  93  64  41]\n",
      "   [101  73  50 ... 101  73  50]\n",
      "   [109  83  61 ... 109  83  61]\n",
      "   ...\n",
      "   [ 92  57  35 ...  92  57  35]\n",
      "   [ 88  52  30 ...  88  52  30]\n",
      "   [ 83  47  26 ...  83  47  26]]\n",
      "\n",
      "  [[101  73  50 ... 101  73  50]\n",
      "   [109  82  60 ... 109  82  60]\n",
      "   [116  92  70 ... 116  92  70]\n",
      "   ...\n",
      "   [ 97  62  39 ...  97  62  39]\n",
      "   [ 92  56  34 ...  92  56  34]\n",
      "   [ 87  51  30 ...  87  51  30]]\n",
      "\n",
      "  [[108  82  59 ... 108  82  59]\n",
      "   [115  91  70 ... 115  91  70]\n",
      "   [122 100  81 ... 122 100  81]\n",
      "   ...\n",
      "   [101  66  44 ... 101  66  44]\n",
      "   [ 96  60  38 ...  96  60  38]\n",
      "   [ 91  54  33 ...  91  54  33]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 87  89  90 ...  87  89  90]\n",
      "   [ 98 100 102 ...  98 100 102]\n",
      "   [112 115 118 ... 112 115 118]\n",
      "   ...\n",
      "   [242 187 163 ... 242 187 163]\n",
      "   [230 169 145 ... 230 169 145]\n",
      "   [209 149 124 ... 209 149 124]]\n",
      "\n",
      "  [[ 81  83  83 ...  81  83  83]\n",
      "   [ 87  89  91 ...  87  89  91]\n",
      "   [106 108 111 ... 106 108 111]\n",
      "   ...\n",
      "   [235 180 155 ... 235 180 155]\n",
      "   [224 167 141 ... 224 167 141]\n",
      "   [204 143 118 ... 204 143 118]]\n",
      "\n",
      "  [[ 78  80  80 ...  78  80  80]\n",
      "   [ 79  80  82 ...  79  80  82]\n",
      "   [ 94  96  99 ...  94  96  99]\n",
      "   ...\n",
      "   [229 174 149 ... 229 174 149]\n",
      "   [218 160 135 ... 218 160 135]\n",
      "   [197 137 113 ... 197 137 113]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (70, 40, 30, 9)\n",
      "70 train samples\n",
      "30 test samples\n",
      "y_train shape: (70, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_744 (Conv2D)             (None, 40, 30, 16)   1312        input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_672 (BatchN (None, 40, 30, 16)   64          conv2d_744[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_672 (Activation)     (None, 40, 30, 16)   0           batch_normalization_672[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_745 (Conv2D)             (None, 40, 30, 16)   272         activation_672[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_673 (BatchN (None, 40, 30, 16)   64          conv2d_745[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_673 (Activation)     (None, 40, 30, 16)   0           batch_normalization_673[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_746 (Conv2D)             (None, 40, 30, 16)   2320        activation_673[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_674 (BatchN (None, 40, 30, 16)   64          conv2d_746[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_674 (Activation)     (None, 40, 30, 16)   0           batch_normalization_674[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_748 (Conv2D)             (None, 40, 30, 64)   1088        activation_672[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_747 (Conv2D)             (None, 40, 30, 64)   1088        activation_674[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_216 (Add)                   (None, 40, 30, 64)   0           conv2d_748[0][0]                 \n",
      "                                                                 conv2d_747[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_675 (BatchN (None, 40, 30, 64)   256         add_216[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_675 (Activation)     (None, 40, 30, 64)   0           batch_normalization_675[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_749 (Conv2D)             (None, 40, 30, 16)   1040        activation_675[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_676 (BatchN (None, 40, 30, 16)   64          conv2d_749[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_676 (Activation)     (None, 40, 30, 16)   0           batch_normalization_676[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_750 (Conv2D)             (None, 40, 30, 16)   2320        activation_676[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_677 (BatchN (None, 40, 30, 16)   64          conv2d_750[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_677 (Activation)     (None, 40, 30, 16)   0           batch_normalization_677[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_751 (Conv2D)             (None, 40, 30, 64)   1088        activation_677[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_217 (Add)                   (None, 40, 30, 64)   0           add_216[0][0]                    \n",
      "                                                                 conv2d_751[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_678 (BatchN (None, 40, 30, 64)   256         add_217[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_678 (Activation)     (None, 40, 30, 64)   0           batch_normalization_678[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_752 (Conv2D)             (None, 40, 30, 16)   1040        activation_678[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_679 (BatchN (None, 40, 30, 16)   64          conv2d_752[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_679 (Activation)     (None, 40, 30, 16)   0           batch_normalization_679[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_753 (Conv2D)             (None, 40, 30, 16)   2320        activation_679[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_680 (BatchN (None, 40, 30, 16)   64          conv2d_753[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_680 (Activation)     (None, 40, 30, 16)   0           batch_normalization_680[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_754 (Conv2D)             (None, 40, 30, 64)   1088        activation_680[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_218 (Add)                   (None, 40, 30, 64)   0           add_217[0][0]                    \n",
      "                                                                 conv2d_754[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_681 (BatchN (None, 40, 30, 64)   256         add_218[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_681 (Activation)     (None, 40, 30, 64)   0           batch_normalization_681[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_755 (Conv2D)             (None, 20, 15, 64)   4160        activation_681[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_682 (BatchN (None, 20, 15, 64)   256         conv2d_755[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_682 (Activation)     (None, 20, 15, 64)   0           batch_normalization_682[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_756 (Conv2D)             (None, 20, 15, 64)   36928       activation_682[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_683 (BatchN (None, 20, 15, 64)   256         conv2d_756[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_683 (Activation)     (None, 20, 15, 64)   0           batch_normalization_683[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_758 (Conv2D)             (None, 20, 15, 128)  8320        add_218[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_757 (Conv2D)             (None, 20, 15, 128)  8320        activation_683[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_219 (Add)                   (None, 20, 15, 128)  0           conv2d_758[0][0]                 \n",
      "                                                                 conv2d_757[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_684 (BatchN (None, 20, 15, 128)  512         add_219[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_684 (Activation)     (None, 20, 15, 128)  0           batch_normalization_684[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_759 (Conv2D)             (None, 20, 15, 64)   8256        activation_684[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_685 (BatchN (None, 20, 15, 64)   256         conv2d_759[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_685 (Activation)     (None, 20, 15, 64)   0           batch_normalization_685[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_760 (Conv2D)             (None, 20, 15, 64)   36928       activation_685[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_686 (BatchN (None, 20, 15, 64)   256         conv2d_760[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_686 (Activation)     (None, 20, 15, 64)   0           batch_normalization_686[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_761 (Conv2D)             (None, 20, 15, 128)  8320        activation_686[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_220 (Add)                   (None, 20, 15, 128)  0           add_219[0][0]                    \n",
      "                                                                 conv2d_761[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_687 (BatchN (None, 20, 15, 128)  512         add_220[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_687 (Activation)     (None, 20, 15, 128)  0           batch_normalization_687[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_762 (Conv2D)             (None, 20, 15, 64)   8256        activation_687[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_688 (BatchN (None, 20, 15, 64)   256         conv2d_762[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_688 (Activation)     (None, 20, 15, 64)   0           batch_normalization_688[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_763 (Conv2D)             (None, 20, 15, 64)   36928       activation_688[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_689 (BatchN (None, 20, 15, 64)   256         conv2d_763[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_689 (Activation)     (None, 20, 15, 64)   0           batch_normalization_689[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_764 (Conv2D)             (None, 20, 15, 128)  8320        activation_689[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_221 (Add)                   (None, 20, 15, 128)  0           add_220[0][0]                    \n",
      "                                                                 conv2d_764[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_690 (BatchN (None, 20, 15, 128)  512         add_221[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_690 (Activation)     (None, 20, 15, 128)  0           batch_normalization_690[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_765 (Conv2D)             (None, 10, 8, 128)   16512       activation_690[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_691 (BatchN (None, 10, 8, 128)   512         conv2d_765[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_691 (Activation)     (None, 10, 8, 128)   0           batch_normalization_691[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_766 (Conv2D)             (None, 10, 8, 128)   147584      activation_691[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_692 (BatchN (None, 10, 8, 128)   512         conv2d_766[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_692 (Activation)     (None, 10, 8, 128)   0           batch_normalization_692[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_768 (Conv2D)             (None, 10, 8, 256)   33024       add_221[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_767 (Conv2D)             (None, 10, 8, 256)   33024       activation_692[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_222 (Add)                   (None, 10, 8, 256)   0           conv2d_768[0][0]                 \n",
      "                                                                 conv2d_767[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_693 (BatchN (None, 10, 8, 256)   1024        add_222[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_693 (Activation)     (None, 10, 8, 256)   0           batch_normalization_693[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_769 (Conv2D)             (None, 10, 8, 128)   32896       activation_693[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_694 (BatchN (None, 10, 8, 128)   512         conv2d_769[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_694 (Activation)     (None, 10, 8, 128)   0           batch_normalization_694[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_770 (Conv2D)             (None, 10, 8, 128)   147584      activation_694[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_695 (BatchN (None, 10, 8, 128)   512         conv2d_770[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_695 (Activation)     (None, 10, 8, 128)   0           batch_normalization_695[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_771 (Conv2D)             (None, 10, 8, 256)   33024       activation_695[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_223 (Add)                   (None, 10, 8, 256)   0           add_222[0][0]                    \n",
      "                                                                 conv2d_771[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_696 (BatchN (None, 10, 8, 256)   1024        add_223[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_696 (Activation)     (None, 10, 8, 256)   0           batch_normalization_696[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_772 (Conv2D)             (None, 10, 8, 128)   32896       activation_696[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_697 (BatchN (None, 10, 8, 128)   512         conv2d_772[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_697 (Activation)     (None, 10, 8, 128)   0           batch_normalization_697[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_773 (Conv2D)             (None, 10, 8, 128)   147584      activation_697[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_698 (BatchN (None, 10, 8, 128)   512         conv2d_773[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_698 (Activation)     (None, 10, 8, 128)   0           batch_normalization_698[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_774 (Conv2D)             (None, 10, 8, 256)   33024       activation_698[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_224 (Add)                   (None, 10, 8, 256)   0           add_223[0][0]                    \n",
      "                                                                 conv2d_774[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_699 (BatchN (None, 10, 8, 256)   1024        add_224[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_699 (Activation)     (None, 10, 8, 256)   0           batch_normalization_699[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_24 (AveragePo (None, 1, 1, 256)    0           activation_699[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 256)          0           average_pooling2d_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 10)           2570        flatten_24[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[0.11764706 0.37254903 0.4117647  ... 0.11764706 0.37254903\n",
      "    0.4117647 ]\n",
      "   [0.12156863 0.38039216 0.41960785 ... 0.12156863 0.38039216\n",
      "    0.41960785]\n",
      "   [0.11764706 0.3764706  0.41568628 ... 0.11764706 0.3764706\n",
      "    0.41568628]\n",
      "   ...\n",
      "   [0.10196079 0.35686275 0.40392157 ... 0.10196079 0.35686275\n",
      "    0.40392157]\n",
      "   [0.09803922 0.36078432 0.4        ... 0.09803922 0.36078432\n",
      "    0.4       ]\n",
      "   [0.09411765 0.35686275 0.39607844 ... 0.09411765 0.35686275\n",
      "    0.39607844]]\n",
      "\n",
      "  [[0.10588235 0.43529412 0.47843137 ... 0.10588235 0.43529412\n",
      "    0.47843137]\n",
      "   [0.10588235 0.43529412 0.48235294 ... 0.10588235 0.43529412\n",
      "    0.48235294]\n",
      "   [0.10980392 0.43529412 0.48235294 ... 0.10980392 0.43529412\n",
      "    0.48235294]\n",
      "   ...\n",
      "   [0.08627451 0.41960785 0.46666667 ... 0.08627451 0.41960785\n",
      "    0.46666667]\n",
      "   [0.08627451 0.42352942 0.4627451  ... 0.08627451 0.42352942\n",
      "    0.4627451 ]\n",
      "   [0.09019608 0.42352942 0.46666667 ... 0.09019608 0.42352942\n",
      "    0.46666667]]\n",
      "\n",
      "  [[0.09411765 0.4117647  0.45490196 ... 0.09411765 0.4117647\n",
      "    0.45490196]\n",
      "   [0.09803922 0.41960785 0.4627451  ... 0.09803922 0.41960785\n",
      "    0.4627451 ]\n",
      "   [0.10980392 0.42352942 0.46666667 ... 0.10980392 0.42352942\n",
      "    0.46666667]\n",
      "   ...\n",
      "   [0.10196079 0.41960785 0.45882353 ... 0.10196079 0.41960785\n",
      "    0.45882353]\n",
      "   [0.09019608 0.4117647  0.4509804  ... 0.09019608 0.4117647\n",
      "    0.4509804 ]\n",
      "   [0.08627451 0.40784314 0.45490196 ... 0.08627451 0.40784314\n",
      "    0.45490196]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.02745098 0.11372549 0.13333334 ... 0.02745098 0.11372549\n",
      "    0.13333334]\n",
      "   [0.34117648 0.2627451  0.21960784 ... 0.34117648 0.2627451\n",
      "    0.21960784]\n",
      "   [0.7019608  0.5019608  0.46666667 ... 0.7019608  0.5019608\n",
      "    0.46666667]\n",
      "   ...\n",
      "   [0.7647059  0.5176471  0.52156866 ... 0.7647059  0.5176471\n",
      "    0.52156866]\n",
      "   [0.46666667 0.3019608  0.28235295 ... 0.46666667 0.3019608\n",
      "    0.28235295]\n",
      "   [0.19607843 0.13725491 0.13725491 ... 0.19607843 0.13725491\n",
      "    0.13725491]]\n",
      "\n",
      "  [[0.00392157 0.16078432 0.19215687 ... 0.00392157 0.16078432\n",
      "    0.19215687]\n",
      "   [0.28235295 0.23921569 0.20392157 ... 0.28235295 0.23921569\n",
      "    0.20392157]\n",
      "   [0.6627451  0.4627451  0.41960785 ... 0.6627451  0.4627451\n",
      "    0.41960785]\n",
      "   ...\n",
      "   [0.7294118  0.5137255  0.49411765 ... 0.7294118  0.5137255\n",
      "    0.49411765]\n",
      "   [0.38039216 0.25490198 0.22745098 ... 0.38039216 0.25490198\n",
      "    0.22745098]\n",
      "   [0.11764706 0.10588235 0.12156863 ... 0.11764706 0.10588235\n",
      "    0.12156863]]\n",
      "\n",
      "  [[0.01960784 0.21960784 0.25490198 ... 0.01960784 0.21960784\n",
      "    0.25490198]\n",
      "   [0.24705882 0.23529412 0.20784314 ... 0.24705882 0.23529412\n",
      "    0.20784314]\n",
      "   [0.6313726  0.44313726 0.40392157 ... 0.6313726  0.44313726\n",
      "    0.40392157]\n",
      "   ...\n",
      "   [0.6901961  0.48235294 0.45882353 ... 0.6901961  0.48235294\n",
      "    0.45882353]\n",
      "   [0.29803923 0.22745098 0.21176471 ... 0.29803923 0.22745098\n",
      "    0.21176471]\n",
      "   [0.07450981 0.13725491 0.16470589 ... 0.07450981 0.13725491\n",
      "    0.16470589]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9529412  0.9647059  0.9490196  ... 0.9529412  0.9647059\n",
      "    0.9490196 ]\n",
      "   [0.8392157  0.8627451  0.8039216  ... 0.8392157  0.8627451\n",
      "    0.8039216 ]\n",
      "   ...\n",
      "   [0.41568628 0.47058824 0.38431373 ... 0.41568628 0.47058824\n",
      "    0.38431373]\n",
      "   [0.8235294  0.84313726 0.8039216  ... 0.8235294  0.84313726\n",
      "    0.8039216 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9647059  0.972549   0.9607843  ... 0.9647059  0.972549\n",
      "    0.9607843 ]\n",
      "   [0.8509804  0.8862745  0.8352941  ... 0.8509804  0.8862745\n",
      "    0.8352941 ]\n",
      "   ...\n",
      "   [0.42352942 0.48235294 0.3882353  ... 0.42352942 0.48235294\n",
      "    0.3882353 ]\n",
      "   [0.827451   0.84705883 0.80784315 ... 0.827451   0.84705883\n",
      "    0.80784315]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.972549   0.9843137  0.972549   ... 0.972549   0.9843137\n",
      "    0.972549  ]\n",
      "   [0.8666667  0.9098039  0.8666667  ... 0.8666667  0.9098039\n",
      "    0.8666667 ]\n",
      "   ...\n",
      "   [0.43137255 0.49019608 0.39215687 ... 0.43137255 0.49019608\n",
      "    0.39215687]\n",
      "   [0.83137256 0.8509804  0.80784315 ... 0.83137256 0.8509804\n",
      "    0.80784315]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9764706  0.98039216 0.9764706  ... 0.9764706  0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.9098039  0.8980392  0.8666667  ... 0.9098039  0.8980392\n",
      "    0.8666667 ]\n",
      "   ...\n",
      "   [0.3372549  0.39607844 0.37254903 ... 0.3372549  0.39607844\n",
      "    0.37254903]\n",
      "   [0.8        0.8235294  0.8156863  ... 0.8        0.8235294\n",
      "    0.8156863 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.98039216 0.98039216 0.9764706  ... 0.98039216 0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.91764706 0.9254902  0.90588236 ... 0.91764706 0.9254902\n",
      "    0.90588236]\n",
      "   ...\n",
      "   [0.4        0.43529412 0.39607844 ... 0.4        0.43529412\n",
      "    0.39607844]\n",
      "   [0.7921569  0.8156863  0.8117647  ... 0.7921569  0.8156863\n",
      "    0.8117647 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.98039216 0.98039216 0.9764706  ... 0.98039216 0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.92941177 0.9411765  0.92156863 ... 0.92941177 0.9411765\n",
      "    0.92156863]\n",
      "   ...\n",
      "   [0.5019608  0.49019608 0.43137255 ... 0.5019608  0.49019608\n",
      "    0.43137255]\n",
      "   [0.78039217 0.80784315 0.8039216  ... 0.78039217 0.80784315\n",
      "    0.8039216 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]]\n",
      "\n",
      "\n",
      " [[[0.3647059  0.2509804  0.16078432 ... 0.3647059  0.2509804\n",
      "    0.16078432]\n",
      "   [0.39607844 0.28627452 0.19607843 ... 0.39607844 0.28627452\n",
      "    0.19607843]\n",
      "   [0.42745098 0.3254902  0.23921569 ... 0.42745098 0.3254902\n",
      "    0.23921569]\n",
      "   ...\n",
      "   [0.36078432 0.22352941 0.13725491 ... 0.36078432 0.22352941\n",
      "    0.13725491]\n",
      "   [0.34509805 0.20392157 0.11764706 ... 0.34509805 0.20392157\n",
      "    0.11764706]\n",
      "   [0.3254902  0.18431373 0.10196079 ... 0.3254902  0.18431373\n",
      "    0.10196079]]\n",
      "\n",
      "  [[0.39607844 0.28627452 0.19607843 ... 0.39607844 0.28627452\n",
      "    0.19607843]\n",
      "   [0.42745098 0.32156864 0.23529412 ... 0.42745098 0.32156864\n",
      "    0.23529412]\n",
      "   [0.45490196 0.36078432 0.27450982 ... 0.45490196 0.36078432\n",
      "    0.27450982]\n",
      "   ...\n",
      "   [0.38039216 0.24313726 0.15294118 ... 0.38039216 0.24313726\n",
      "    0.15294118]\n",
      "   [0.36078432 0.21960784 0.13333334 ... 0.36078432 0.21960784\n",
      "    0.13333334]\n",
      "   [0.34117648 0.2        0.11764706 ... 0.34117648 0.2\n",
      "    0.11764706]]\n",
      "\n",
      "  [[0.42352942 0.32156864 0.23137255 ... 0.42352942 0.32156864\n",
      "    0.23137255]\n",
      "   [0.4509804  0.35686275 0.27450982 ... 0.4509804  0.35686275\n",
      "    0.27450982]\n",
      "   [0.47843137 0.39215687 0.31764707 ... 0.47843137 0.39215687\n",
      "    0.31764707]\n",
      "   ...\n",
      "   [0.39607844 0.25882354 0.17254902 ... 0.39607844 0.25882354\n",
      "    0.17254902]\n",
      "   [0.3764706  0.23529412 0.14901961 ... 0.3764706  0.23529412\n",
      "    0.14901961]\n",
      "   [0.35686275 0.21176471 0.12941177 ... 0.35686275 0.21176471\n",
      "    0.12941177]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.34117648 0.34901962 0.3529412  ... 0.34117648 0.34901962\n",
      "    0.3529412 ]\n",
      "   [0.38431373 0.39215687 0.4        ... 0.38431373 0.39215687\n",
      "    0.4       ]\n",
      "   [0.4392157  0.4509804  0.4627451  ... 0.4392157  0.4509804\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.9490196  0.73333335 0.6392157  ... 0.9490196  0.73333335\n",
      "    0.6392157 ]\n",
      "   [0.9019608  0.6627451  0.5686275  ... 0.9019608  0.6627451\n",
      "    0.5686275 ]\n",
      "   [0.81960785 0.58431375 0.4862745  ... 0.81960785 0.58431375\n",
      "    0.4862745 ]]\n",
      "\n",
      "  [[0.31764707 0.3254902  0.3254902  ... 0.31764707 0.3254902\n",
      "    0.3254902 ]\n",
      "   [0.34117648 0.34901962 0.35686275 ... 0.34117648 0.34901962\n",
      "    0.35686275]\n",
      "   [0.41568628 0.42352942 0.43529412 ... 0.41568628 0.42352942\n",
      "    0.43529412]\n",
      "   ...\n",
      "   [0.92156863 0.7058824  0.60784316 ... 0.92156863 0.7058824\n",
      "    0.60784316]\n",
      "   [0.8784314  0.654902   0.5529412  ... 0.8784314  0.654902\n",
      "    0.5529412 ]\n",
      "   [0.8        0.56078434 0.4627451  ... 0.8        0.56078434\n",
      "    0.4627451 ]]\n",
      "\n",
      "  [[0.30588236 0.3137255  0.3137255  ... 0.30588236 0.3137255\n",
      "    0.3137255 ]\n",
      "   [0.30980393 0.3137255  0.32156864 ... 0.30980393 0.3137255\n",
      "    0.32156864]\n",
      "   [0.36862746 0.3764706  0.3882353  ... 0.36862746 0.3764706\n",
      "    0.3882353 ]\n",
      "   ...\n",
      "   [0.8980392  0.68235296 0.58431375 ... 0.8980392  0.68235296\n",
      "    0.58431375]\n",
      "   [0.85490197 0.627451   0.5294118  ... 0.85490197 0.627451\n",
      "    0.5294118 ]\n",
      "   [0.77254903 0.5372549  0.44313726 ... 0.77254903 0.5372549\n",
      "    0.44313726]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   ...\n",
      "   [0.8980392  0.9098039  0.9372549  ... 0.8980392  0.9098039\n",
      "    0.9372549 ]\n",
      "   [0.8901961  0.9019608  0.92941177 ... 0.8901961  0.9019608\n",
      "    0.92941177]\n",
      "   [0.8862745  0.8980392  0.9254902  ... 0.8862745  0.8980392\n",
      "    0.9254902 ]]\n",
      "\n",
      "  [[0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   ...\n",
      "   [0.8901961  0.9019608  0.92941177 ... 0.8901961  0.9019608\n",
      "    0.92941177]\n",
      "   [0.8862745  0.89411765 0.92156863 ... 0.8862745  0.89411765\n",
      "    0.92156863]\n",
      "   [0.8862745  0.8901961  0.92156863 ... 0.8862745  0.8901961\n",
      "    0.92156863]]\n",
      "\n",
      "  [[0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.9647059  ... 0.96862745 0.96862745\n",
      "    0.9647059 ]\n",
      "   [0.972549   0.972549   0.9647059  ... 0.972549   0.972549\n",
      "    0.9647059 ]\n",
      "   ...\n",
      "   [0.8862745  0.8901961  0.92156863 ... 0.8862745  0.8901961\n",
      "    0.92156863]\n",
      "   [0.8862745  0.8862745  0.91764706 ... 0.8862745  0.8862745\n",
      "    0.91764706]\n",
      "   [0.88235295 0.88235295 0.9137255  ... 0.88235295 0.88235295\n",
      "    0.9137255 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.827451   0.8117647  0.8352941  ... 0.827451   0.8117647\n",
      "    0.8352941 ]\n",
      "   [0.8352941  0.8156863  0.8352941  ... 0.8352941  0.8156863\n",
      "    0.8352941 ]\n",
      "   [0.8666667  0.8509804  0.8666667  ... 0.8666667  0.8509804\n",
      "    0.8666667 ]\n",
      "   ...\n",
      "   [0.7490196  0.74509805 0.76862746 ... 0.7490196  0.74509805\n",
      "    0.76862746]\n",
      "   [0.7647059  0.75686276 0.7921569  ... 0.7647059  0.75686276\n",
      "    0.7921569 ]\n",
      "   [0.78431374 0.77254903 0.81960785 ... 0.78431374 0.77254903\n",
      "    0.81960785]]\n",
      "\n",
      "  [[0.84705883 0.827451   0.8509804  ... 0.84705883 0.827451\n",
      "    0.8509804 ]\n",
      "   [0.8509804  0.83137256 0.84705883 ... 0.8509804  0.83137256\n",
      "    0.84705883]\n",
      "   [0.88235295 0.8627451  0.8784314  ... 0.88235295 0.8627451\n",
      "    0.8784314 ]\n",
      "   ...\n",
      "   [0.7372549  0.73333335 0.75686276 ... 0.7372549  0.73333335\n",
      "    0.75686276]\n",
      "   [0.75686276 0.7490196  0.7764706  ... 0.75686276 0.7490196\n",
      "    0.7764706 ]\n",
      "   [0.77254903 0.7647059  0.8039216  ... 0.77254903 0.7647059\n",
      "    0.8039216 ]]\n",
      "\n",
      "  [[0.8509804  0.8352941  0.85490197 ... 0.8509804  0.8352941\n",
      "    0.85490197]\n",
      "   [0.8666667  0.84705883 0.8627451  ... 0.8666667  0.84705883\n",
      "    0.8627451 ]\n",
      "   [0.8901961  0.87058824 0.8862745  ... 0.8901961  0.87058824\n",
      "    0.8862745 ]\n",
      "   ...\n",
      "   [0.7254902  0.7254902  0.7490196  ... 0.7254902  0.7254902\n",
      "    0.7490196 ]\n",
      "   [0.7411765  0.7372549  0.7647059  ... 0.7411765  0.7372549\n",
      "    0.7647059 ]\n",
      "   [0.7607843  0.7529412  0.78431374 ... 0.7607843  0.7529412\n",
      "    0.78431374]]]\n",
      "\n",
      "\n",
      " [[[0.04705882 0.04705882 0.04705882 ... 0.04705882 0.04705882\n",
      "    0.04705882]\n",
      "   [0.01176471 0.01176471 0.01176471 ... 0.01176471 0.01176471\n",
      "    0.01176471]\n",
      "   [0.03529412 0.03529412 0.03529412 ... 0.03529412 0.03529412\n",
      "    0.03529412]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.01176471 0.01176471 0.01176471 ... 0.01176471 0.01176471\n",
      "    0.01176471]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.99215686 0.99215686 0.99215686 ... 0.99215686 0.99215686\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6509804  0.6117647  0.60784316 ... 0.6509804  0.6117647\n",
      "    0.60784316]\n",
      "   [0.6431373  0.6        0.5921569  ... 0.6431373  0.6\n",
      "    0.5921569 ]\n",
      "   [0.63529414 0.5921569  0.5803922  ... 0.63529414 0.5921569\n",
      "    0.5803922 ]\n",
      "   ...\n",
      "   [0.80784315 0.69803923 0.67058825 ... 0.80784315 0.69803923\n",
      "    0.67058825]\n",
      "   [0.80784315 0.7176471  0.6901961  ... 0.80784315 0.7176471\n",
      "    0.6901961 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]]\n",
      "\n",
      "  [[0.64705884 0.59607846 0.59607846 ... 0.64705884 0.59607846\n",
      "    0.59607846]\n",
      "   [0.63529414 0.5921569  0.58431375 ... 0.63529414 0.5921569\n",
      "    0.58431375]\n",
      "   [0.62352943 0.5803922  0.57254905 ... 0.62352943 0.5803922\n",
      "    0.57254905]\n",
      "   ...\n",
      "   [0.8        0.69803923 0.6745098  ... 0.8        0.69803923\n",
      "    0.6745098 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]\n",
      "   [0.7921569  0.7019608  0.68235296 ... 0.7921569  0.7019608\n",
      "    0.68235296]]\n",
      "\n",
      "  [[0.6431373  0.59607846 0.5921569  ... 0.6431373  0.59607846\n",
      "    0.5921569 ]\n",
      "   [0.6313726  0.5882353  0.5803922  ... 0.6313726  0.5882353\n",
      "    0.5803922 ]\n",
      "   [0.627451   0.57254905 0.5686275  ... 0.627451   0.57254905\n",
      "    0.5686275 ]\n",
      "   ...\n",
      "   [0.7921569  0.6901961  0.6784314  ... 0.7921569  0.6901961\n",
      "    0.6784314 ]\n",
      "   [0.80784315 0.7058824  0.6901961  ... 0.80784315 0.7058824\n",
      "    0.6901961 ]\n",
      "   [0.80784315 0.7176471  0.7019608  ... 0.80784315 0.7176471\n",
      "    0.7019608 ]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (70, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (70, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 7s 1s/step - loss: 2.6387 - accuracy: 0.1000 - val_loss: 2.8907 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/100\n",
      "Learning rate:  1e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.6332 - accuracy: 0.0714 - val_loss: 2.8555 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/100\n",
      "Learning rate:  2e-05\n",
      "2/2 [==============================] - 2s 381ms/step - loss: 2.6240 - accuracy: 0.0714 - val_loss: 2.8362 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/100\n",
      "Learning rate:  3e-05\n",
      "2/2 [==============================] - 2s 472ms/step - loss: 2.6043 - accuracy: 0.0571 - val_loss: 2.8106 - val_accuracy: 0.0333\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/100\n",
      "Learning rate:  4e-05\n",
      "2/2 [==============================] - 3s 483ms/step - loss: 2.5887 - accuracy: 0.0857 - val_loss: 2.7829 - val_accuracy: 0.3333\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/100\n",
      "Learning rate:  5e-05\n",
      "2/2 [==============================] - 2s 384ms/step - loss: 2.5560 - accuracy: 0.1143 - val_loss: 2.7508 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/100\n",
      "Learning rate:  6e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.5115 - accuracy: 0.1714 - val_loss: 2.7170 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/100\n",
      "Learning rate:  7.000000000000001e-05\n",
      "2/2 [==============================] - 2s 417ms/step - loss: 2.4744 - accuracy: 0.2000 - val_loss: 2.6737 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/100\n",
      "Learning rate:  8e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.3827 - accuracy: 0.2429 - val_loss: 2.6336 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/100\n",
      "Learning rate:  8.999999999999999e-05\n",
      "2/2 [==============================] - 3s 496ms/step - loss: 2.3674 - accuracy: 0.3000 - val_loss: 2.5998 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/100\n",
      "Learning rate:  0.0001\n",
      "2/2 [==============================] - 2s 385ms/step - loss: 2.3315 - accuracy: 0.3143 - val_loss: 2.5637 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/100\n",
      "Learning rate:  0.00011\n",
      "2/2 [==============================] - 2s 413ms/step - loss: 2.2489 - accuracy: 0.3857 - val_loss: 2.5185 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/100\n",
      "Learning rate:  0.00012\n",
      "2/2 [==============================] - 2s 398ms/step - loss: 2.1934 - accuracy: 0.4000 - val_loss: 2.4773 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/100\n",
      "Learning rate:  0.00013000000000000002\n",
      "2/2 [==============================] - 3s 452ms/step - loss: 2.1102 - accuracy: 0.4857 - val_loss: 2.4328 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/100\n",
      "Learning rate:  0.00014000000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.0072 - accuracy: 0.6571 - val_loss: 2.3797 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/100\n",
      "Learning rate:  0.00015\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.9142 - accuracy: 0.7143 - val_loss: 2.3148 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/100\n",
      "Learning rate:  0.00016\n",
      "2/2 [==============================] - 3s 434ms/step - loss: 1.8963 - accuracy: 0.7143 - val_loss: 2.2422 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/100\n",
      "Learning rate:  0.00017\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.8105 - accuracy: 0.8000 - val_loss: 2.1653 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/100\n",
      "Learning rate:  0.00017999999999999998\n",
      "2/2 [==============================] - 2s 391ms/step - loss: 1.7460 - accuracy: 0.8429 - val_loss: 2.0933 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/100\n",
      "Learning rate:  0.00019\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.6528 - accuracy: 0.7857 - val_loss: 2.0191 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 21/100\n",
      "Learning rate:  0.0002\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.5698 - accuracy: 0.8143 - val_loss: 1.9396 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 22/100\n",
      "Learning rate:  0.00021\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.5014 - accuracy: 0.8143 - val_loss: 1.8631 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 23/100\n",
      "Learning rate:  0.00022\n",
      "2/2 [==============================] - 2s 398ms/step - loss: 1.4825 - accuracy: 0.8571 - val_loss: 1.7812 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 24/100\n",
      "Learning rate:  0.00023\n",
      "2/2 [==============================] - 2s 387ms/step - loss: 1.3950 - accuracy: 0.8000 - val_loss: 1.7009 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 25/100\n",
      "Learning rate:  0.00024\n",
      "2/2 [==============================] - 2s 377ms/step - loss: 1.3660 - accuracy: 0.8143 - val_loss: 1.6131 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 26/100\n",
      "Learning rate:  0.00025\n",
      "2/2 [==============================] - 2s 432ms/step - loss: 1.2810 - accuracy: 0.8714 - val_loss: 1.5239 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 27/100\n",
      "Learning rate:  0.00026000000000000003\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.2470 - accuracy: 0.8714 - val_loss: 1.4153 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 28/100\n",
      "Learning rate:  0.00027\n",
      "2/2 [==============================] - 2s 449ms/step - loss: 1.1890 - accuracy: 0.8857 - val_loss: 1.3218 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 29/100\n",
      "Learning rate:  0.00028000000000000003\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.1969 - accuracy: 0.8429 - val_loss: 1.2524 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 30/100\n",
      "Learning rate:  0.00029\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.1785 - accuracy: 0.8429 - val_loss: 1.1793 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 31/100\n",
      "Learning rate:  0.0003\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0707 - accuracy: 0.9000 - val_loss: 1.1080 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 32/100\n",
      "Learning rate:  0.00031\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0511 - accuracy: 0.8286 - val_loss: 1.0446 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 33/100\n",
      "Learning rate:  0.00032\n",
      "2/2 [==============================] - 2s 433ms/step - loss: 1.0672 - accuracy: 0.8714 - val_loss: 0.9807 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 34/100\n",
      "Learning rate:  0.00033\n",
      "2/2 [==============================] - 2s 435ms/step - loss: 0.9970 - accuracy: 0.9000 - val_loss: 0.9376 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 35/100\n",
      "Learning rate:  0.00034\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9753 - accuracy: 0.9000 - val_loss: 0.9031 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 36/100\n",
      "Learning rate:  0.00035\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9840 - accuracy: 0.8714 - val_loss: 0.8902 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 37/100\n",
      "Learning rate:  0.00035999999999999997\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9231 - accuracy: 0.9000 - val_loss: 0.8496 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 38/100\n",
      "Learning rate:  0.00037\n",
      "2/2 [==============================] - 2s 406ms/step - loss: 0.8814 - accuracy: 0.9286 - val_loss: 0.8080 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 39/100\n",
      "Learning rate:  0.00038\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8943 - accuracy: 0.9143 - val_loss: 0.7273 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 40/100\n",
      "Learning rate:  0.00039000000000000005\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8696 - accuracy: 0.9000 - val_loss: 0.6762 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 41/100\n",
      "Learning rate:  0.0004\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8597 - accuracy: 0.9286 - val_loss: 0.6533 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 42/100\n",
      "Learning rate:  0.00041\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8663 - accuracy: 0.8857 - val_loss: 0.6423 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 43/100\n",
      "Learning rate:  0.00042\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8401 - accuracy: 0.9429 - val_loss: 0.6488 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 44/100\n",
      "Learning rate:  0.00043\n",
      "2/2 [==============================] - 2s 450ms/step - loss: 0.8273 - accuracy: 0.9286 - val_loss: 0.6605 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 45/100\n",
      "Learning rate:  0.00044\n",
      "2/2 [==============================] - 2s 409ms/step - loss: 0.8280 - accuracy: 0.9286 - val_loss: 0.6481 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 46/100\n",
      "Learning rate:  0.00045000000000000004\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7885 - accuracy: 0.9286 - val_loss: 0.6298 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 47/100\n",
      "Learning rate:  0.00046\n",
      "2/2 [==============================] - 2s 420ms/step - loss: 0.8005 - accuracy: 0.9286 - val_loss: 0.6187 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 48/100\n",
      "Learning rate:  0.00047\n",
      "2/2 [==============================] - 2s 391ms/step - loss: 0.7852 - accuracy: 0.9571 - val_loss: 0.6134 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 49/100\n",
      "Learning rate:  0.00048\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7193 - accuracy: 0.9857 - val_loss: 0.6121 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 50/100\n",
      "Learning rate:  0.00049\n",
      "2/2 [==============================] - 2s 395ms/step - loss: 0.8059 - accuracy: 0.9286 - val_loss: 0.6093 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 51/100\n",
      "Learning rate:  0.0005\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7764 - accuracy: 0.9714 - val_loss: 0.6040 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 52/100\n",
      "Learning rate:  0.00051\n",
      "2/2 [==============================] - 2s 376ms/step - loss: 0.7821 - accuracy: 0.9286 - val_loss: 0.6032 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 53/100\n",
      "Learning rate:  0.0005200000000000001\n",
      "2/2 [==============================] - 2s 382ms/step - loss: 0.7619 - accuracy: 0.9429 - val_loss: 0.6038 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 54/100\n",
      "Learning rate:  0.0005300000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8383 - accuracy: 0.8857 - val_loss: 0.6047 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 55/100\n",
      "Learning rate:  0.00054\n",
      "2/2 [==============================] - 2s 421ms/step - loss: 0.7700 - accuracy: 0.9429 - val_loss: 0.6057 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 56/100\n",
      "Learning rate:  0.00055\n",
      "2/2 [==============================] - 2s 431ms/step - loss: 0.7615 - accuracy: 0.9571 - val_loss: 0.6119 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 57/100\n",
      "Learning rate:  0.0005600000000000001\n",
      "2/2 [==============================] - 2s 376ms/step - loss: 0.7522 - accuracy: 0.9571 - val_loss: 0.6900 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 58/100\n",
      "Learning rate:  0.00057\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7570 - accuracy: 0.9571 - val_loss: 2.6995 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 59/100\n",
      "Learning rate:  0.00058\n",
      "2/2 [==============================] - 2s 363ms/step - loss: 0.7747 - accuracy: 0.9286 - val_loss: 4.7554 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 60/100\n",
      "Learning rate:  0.00059\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8253 - accuracy: 0.9000 - val_loss: 1.4299 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 61/100\n",
      "Learning rate:  0.0006\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7124 - accuracy: 0.9714 - val_loss: 0.6376 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 62/100\n",
      "Learning rate:  0.00061\n",
      "2/2 [==============================] - 2s 386ms/step - loss: 0.7087 - accuracy: 0.9857 - val_loss: 0.6017 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 63/100\n",
      "Learning rate:  0.00062\n",
      "2/2 [==============================] - 2s 382ms/step - loss: 0.8241 - accuracy: 0.9286 - val_loss: 0.5963 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 64/100\n",
      "Learning rate:  0.00063\n",
      "2/2 [==============================] - 2s 372ms/step - loss: 0.7447 - accuracy: 0.9286 - val_loss: 0.5955 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 65/100\n",
      "Learning rate:  0.00064\n",
      "2/2 [==============================] - 2s 410ms/step - loss: 0.7342 - accuracy: 0.9714 - val_loss: 0.5950 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 66/100\n",
      "Learning rate:  0.0006500000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8001 - accuracy: 0.9143 - val_loss: 0.5945 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 67/100\n",
      "Learning rate:  0.00066\n",
      "2/2 [==============================] - 2s 405ms/step - loss: 0.8878 - accuracy: 0.8571 - val_loss: 0.5940 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 68/100\n",
      "Learning rate:  0.00067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 2s/step - loss: 0.7940 - accuracy: 0.9143 - val_loss: 0.5937 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 69/100\n",
      "Learning rate:  0.00068\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7568 - accuracy: 0.9286 - val_loss: 0.5952 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 70/100\n",
      "Learning rate:  0.00069\n",
      "2/2 [==============================] - 2s 389ms/step - loss: 0.7483 - accuracy: 0.9429 - val_loss: 0.6038 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 71/100\n",
      "Learning rate:  0.0007\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7182 - accuracy: 0.9714 - val_loss: 0.6298 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 72/100\n",
      "Learning rate:  0.00071\n",
      "2/2 [==============================] - 2s 391ms/step - loss: 0.7156 - accuracy: 0.9571 - val_loss: 0.6958 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 73/100\n",
      "Learning rate:  0.0007199999999999999\n",
      "2/2 [==============================] - 2s 386ms/step - loss: 0.7206 - accuracy: 0.9571 - val_loss: 0.8401 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 74/100\n",
      "Learning rate:  0.00073\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6876 - accuracy: 0.9857 - val_loss: 0.8664 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 75/100\n",
      "Learning rate:  0.00074\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6943 - accuracy: 0.9714 - val_loss: 0.8638 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 76/100\n",
      "Learning rate:  0.00075\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6810 - accuracy: 0.9714 - val_loss: 0.7313 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 77/100\n",
      "Learning rate:  0.00076\n",
      "2/2 [==============================] - 2s 409ms/step - loss: 0.6781 - accuracy: 0.9857 - val_loss: 0.6733 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 78/100\n",
      "Learning rate:  0.0007700000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6712 - accuracy: 0.9857 - val_loss: 0.6362 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 79/100\n",
      "Learning rate:  0.0007800000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6743 - accuracy: 0.9714 - val_loss: 0.6201 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 80/100\n",
      "Learning rate:  0.00079\n",
      "2/2 [==============================] - 2s 385ms/step - loss: 0.6691 - accuracy: 0.9857 - val_loss: 0.6255 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 81/100\n",
      "Learning rate:  0.0008\n",
      "2/2 [==============================] - 2s 460ms/step - loss: 0.6927 - accuracy: 0.9714 - val_loss: 0.6357 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 82/100\n",
      "Learning rate:  8.1e-05\n",
      "2/2 [==============================] - 2s 374ms/step - loss: 0.6452 - accuracy: 0.9857 - val_loss: 0.6392 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 83/100\n",
      "Learning rate:  8.2e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6476 - accuracy: 0.9857 - val_loss: 0.6414 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 84/100\n",
      "Learning rate:  8.3e-05\n",
      "2/2 [==============================] - 2s 378ms/step - loss: 0.6412 - accuracy: 1.0000 - val_loss: 0.6480 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 85/100\n",
      "Learning rate:  8.4e-05\n",
      "2/2 [==============================] - 2s 370ms/step - loss: 0.6950 - accuracy: 0.9286 - val_loss: 0.6634 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 86/100\n",
      "Learning rate:  8.5e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6746 - accuracy: 0.9714 - val_loss: 0.6702 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 87/100\n",
      "Learning rate:  8.6e-05\n",
      "2/2 [==============================] - 2s 362ms/step - loss: 0.7205 - accuracy: 0.9714 - val_loss: 0.6801 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 88/100\n",
      "Learning rate:  8.7e-05\n",
      "2/2 [==============================] - 2s 375ms/step - loss: 0.6363 - accuracy: 1.0000 - val_loss: 0.6938 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 89/100\n",
      "Learning rate:  8.800000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6438 - accuracy: 0.9857 - val_loss: 0.6928 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 90/100\n",
      "Learning rate:  8.900000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6574 - accuracy: 0.9857 - val_loss: 0.6950 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 91/100\n",
      "Learning rate:  9e-05\n",
      "2/2 [==============================] - 2s 395ms/step - loss: 0.6268 - accuracy: 1.0000 - val_loss: 0.6943 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 92/100\n",
      "Learning rate:  9.1e-05\n",
      "2/2 [==============================] - 2s 389ms/step - loss: 0.6622 - accuracy: 0.9714 - val_loss: 0.6874 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 93/100\n",
      "Learning rate:  9.200000000000001e-05\n",
      "2/2 [==============================] - 2s 382ms/step - loss: 0.6412 - accuracy: 0.9857 - val_loss: 0.6957 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 94/100\n",
      "Learning rate:  9.300000000000001e-05\n",
      "2/2 [==============================] - 2s 436ms/step - loss: 0.6501 - accuracy: 1.0000 - val_loss: 0.7078 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 95/100\n",
      "Learning rate:  9.4e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6679 - accuracy: 0.9714 - val_loss: 0.7215 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 96/100\n",
      "Learning rate:  9.5e-05\n",
      "2/2 [==============================] - 2s 405ms/step - loss: 0.6516 - accuracy: 0.9857 - val_loss: 0.7499 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 97/100\n",
      "Learning rate:  9.6e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6273 - accuracy: 1.0000 - val_loss: 0.7922 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 98/100\n",
      "Learning rate:  9.7e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6436 - accuracy: 1.0000 - val_loss: 0.8253 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 99/100\n",
      "Learning rate:  9.8e-05\n",
      "2/2 [==============================] - 2s 433ms/step - loss: 0.7451 - accuracy: 0.9571 - val_loss: 0.8446 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 100/100\n",
      "Learning rate:  9.900000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6339 - accuracy: 1.0000 - val_loss: 0.8524 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 1.1234 - accuracy: 0.8000\n",
      "Test loss: 1.1234409809112549\n",
      "Test accuracy: 0.800000011920929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision:  0.8\n",
      "Recall:  0.8\n",
      "F1 Score:  0.8\n",
      "33\n",
      "33\n",
      "[[[[243 243 243 ... 243 243 243]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  [[242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  [[242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[243 242 244 ... 243 242 244]\n",
      "   [245 242 248 ... 245 242 248]\n",
      "   [250 250 253 ... 250 250 253]\n",
      "   ...\n",
      "   [214 218 239 ... 214 218 239]\n",
      "   [205 209 233 ... 205 209 233]\n",
      "   [213 215 234 ... 213 215 234]]\n",
      "\n",
      "  [[238 236 241 ... 238 236 241]\n",
      "   [235 230 238 ... 235 230 238]\n",
      "   [240 236 243 ... 240 236 243]\n",
      "   ...\n",
      "   [210 214 238 ... 210 214 238]\n",
      "   [202 206 232 ... 202 206 232]\n",
      "   [206 208 231 ... 206 208 231]]\n",
      "\n",
      "  [[226 219 227 ... 226 219 227]\n",
      "   [221 211 220 ... 221 211 220]\n",
      "   [219 210 222 ... 219 210 222]\n",
      "   ...\n",
      "   [207 212 237 ... 207 212 237]\n",
      "   [207 212 236 ... 207 212 236]\n",
      "   [207 210 233 ... 207 210 233]]]\n",
      "\n",
      "\n",
      " [[[107 107 104 ... 107 107 104]\n",
      "   [111 109 106 ... 111 109 106]\n",
      "   [105  94  85 ... 105  94  85]\n",
      "   ...\n",
      "   [ 94  76  54 ...  94  76  54]\n",
      "   [ 93  76  56 ...  93  76  56]\n",
      "   [ 85  67  47 ...  85  67  47]]\n",
      "\n",
      "  [[111 111 110 ... 111 111 110]\n",
      "   [112 110 107 ... 112 110 107]\n",
      "   [107  95  87 ... 107  95  87]\n",
      "   ...\n",
      "   [ 85  60  35 ...  85  60  35]\n",
      "   [ 94  77  57 ...  94  77  57]\n",
      "   [ 86  68  48 ...  86  68  48]]\n",
      "\n",
      "  [[114 115 115 ... 114 115 115]\n",
      "   [111 109 105 ... 111 109 105]\n",
      "   [108  97  88 ... 108  97  88]\n",
      "   ...\n",
      "   [ 75  43  19 ...  75  43  19]\n",
      "   [ 94  76  55 ...  94  76  55]\n",
      "   [ 87  69  49 ...  87  69  49]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[172 126  92 ... 172 126  92]\n",
      "   [186 133  98 ... 186 133  98]\n",
      "   [157 106  71 ... 157 106  71]\n",
      "   ...\n",
      "   [ 57  31  16 ...  57  31  16]\n",
      "   [ 56  30  16 ...  56  30  16]\n",
      "   [ 55  29  15 ...  55  29  15]]\n",
      "\n",
      "  [[173 126  94 ... 173 126  94]\n",
      "   [185 128  92 ... 185 128  92]\n",
      "   [147  96  65 ... 147  96  65]\n",
      "   ...\n",
      "   [ 54  30  15 ...  54  30  15]\n",
      "   [ 54  29  16 ...  54  29  16]\n",
      "   [ 53  28  14 ...  53  28  14]]\n",
      "\n",
      "  [[173 120  90 ... 173 120  90]\n",
      "   [187 128  93 ... 187 128  93]\n",
      "   [138  85  58 ... 138  85  58]\n",
      "   ...\n",
      "   [ 50  28  14 ...  50  28  14]\n",
      "   [ 52  28  14 ...  52  28  14]\n",
      "   [ 51  27  13 ...  51  27  13]]]\n",
      "\n",
      "\n",
      " [[[166 181 195 ... 166 181 195]\n",
      "   [170 183 194 ... 170 183 194]\n",
      "   [175 188 197 ... 175 188 197]\n",
      "   ...\n",
      "   [178 188 187 ... 178 188 187]\n",
      "   [177 186 185 ... 177 186 185]\n",
      "   [175 185 183 ... 175 185 183]]\n",
      "\n",
      "  [[163 181 196 ... 163 181 196]\n",
      "   [167 183 195 ... 167 183 195]\n",
      "   [170 184 193 ... 170 184 193]\n",
      "   ...\n",
      "   [173 181 176 ... 173 181 176]\n",
      "   [170 180 176 ... 170 180 176]\n",
      "   [172 179 176 ... 172 179 176]]\n",
      "\n",
      "  [[159 178 193 ... 159 178 193]\n",
      "   [165 182 196 ... 165 182 196]\n",
      "   [164 178 188 ... 164 178 188]\n",
      "   ...\n",
      "   [170 177 167 ... 170 177 167]\n",
      "   [160 170 164 ... 160 170 164]\n",
      "   [159 169 165 ... 159 169 165]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[148 100 114 ... 148 100 114]\n",
      "   [144  89  88 ... 144  89  88]\n",
      "   [112  62  51 ... 112  62  51]\n",
      "   ...\n",
      "   [159 103 102 ... 159 103 102]\n",
      "   [156 102  99 ... 156 102  99]\n",
      "   [150 102  97 ... 150 102  97]]\n",
      "\n",
      "  [[141  96 115 ... 141  96 115]\n",
      "   [145  90  93 ... 145  90  93]\n",
      "   [110  56  45 ... 110  56  45]\n",
      "   ...\n",
      "   [156 101 100 ... 156 101 100]\n",
      "   [153  99  96 ... 153  99  96]\n",
      "   [148 101  96 ... 148 101  96]]\n",
      "\n",
      "  [[117  71  89 ... 117  71  89]\n",
      "   [142  85  91 ... 142  85  91]\n",
      "   [108  52  42 ... 108  52  42]\n",
      "   ...\n",
      "   [155 102 102 ... 155 102 102]\n",
      "   [151  98  96 ... 151  98  96]\n",
      "   [146  98  95 ... 146  98  95]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (65, 40, 30, 9)\n",
      "65 train samples\n",
      "32 test samples\n",
      "y_train shape: (65, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_25\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_26 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_775 (Conv2D)             (None, 40, 30, 16)   1312        input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_700 (BatchN (None, 40, 30, 16)   64          conv2d_775[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_700 (Activation)     (None, 40, 30, 16)   0           batch_normalization_700[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_776 (Conv2D)             (None, 40, 30, 16)   272         activation_700[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_701 (BatchN (None, 40, 30, 16)   64          conv2d_776[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_701 (Activation)     (None, 40, 30, 16)   0           batch_normalization_701[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_777 (Conv2D)             (None, 40, 30, 16)   2320        activation_701[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_702 (BatchN (None, 40, 30, 16)   64          conv2d_777[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_702 (Activation)     (None, 40, 30, 16)   0           batch_normalization_702[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_779 (Conv2D)             (None, 40, 30, 64)   1088        activation_700[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_778 (Conv2D)             (None, 40, 30, 64)   1088        activation_702[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_225 (Add)                   (None, 40, 30, 64)   0           conv2d_779[0][0]                 \n",
      "                                                                 conv2d_778[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_703 (BatchN (None, 40, 30, 64)   256         add_225[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_703 (Activation)     (None, 40, 30, 64)   0           batch_normalization_703[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_780 (Conv2D)             (None, 40, 30, 16)   1040        activation_703[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_704 (BatchN (None, 40, 30, 16)   64          conv2d_780[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_704 (Activation)     (None, 40, 30, 16)   0           batch_normalization_704[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_781 (Conv2D)             (None, 40, 30, 16)   2320        activation_704[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_705 (BatchN (None, 40, 30, 16)   64          conv2d_781[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_705 (Activation)     (None, 40, 30, 16)   0           batch_normalization_705[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_782 (Conv2D)             (None, 40, 30, 64)   1088        activation_705[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_226 (Add)                   (None, 40, 30, 64)   0           add_225[0][0]                    \n",
      "                                                                 conv2d_782[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_706 (BatchN (None, 40, 30, 64)   256         add_226[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_706 (Activation)     (None, 40, 30, 64)   0           batch_normalization_706[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_783 (Conv2D)             (None, 40, 30, 16)   1040        activation_706[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_707 (BatchN (None, 40, 30, 16)   64          conv2d_783[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_707 (Activation)     (None, 40, 30, 16)   0           batch_normalization_707[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_784 (Conv2D)             (None, 40, 30, 16)   2320        activation_707[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_708 (BatchN (None, 40, 30, 16)   64          conv2d_784[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_708 (Activation)     (None, 40, 30, 16)   0           batch_normalization_708[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_785 (Conv2D)             (None, 40, 30, 64)   1088        activation_708[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_227 (Add)                   (None, 40, 30, 64)   0           add_226[0][0]                    \n",
      "                                                                 conv2d_785[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_709 (BatchN (None, 40, 30, 64)   256         add_227[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_709 (Activation)     (None, 40, 30, 64)   0           batch_normalization_709[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_786 (Conv2D)             (None, 20, 15, 64)   4160        activation_709[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_710 (BatchN (None, 20, 15, 64)   256         conv2d_786[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_710 (Activation)     (None, 20, 15, 64)   0           batch_normalization_710[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_787 (Conv2D)             (None, 20, 15, 64)   36928       activation_710[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_711 (BatchN (None, 20, 15, 64)   256         conv2d_787[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_711 (Activation)     (None, 20, 15, 64)   0           batch_normalization_711[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_789 (Conv2D)             (None, 20, 15, 128)  8320        add_227[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_788 (Conv2D)             (None, 20, 15, 128)  8320        activation_711[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_228 (Add)                   (None, 20, 15, 128)  0           conv2d_789[0][0]                 \n",
      "                                                                 conv2d_788[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_712 (BatchN (None, 20, 15, 128)  512         add_228[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_712 (Activation)     (None, 20, 15, 128)  0           batch_normalization_712[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_790 (Conv2D)             (None, 20, 15, 64)   8256        activation_712[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_713 (BatchN (None, 20, 15, 64)   256         conv2d_790[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_713 (Activation)     (None, 20, 15, 64)   0           batch_normalization_713[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_791 (Conv2D)             (None, 20, 15, 64)   36928       activation_713[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_714 (BatchN (None, 20, 15, 64)   256         conv2d_791[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_714 (Activation)     (None, 20, 15, 64)   0           batch_normalization_714[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_792 (Conv2D)             (None, 20, 15, 128)  8320        activation_714[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_229 (Add)                   (None, 20, 15, 128)  0           add_228[0][0]                    \n",
      "                                                                 conv2d_792[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_715 (BatchN (None, 20, 15, 128)  512         add_229[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_715 (Activation)     (None, 20, 15, 128)  0           batch_normalization_715[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_793 (Conv2D)             (None, 20, 15, 64)   8256        activation_715[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_716 (BatchN (None, 20, 15, 64)   256         conv2d_793[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_716 (Activation)     (None, 20, 15, 64)   0           batch_normalization_716[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_794 (Conv2D)             (None, 20, 15, 64)   36928       activation_716[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_717 (BatchN (None, 20, 15, 64)   256         conv2d_794[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_717 (Activation)     (None, 20, 15, 64)   0           batch_normalization_717[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_795 (Conv2D)             (None, 20, 15, 128)  8320        activation_717[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_230 (Add)                   (None, 20, 15, 128)  0           add_229[0][0]                    \n",
      "                                                                 conv2d_795[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_718 (BatchN (None, 20, 15, 128)  512         add_230[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_718 (Activation)     (None, 20, 15, 128)  0           batch_normalization_718[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_796 (Conv2D)             (None, 10, 8, 128)   16512       activation_718[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_719 (BatchN (None, 10, 8, 128)   512         conv2d_796[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_719 (Activation)     (None, 10, 8, 128)   0           batch_normalization_719[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_797 (Conv2D)             (None, 10, 8, 128)   147584      activation_719[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_720 (BatchN (None, 10, 8, 128)   512         conv2d_797[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_720 (Activation)     (None, 10, 8, 128)   0           batch_normalization_720[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_799 (Conv2D)             (None, 10, 8, 256)   33024       add_230[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_798 (Conv2D)             (None, 10, 8, 256)   33024       activation_720[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_231 (Add)                   (None, 10, 8, 256)   0           conv2d_799[0][0]                 \n",
      "                                                                 conv2d_798[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_721 (BatchN (None, 10, 8, 256)   1024        add_231[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_721 (Activation)     (None, 10, 8, 256)   0           batch_normalization_721[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_800 (Conv2D)             (None, 10, 8, 128)   32896       activation_721[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_722 (BatchN (None, 10, 8, 128)   512         conv2d_800[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_722 (Activation)     (None, 10, 8, 128)   0           batch_normalization_722[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_801 (Conv2D)             (None, 10, 8, 128)   147584      activation_722[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_723 (BatchN (None, 10, 8, 128)   512         conv2d_801[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_723 (Activation)     (None, 10, 8, 128)   0           batch_normalization_723[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_802 (Conv2D)             (None, 10, 8, 256)   33024       activation_723[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_232 (Add)                   (None, 10, 8, 256)   0           add_231[0][0]                    \n",
      "                                                                 conv2d_802[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_724 (BatchN (None, 10, 8, 256)   1024        add_232[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_724 (Activation)     (None, 10, 8, 256)   0           batch_normalization_724[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_803 (Conv2D)             (None, 10, 8, 128)   32896       activation_724[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_725 (BatchN (None, 10, 8, 128)   512         conv2d_803[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_725 (Activation)     (None, 10, 8, 128)   0           batch_normalization_725[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_804 (Conv2D)             (None, 10, 8, 128)   147584      activation_725[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_726 (BatchN (None, 10, 8, 128)   512         conv2d_804[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_726 (Activation)     (None, 10, 8, 128)   0           batch_normalization_726[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_805 (Conv2D)             (None, 10, 8, 256)   33024       activation_726[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_233 (Add)                   (None, 10, 8, 256)   0           add_232[0][0]                    \n",
      "                                                                 conv2d_805[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_727 (BatchN (None, 10, 8, 256)   1024        add_233[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_727 (Activation)     (None, 10, 8, 256)   0           batch_normalization_727[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_25 (AveragePo (None, 1, 1, 256)    0           activation_727[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 256)          0           average_pooling2d_25[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 10)           2570        flatten_25[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[0.9529412  0.9529412  0.9529412  ... 0.9529412  0.9529412\n",
      "    0.9529412 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  [[0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  [[0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.9529412  0.9490196  0.95686275 ... 0.9529412  0.9490196\n",
      "    0.95686275]\n",
      "   [0.9607843  0.9490196  0.972549   ... 0.9607843  0.9490196\n",
      "    0.972549  ]\n",
      "   [0.98039216 0.98039216 0.99215686 ... 0.98039216 0.98039216\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [0.8392157  0.85490197 0.9372549  ... 0.8392157  0.85490197\n",
      "    0.9372549 ]\n",
      "   [0.8039216  0.81960785 0.9137255  ... 0.8039216  0.81960785\n",
      "    0.9137255 ]\n",
      "   [0.8352941  0.84313726 0.91764706 ... 0.8352941  0.84313726\n",
      "    0.91764706]]\n",
      "\n",
      "  [[0.93333334 0.9254902  0.94509804 ... 0.93333334 0.9254902\n",
      "    0.94509804]\n",
      "   [0.92156863 0.9019608  0.93333334 ... 0.92156863 0.9019608\n",
      "    0.93333334]\n",
      "   [0.9411765  0.9254902  0.9529412  ... 0.9411765  0.9254902\n",
      "    0.9529412 ]\n",
      "   ...\n",
      "   [0.8235294  0.8392157  0.93333334 ... 0.8235294  0.8392157\n",
      "    0.93333334]\n",
      "   [0.7921569  0.80784315 0.9098039  ... 0.7921569  0.80784315\n",
      "    0.9098039 ]\n",
      "   [0.80784315 0.8156863  0.90588236 ... 0.80784315 0.8156863\n",
      "    0.90588236]]\n",
      "\n",
      "  [[0.8862745  0.85882354 0.8901961  ... 0.8862745  0.85882354\n",
      "    0.8901961 ]\n",
      "   [0.8666667  0.827451   0.8627451  ... 0.8666667  0.827451\n",
      "    0.8627451 ]\n",
      "   [0.85882354 0.8235294  0.87058824 ... 0.85882354 0.8235294\n",
      "    0.87058824]\n",
      "   ...\n",
      "   [0.8117647  0.83137256 0.92941177 ... 0.8117647  0.83137256\n",
      "    0.92941177]\n",
      "   [0.8117647  0.83137256 0.9254902  ... 0.8117647  0.83137256\n",
      "    0.9254902 ]\n",
      "   [0.8117647  0.8235294  0.9137255  ... 0.8117647  0.8235294\n",
      "    0.9137255 ]]]\n",
      "\n",
      "\n",
      " [[[0.41960785 0.41960785 0.40784314 ... 0.41960785 0.41960785\n",
      "    0.40784314]\n",
      "   [0.43529412 0.42745098 0.41568628 ... 0.43529412 0.42745098\n",
      "    0.41568628]\n",
      "   [0.4117647  0.36862746 0.33333334 ... 0.4117647  0.36862746\n",
      "    0.33333334]\n",
      "   ...\n",
      "   [0.36862746 0.29803923 0.21176471 ... 0.36862746 0.29803923\n",
      "    0.21176471]\n",
      "   [0.3647059  0.29803923 0.21960784 ... 0.3647059  0.29803923\n",
      "    0.21960784]\n",
      "   [0.33333334 0.2627451  0.18431373 ... 0.33333334 0.2627451\n",
      "    0.18431373]]\n",
      "\n",
      "  [[0.43529412 0.43529412 0.43137255 ... 0.43529412 0.43529412\n",
      "    0.43137255]\n",
      "   [0.4392157  0.43137255 0.41960785 ... 0.4392157  0.43137255\n",
      "    0.41960785]\n",
      "   [0.41960785 0.37254903 0.34117648 ... 0.41960785 0.37254903\n",
      "    0.34117648]\n",
      "   ...\n",
      "   [0.33333334 0.23529412 0.13725491 ... 0.33333334 0.23529412\n",
      "    0.13725491]\n",
      "   [0.36862746 0.3019608  0.22352941 ... 0.36862746 0.3019608\n",
      "    0.22352941]\n",
      "   [0.3372549  0.26666668 0.1882353  ... 0.3372549  0.26666668\n",
      "    0.1882353 ]]\n",
      "\n",
      "  [[0.44705883 0.4509804  0.4509804  ... 0.44705883 0.4509804\n",
      "    0.4509804 ]\n",
      "   [0.43529412 0.42745098 0.4117647  ... 0.43529412 0.42745098\n",
      "    0.4117647 ]\n",
      "   [0.42352942 0.38039216 0.34509805 ... 0.42352942 0.38039216\n",
      "    0.34509805]\n",
      "   ...\n",
      "   [0.29411766 0.16862746 0.07450981 ... 0.29411766 0.16862746\n",
      "    0.07450981]\n",
      "   [0.36862746 0.29803923 0.21568628 ... 0.36862746 0.29803923\n",
      "    0.21568628]\n",
      "   [0.34117648 0.27058825 0.19215687 ... 0.34117648 0.27058825\n",
      "    0.19215687]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6745098  0.49411765 0.36078432 ... 0.6745098  0.49411765\n",
      "    0.36078432]\n",
      "   [0.7294118  0.52156866 0.38431373 ... 0.7294118  0.52156866\n",
      "    0.38431373]\n",
      "   [0.6156863  0.41568628 0.2784314  ... 0.6156863  0.41568628\n",
      "    0.2784314 ]\n",
      "   ...\n",
      "   [0.22352941 0.12156863 0.0627451  ... 0.22352941 0.12156863\n",
      "    0.0627451 ]\n",
      "   [0.21960784 0.11764706 0.0627451  ... 0.21960784 0.11764706\n",
      "    0.0627451 ]\n",
      "   [0.21568628 0.11372549 0.05882353 ... 0.21568628 0.11372549\n",
      "    0.05882353]]\n",
      "\n",
      "  [[0.6784314  0.49411765 0.36862746 ... 0.6784314  0.49411765\n",
      "    0.36862746]\n",
      "   [0.7254902  0.5019608  0.36078432 ... 0.7254902  0.5019608\n",
      "    0.36078432]\n",
      "   [0.5764706  0.3764706  0.25490198 ... 0.5764706  0.3764706\n",
      "    0.25490198]\n",
      "   ...\n",
      "   [0.21176471 0.11764706 0.05882353 ... 0.21176471 0.11764706\n",
      "    0.05882353]\n",
      "   [0.21176471 0.11372549 0.0627451  ... 0.21176471 0.11372549\n",
      "    0.0627451 ]\n",
      "   [0.20784314 0.10980392 0.05490196 ... 0.20784314 0.10980392\n",
      "    0.05490196]]\n",
      "\n",
      "  [[0.6784314  0.47058824 0.3529412  ... 0.6784314  0.47058824\n",
      "    0.3529412 ]\n",
      "   [0.73333335 0.5019608  0.3647059  ... 0.73333335 0.5019608\n",
      "    0.3647059 ]\n",
      "   [0.5411765  0.33333334 0.22745098 ... 0.5411765  0.33333334\n",
      "    0.22745098]\n",
      "   ...\n",
      "   [0.19607843 0.10980392 0.05490196 ... 0.19607843 0.10980392\n",
      "    0.05490196]\n",
      "   [0.20392157 0.10980392 0.05490196 ... 0.20392157 0.10980392\n",
      "    0.05490196]\n",
      "   [0.2        0.10588235 0.05098039 ... 0.2        0.10588235\n",
      "    0.05098039]]]\n",
      "\n",
      "\n",
      " [[[0.6509804  0.70980394 0.7647059  ... 0.6509804  0.70980394\n",
      "    0.7647059 ]\n",
      "   [0.6666667  0.7176471  0.7607843  ... 0.6666667  0.7176471\n",
      "    0.7607843 ]\n",
      "   [0.6862745  0.7372549  0.77254903 ... 0.6862745  0.7372549\n",
      "    0.77254903]\n",
      "   ...\n",
      "   [0.69803923 0.7372549  0.73333335 ... 0.69803923 0.7372549\n",
      "    0.73333335]\n",
      "   [0.69411767 0.7294118  0.7254902  ... 0.69411767 0.7294118\n",
      "    0.7254902 ]\n",
      "   [0.6862745  0.7254902  0.7176471  ... 0.6862745  0.7254902\n",
      "    0.7176471 ]]\n",
      "\n",
      "  [[0.6392157  0.70980394 0.76862746 ... 0.6392157  0.70980394\n",
      "    0.76862746]\n",
      "   [0.654902   0.7176471  0.7647059  ... 0.654902   0.7176471\n",
      "    0.7647059 ]\n",
      "   [0.6666667  0.72156864 0.75686276 ... 0.6666667  0.72156864\n",
      "    0.75686276]\n",
      "   ...\n",
      "   [0.6784314  0.70980394 0.6901961  ... 0.6784314  0.70980394\n",
      "    0.6901961 ]\n",
      "   [0.6666667  0.7058824  0.6901961  ... 0.6666667  0.7058824\n",
      "    0.6901961 ]\n",
      "   [0.6745098  0.7019608  0.6901961  ... 0.6745098  0.7019608\n",
      "    0.6901961 ]]\n",
      "\n",
      "  [[0.62352943 0.69803923 0.75686276 ... 0.62352943 0.69803923\n",
      "    0.75686276]\n",
      "   [0.64705884 0.7137255  0.76862746 ... 0.64705884 0.7137255\n",
      "    0.76862746]\n",
      "   [0.6431373  0.69803923 0.7372549  ... 0.6431373  0.69803923\n",
      "    0.7372549 ]\n",
      "   ...\n",
      "   [0.6666667  0.69411767 0.654902   ... 0.6666667  0.69411767\n",
      "    0.654902  ]\n",
      "   [0.627451   0.6666667  0.6431373  ... 0.627451   0.6666667\n",
      "    0.6431373 ]\n",
      "   [0.62352943 0.6627451  0.64705884 ... 0.62352943 0.6627451\n",
      "    0.64705884]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.5803922  0.39215687 0.44705883 ... 0.5803922  0.39215687\n",
      "    0.44705883]\n",
      "   [0.5647059  0.34901962 0.34509805 ... 0.5647059  0.34901962\n",
      "    0.34509805]\n",
      "   [0.4392157  0.24313726 0.2        ... 0.4392157  0.24313726\n",
      "    0.2       ]\n",
      "   ...\n",
      "   [0.62352943 0.40392157 0.4        ... 0.62352943 0.40392157\n",
      "    0.4       ]\n",
      "   [0.6117647  0.4        0.3882353  ... 0.6117647  0.4\n",
      "    0.3882353 ]\n",
      "   [0.5882353  0.4        0.38039216 ... 0.5882353  0.4\n",
      "    0.38039216]]\n",
      "\n",
      "  [[0.5529412  0.3764706  0.4509804  ... 0.5529412  0.3764706\n",
      "    0.4509804 ]\n",
      "   [0.5686275  0.3529412  0.3647059  ... 0.5686275  0.3529412\n",
      "    0.3647059 ]\n",
      "   [0.43137255 0.21960784 0.1764706  ... 0.43137255 0.21960784\n",
      "    0.1764706 ]\n",
      "   ...\n",
      "   [0.6117647  0.39607844 0.39215687 ... 0.6117647  0.39607844\n",
      "    0.39215687]\n",
      "   [0.6        0.3882353  0.3764706  ... 0.6        0.3882353\n",
      "    0.3764706 ]\n",
      "   [0.5803922  0.39607844 0.3764706  ... 0.5803922  0.39607844\n",
      "    0.3764706 ]]\n",
      "\n",
      "  [[0.45882353 0.2784314  0.34901962 ... 0.45882353 0.2784314\n",
      "    0.34901962]\n",
      "   [0.5568628  0.33333334 0.35686275 ... 0.5568628  0.33333334\n",
      "    0.35686275]\n",
      "   [0.42352942 0.20392157 0.16470589 ... 0.42352942 0.20392157\n",
      "    0.16470589]\n",
      "   ...\n",
      "   [0.60784316 0.4        0.4        ... 0.60784316 0.4\n",
      "    0.4       ]\n",
      "   [0.5921569  0.38431373 0.3764706  ... 0.5921569  0.38431373\n",
      "    0.3764706 ]\n",
      "   [0.57254905 0.38431373 0.37254903 ... 0.57254905 0.38431373\n",
      "    0.37254903]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   ...\n",
      "   [0.8980392  0.9098039  0.9372549  ... 0.8980392  0.9098039\n",
      "    0.9372549 ]\n",
      "   [0.8901961  0.9019608  0.92941177 ... 0.8901961  0.9019608\n",
      "    0.92941177]\n",
      "   [0.8862745  0.8980392  0.9254902  ... 0.8862745  0.8980392\n",
      "    0.9254902 ]]\n",
      "\n",
      "  [[0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   ...\n",
      "   [0.8901961  0.9019608  0.92941177 ... 0.8901961  0.9019608\n",
      "    0.92941177]\n",
      "   [0.8862745  0.89411765 0.92156863 ... 0.8862745  0.89411765\n",
      "    0.92156863]\n",
      "   [0.8862745  0.8901961  0.92156863 ... 0.8862745  0.8901961\n",
      "    0.92156863]]\n",
      "\n",
      "  [[0.96862745 0.96862745 0.96862745 ... 0.96862745 0.96862745\n",
      "    0.96862745]\n",
      "   [0.96862745 0.96862745 0.9647059  ... 0.96862745 0.96862745\n",
      "    0.9647059 ]\n",
      "   [0.972549   0.972549   0.9647059  ... 0.972549   0.972549\n",
      "    0.9647059 ]\n",
      "   ...\n",
      "   [0.8862745  0.8901961  0.92156863 ... 0.8862745  0.8901961\n",
      "    0.92156863]\n",
      "   [0.8862745  0.8862745  0.91764706 ... 0.8862745  0.8862745\n",
      "    0.91764706]\n",
      "   [0.88235295 0.88235295 0.9137255  ... 0.88235295 0.88235295\n",
      "    0.9137255 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.827451   0.8117647  0.8352941  ... 0.827451   0.8117647\n",
      "    0.8352941 ]\n",
      "   [0.8352941  0.8156863  0.8352941  ... 0.8352941  0.8156863\n",
      "    0.8352941 ]\n",
      "   [0.8666667  0.8509804  0.8666667  ... 0.8666667  0.8509804\n",
      "    0.8666667 ]\n",
      "   ...\n",
      "   [0.7490196  0.74509805 0.76862746 ... 0.7490196  0.74509805\n",
      "    0.76862746]\n",
      "   [0.7647059  0.75686276 0.7921569  ... 0.7647059  0.75686276\n",
      "    0.7921569 ]\n",
      "   [0.78431374 0.77254903 0.81960785 ... 0.78431374 0.77254903\n",
      "    0.81960785]]\n",
      "\n",
      "  [[0.84705883 0.827451   0.8509804  ... 0.84705883 0.827451\n",
      "    0.8509804 ]\n",
      "   [0.8509804  0.83137256 0.84705883 ... 0.8509804  0.83137256\n",
      "    0.84705883]\n",
      "   [0.88235295 0.8627451  0.8784314  ... 0.88235295 0.8627451\n",
      "    0.8784314 ]\n",
      "   ...\n",
      "   [0.7372549  0.73333335 0.75686276 ... 0.7372549  0.73333335\n",
      "    0.75686276]\n",
      "   [0.75686276 0.7490196  0.7764706  ... 0.75686276 0.7490196\n",
      "    0.7764706 ]\n",
      "   [0.77254903 0.7647059  0.8039216  ... 0.77254903 0.7647059\n",
      "    0.8039216 ]]\n",
      "\n",
      "  [[0.8509804  0.8352941  0.85490197 ... 0.8509804  0.8352941\n",
      "    0.85490197]\n",
      "   [0.8666667  0.84705883 0.8627451  ... 0.8666667  0.84705883\n",
      "    0.8627451 ]\n",
      "   [0.8901961  0.87058824 0.8862745  ... 0.8901961  0.87058824\n",
      "    0.8862745 ]\n",
      "   ...\n",
      "   [0.7254902  0.7254902  0.7490196  ... 0.7254902  0.7254902\n",
      "    0.7490196 ]\n",
      "   [0.7411765  0.7372549  0.7647059  ... 0.7411765  0.7372549\n",
      "    0.7647059 ]\n",
      "   [0.7607843  0.7529412  0.78431374 ... 0.7607843  0.7529412\n",
      "    0.78431374]]]\n",
      "\n",
      "\n",
      " [[[0.04705882 0.04705882 0.04705882 ... 0.04705882 0.04705882\n",
      "    0.04705882]\n",
      "   [0.01176471 0.01176471 0.01176471 ... 0.01176471 0.01176471\n",
      "    0.01176471]\n",
      "   [0.03529412 0.03529412 0.03529412 ... 0.03529412 0.03529412\n",
      "    0.03529412]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.01176471 0.01176471 0.01176471 ... 0.01176471 0.01176471\n",
      "    0.01176471]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.65882355 0.5411765  0.34901962 ... 0.65882355 0.5411765\n",
      "    0.34901962]\n",
      "   [0.79607844 0.6862745  0.44313726 ... 0.79607844 0.6862745\n",
      "    0.44313726]\n",
      "   [0.84705883 0.7372549  0.4862745  ... 0.84705883 0.7372549\n",
      "    0.4862745 ]\n",
      "   ...\n",
      "   [0.9019608  0.78431374 0.57254905 ... 0.9019608  0.78431374\n",
      "    0.57254905]\n",
      "   [0.8784314  0.77254903 0.5803922  ... 0.8784314  0.77254903\n",
      "    0.5803922 ]\n",
      "   [0.8666667  0.7607843  0.58431375 ... 0.8666667  0.7607843\n",
      "    0.58431375]]\n",
      "\n",
      "  [[0.6313726  0.5058824  0.27450982 ... 0.6313726  0.5058824\n",
      "    0.27450982]\n",
      "   [0.7372549  0.61960787 0.3764706  ... 0.7372549  0.61960787\n",
      "    0.3764706 ]\n",
      "   [0.83137256 0.72156864 0.4627451  ... 0.83137256 0.72156864\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.8784314  0.7607843  0.5568628  ... 0.8784314  0.7607843\n",
      "    0.5568628 ]\n",
      "   [0.85882354 0.7490196  0.56078434 ... 0.85882354 0.7490196\n",
      "    0.56078434]\n",
      "   [0.85490197 0.7529412  0.5647059  ... 0.85490197 0.7529412\n",
      "    0.5647059 ]]\n",
      "\n",
      "  [[0.6        0.46666667 0.22352941 ... 0.6        0.46666667\n",
      "    0.22352941]\n",
      "   [0.6313726  0.49803922 0.27058825 ... 0.6313726  0.49803922\n",
      "    0.27058825]\n",
      "   [0.80784315 0.69803923 0.44705883 ... 0.80784315 0.69803923\n",
      "    0.44705883]\n",
      "   ...\n",
      "   [0.8352941  0.72156864 0.5372549  ... 0.8352941  0.72156864\n",
      "    0.5372549 ]\n",
      "   [0.83137256 0.7254902  0.5411765  ... 0.83137256 0.7254902\n",
      "    0.5411765 ]\n",
      "   [0.8352941  0.73333335 0.5372549  ... 0.8352941  0.73333335\n",
      "    0.5372549 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6039216  0.3764706  0.27058825 ... 0.6039216  0.3764706\n",
      "    0.27058825]\n",
      "   [0.627451   0.40784314 0.31764707 ... 0.627451   0.40784314\n",
      "    0.31764707]\n",
      "   [0.67058825 0.47058824 0.4        ... 0.67058825 0.47058824\n",
      "    0.4       ]\n",
      "   ...\n",
      "   [0.76862746 0.58431375 0.4745098  ... 0.76862746 0.58431375\n",
      "    0.4745098 ]\n",
      "   [0.7058824  0.49803922 0.38431373 ... 0.7058824  0.49803922\n",
      "    0.38431373]\n",
      "   [0.65882355 0.44313726 0.32156864 ... 0.65882355 0.44313726\n",
      "    0.32156864]]\n",
      "\n",
      "  [[0.6        0.38039216 0.26666668 ... 0.6        0.38039216\n",
      "    0.26666668]\n",
      "   [0.63529414 0.41568628 0.32156864 ... 0.63529414 0.41568628\n",
      "    0.32156864]\n",
      "   [0.7372549  0.56078434 0.4627451  ... 0.7372549  0.56078434\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.83137256 0.654902   0.54901963 ... 0.83137256 0.654902\n",
      "    0.54901963]\n",
      "   [0.77254903 0.5647059  0.44313726 ... 0.77254903 0.5647059\n",
      "    0.44313726]\n",
      "   [0.7176471  0.5019608  0.38039216 ... 0.7176471  0.5019608\n",
      "    0.38039216]]\n",
      "\n",
      "  [[0.627451   0.41960785 0.29411766 ... 0.627451   0.41960785\n",
      "    0.29411766]\n",
      "   [0.68235296 0.46666667 0.3882353  ... 0.68235296 0.46666667\n",
      "    0.3882353 ]\n",
      "   [0.8509804  0.7294118  0.49019608 ... 0.8509804  0.7294118\n",
      "    0.49019608]\n",
      "   ...\n",
      "   [0.89411765 0.72156864 0.6117647  ... 0.89411765 0.72156864\n",
      "    0.6117647 ]\n",
      "   [0.8392157  0.627451   0.50980395 ... 0.8392157  0.627451\n",
      "    0.50980395]\n",
      "   [0.76862746 0.56078434 0.43529412 ... 0.76862746 0.56078434\n",
      "    0.43529412]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (65, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (65, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 3.3058 - accuracy: 0.0000e+0028\n",
      "28\n",
      "[[[[ 30  95 105 ...  30  95 105]\n",
      "   [ 31  97 107 ...  31  97 107]\n",
      "   [ 30  96 106 ...  30  96 106]\n",
      "   ...\n",
      "   [ 26  91 103 ...  26  91 103]\n",
      "   [ 25  92 102 ...  25  92 102]\n",
      "   [ 24  91 101 ...  24  91 101]]\n",
      "\n",
      "  [[ 27 111 122 ...  27 111 122]\n",
      "   [ 27 111 123 ...  27 111 123]\n",
      "   [ 28 111 123 ...  28 111 123]\n",
      "   ...\n",
      "   [ 22 107 119 ...  22 107 119]\n",
      "   [ 22 108 118 ...  22 108 118]\n",
      "   [ 23 108 119 ...  23 108 119]]\n",
      "\n",
      "  [[ 24 105 116 ...  24 105 116]\n",
      "   [ 25 107 118 ...  25 107 118]\n",
      "   [ 28 108 119 ...  28 108 119]\n",
      "   ...\n",
      "   [ 26 107 117 ...  26 107 117]\n",
      "   [ 23 105 115 ...  23 105 115]\n",
      "   [ 22 104 116 ...  22 104 116]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  7  29  34 ...   7  29  34]\n",
      "   [ 87  67  56 ...  87  67  56]\n",
      "   [179 128 119 ... 179 128 119]\n",
      "   ...\n",
      "   [195 132 133 ... 195 132 133]\n",
      "   [119  77  72 ... 119  77  72]\n",
      "   [ 50  35  35 ...  50  35  35]]\n",
      "\n",
      "  [[  1  41  49 ...   1  41  49]\n",
      "   [ 72  61  52 ...  72  61  52]\n",
      "   [169 118 107 ... 169 118 107]\n",
      "   ...\n",
      "   [186 131 126 ... 186 131 126]\n",
      "   [ 97  65  58 ...  97  65  58]\n",
      "   [ 30  27  31 ...  30  27  31]]\n",
      "\n",
      "  [[  5  56  65 ...   5  56  65]\n",
      "   [ 63  60  53 ...  63  60  53]\n",
      "   [161 113 103 ... 161 113 103]\n",
      "   ...\n",
      "   [176 123 117 ... 176 123 117]\n",
      "   [ 76  58  54 ...  76  58  54]\n",
      "   [ 19  35  42 ...  19  35  42]]]\n",
      "\n",
      "\n",
      " [[[  9   9   2 ...   9   9   2]\n",
      "   [  6   6   2 ...   6   6   2]\n",
      "   [  4   3   1 ...   4   3   1]\n",
      "   ...\n",
      "   [  0   1   1 ...   0   1   1]\n",
      "   [  1   2   2 ...   1   2   2]\n",
      "   [  9  10   7 ...   9  10   7]]\n",
      "\n",
      "  [[  9   8   4 ...   9   8   4]\n",
      "   [  9   9   4 ...   9   9   4]\n",
      "   [  9   8   3 ...   9   8   3]\n",
      "   ...\n",
      "   [  1   1   1 ...   1   1   1]\n",
      "   [  4   4   3 ...   4   4   3]\n",
      "   [  4   5   3 ...   4   5   3]]\n",
      "\n",
      "  [[ 26  23  14 ...  26  23  14]\n",
      "   [ 33  27  15 ...  33  27  15]\n",
      "   [ 45  37  24 ...  45  37  24]\n",
      "   ...\n",
      "   [  2   3   2 ...   2   3   2]\n",
      "   [  2   2   2 ...   2   2   2]\n",
      "   [  4   4   3 ...   4   4   3]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[188 178 161 ... 188 178 161]\n",
      "   [190 180 162 ... 190 180 162]\n",
      "   [187 177 158 ... 187 177 158]\n",
      "   ...\n",
      "   [150 142 124 ... 150 142 124]\n",
      "   [145 135 118 ... 145 135 118]\n",
      "   [145 135 117 ... 145 135 117]]\n",
      "\n",
      "  [[197 186 170 ... 197 186 170]\n",
      "   [194 184 168 ... 194 184 168]\n",
      "   [192 181 164 ... 192 181 164]\n",
      "   ...\n",
      "   [148 139 120 ... 148 139 120]\n",
      "   [149 138 119 ... 149 138 119]\n",
      "   [147 136 117 ... 147 136 117]]\n",
      "\n",
      "  [[201 191 174 ... 201 191 174]\n",
      "   [198 188 172 ... 198 188 172]\n",
      "   [194 183 167 ... 194 183 167]\n",
      "   ...\n",
      "   [150 139 120 ... 150 139 120]\n",
      "   [152 141 121 ... 152 141 121]\n",
      "   [152 142 124 ... 152 142 124]]]\n",
      "\n",
      "\n",
      " [[[255 255 255 ... 255 255 255]\n",
      "   [243 246 242 ... 243 246 242]\n",
      "   [214 220 205 ... 214 220 205]\n",
      "   ...\n",
      "   [106 120  98 ... 106 120  98]\n",
      "   [210 215 205 ... 210 215 205]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [246 248 245 ... 246 248 245]\n",
      "   [217 226 213 ... 217 226 213]\n",
      "   ...\n",
      "   [108 123  99 ... 108 123  99]\n",
      "   [211 216 206 ... 211 216 206]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [248 251 248 ... 248 251 248]\n",
      "   [221 232 221 ... 221 232 221]\n",
      "   ...\n",
      "   [110 125 100 ... 110 125 100]\n",
      "   [212 217 206 ... 212 217 206]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [249 250 249 ... 249 250 249]\n",
      "   [232 229 221 ... 232 229 221]\n",
      "   ...\n",
      "   [ 86 101  95 ...  86 101  95]\n",
      "   [204 210 208 ... 204 210 208]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [250 250 249 ... 250 250 249]\n",
      "   [234 236 231 ... 234 236 231]\n",
      "   ...\n",
      "   [102 111 101 ... 102 111 101]\n",
      "   [202 208 207 ... 202 208 207]\n",
      "   [255 255 255 ... 255 255 255]]\n",
      "\n",
      "  [[255 255 255 ... 255 255 255]\n",
      "   [250 250 249 ... 250 250 249]\n",
      "   [237 240 235 ... 237 240 235]\n",
      "   ...\n",
      "   [128 125 110 ... 128 125 110]\n",
      "   [199 206 205 ... 199 206 205]\n",
      "   [255 255 255 ... 255 255 255]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (74, 40, 30, 9)\n",
      "74 train samples\n",
      "28 test samples\n",
      "y_train shape: (74, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_806 (Conv2D)             (None, 40, 30, 16)   1312        input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_728 (BatchN (None, 40, 30, 16)   64          conv2d_806[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_728 (Activation)     (None, 40, 30, 16)   0           batch_normalization_728[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_807 (Conv2D)             (None, 40, 30, 16)   272         activation_728[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_729 (BatchN (None, 40, 30, 16)   64          conv2d_807[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_729 (Activation)     (None, 40, 30, 16)   0           batch_normalization_729[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_808 (Conv2D)             (None, 40, 30, 16)   2320        activation_729[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_730 (BatchN (None, 40, 30, 16)   64          conv2d_808[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_730 (Activation)     (None, 40, 30, 16)   0           batch_normalization_730[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_810 (Conv2D)             (None, 40, 30, 64)   1088        activation_728[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_809 (Conv2D)             (None, 40, 30, 64)   1088        activation_730[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_234 (Add)                   (None, 40, 30, 64)   0           conv2d_810[0][0]                 \n",
      "                                                                 conv2d_809[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_731 (BatchN (None, 40, 30, 64)   256         add_234[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_731 (Activation)     (None, 40, 30, 64)   0           batch_normalization_731[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_811 (Conv2D)             (None, 40, 30, 16)   1040        activation_731[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_732 (BatchN (None, 40, 30, 16)   64          conv2d_811[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_732 (Activation)     (None, 40, 30, 16)   0           batch_normalization_732[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_812 (Conv2D)             (None, 40, 30, 16)   2320        activation_732[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_733 (BatchN (None, 40, 30, 16)   64          conv2d_812[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_733 (Activation)     (None, 40, 30, 16)   0           batch_normalization_733[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_813 (Conv2D)             (None, 40, 30, 64)   1088        activation_733[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_235 (Add)                   (None, 40, 30, 64)   0           add_234[0][0]                    \n",
      "                                                                 conv2d_813[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_734 (BatchN (None, 40, 30, 64)   256         add_235[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_734 (Activation)     (None, 40, 30, 64)   0           batch_normalization_734[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_814 (Conv2D)             (None, 40, 30, 16)   1040        activation_734[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_735 (BatchN (None, 40, 30, 16)   64          conv2d_814[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_735 (Activation)     (None, 40, 30, 16)   0           batch_normalization_735[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_815 (Conv2D)             (None, 40, 30, 16)   2320        activation_735[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_736 (BatchN (None, 40, 30, 16)   64          conv2d_815[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_736 (Activation)     (None, 40, 30, 16)   0           batch_normalization_736[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_816 (Conv2D)             (None, 40, 30, 64)   1088        activation_736[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_236 (Add)                   (None, 40, 30, 64)   0           add_235[0][0]                    \n",
      "                                                                 conv2d_816[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_737 (BatchN (None, 40, 30, 64)   256         add_236[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_737 (Activation)     (None, 40, 30, 64)   0           batch_normalization_737[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_817 (Conv2D)             (None, 20, 15, 64)   4160        activation_737[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_738 (BatchN (None, 20, 15, 64)   256         conv2d_817[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_738 (Activation)     (None, 20, 15, 64)   0           batch_normalization_738[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_818 (Conv2D)             (None, 20, 15, 64)   36928       activation_738[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_739 (BatchN (None, 20, 15, 64)   256         conv2d_818[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_739 (Activation)     (None, 20, 15, 64)   0           batch_normalization_739[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_820 (Conv2D)             (None, 20, 15, 128)  8320        add_236[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_819 (Conv2D)             (None, 20, 15, 128)  8320        activation_739[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_237 (Add)                   (None, 20, 15, 128)  0           conv2d_820[0][0]                 \n",
      "                                                                 conv2d_819[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_740 (BatchN (None, 20, 15, 128)  512         add_237[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_740 (Activation)     (None, 20, 15, 128)  0           batch_normalization_740[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_821 (Conv2D)             (None, 20, 15, 64)   8256        activation_740[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_741 (BatchN (None, 20, 15, 64)   256         conv2d_821[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_741 (Activation)     (None, 20, 15, 64)   0           batch_normalization_741[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_822 (Conv2D)             (None, 20, 15, 64)   36928       activation_741[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_742 (BatchN (None, 20, 15, 64)   256         conv2d_822[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_742 (Activation)     (None, 20, 15, 64)   0           batch_normalization_742[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_823 (Conv2D)             (None, 20, 15, 128)  8320        activation_742[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_238 (Add)                   (None, 20, 15, 128)  0           add_237[0][0]                    \n",
      "                                                                 conv2d_823[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_743 (BatchN (None, 20, 15, 128)  512         add_238[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_743 (Activation)     (None, 20, 15, 128)  0           batch_normalization_743[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_824 (Conv2D)             (None, 20, 15, 64)   8256        activation_743[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_744 (BatchN (None, 20, 15, 64)   256         conv2d_824[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_744 (Activation)     (None, 20, 15, 64)   0           batch_normalization_744[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_825 (Conv2D)             (None, 20, 15, 64)   36928       activation_744[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_745 (BatchN (None, 20, 15, 64)   256         conv2d_825[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_745 (Activation)     (None, 20, 15, 64)   0           batch_normalization_745[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_826 (Conv2D)             (None, 20, 15, 128)  8320        activation_745[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_239 (Add)                   (None, 20, 15, 128)  0           add_238[0][0]                    \n",
      "                                                                 conv2d_826[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_746 (BatchN (None, 20, 15, 128)  512         add_239[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_746 (Activation)     (None, 20, 15, 128)  0           batch_normalization_746[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_827 (Conv2D)             (None, 10, 8, 128)   16512       activation_746[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_747 (BatchN (None, 10, 8, 128)   512         conv2d_827[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_747 (Activation)     (None, 10, 8, 128)   0           batch_normalization_747[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_828 (Conv2D)             (None, 10, 8, 128)   147584      activation_747[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_748 (BatchN (None, 10, 8, 128)   512         conv2d_828[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_748 (Activation)     (None, 10, 8, 128)   0           batch_normalization_748[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_830 (Conv2D)             (None, 10, 8, 256)   33024       add_239[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_829 (Conv2D)             (None, 10, 8, 256)   33024       activation_748[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_240 (Add)                   (None, 10, 8, 256)   0           conv2d_830[0][0]                 \n",
      "                                                                 conv2d_829[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_749 (BatchN (None, 10, 8, 256)   1024        add_240[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_749 (Activation)     (None, 10, 8, 256)   0           batch_normalization_749[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_831 (Conv2D)             (None, 10, 8, 128)   32896       activation_749[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_750 (BatchN (None, 10, 8, 128)   512         conv2d_831[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_750 (Activation)     (None, 10, 8, 128)   0           batch_normalization_750[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_832 (Conv2D)             (None, 10, 8, 128)   147584      activation_750[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_751 (BatchN (None, 10, 8, 128)   512         conv2d_832[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_751 (Activation)     (None, 10, 8, 128)   0           batch_normalization_751[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_833 (Conv2D)             (None, 10, 8, 256)   33024       activation_751[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_241 (Add)                   (None, 10, 8, 256)   0           add_240[0][0]                    \n",
      "                                                                 conv2d_833[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_752 (BatchN (None, 10, 8, 256)   1024        add_241[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_752 (Activation)     (None, 10, 8, 256)   0           batch_normalization_752[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_834 (Conv2D)             (None, 10, 8, 128)   32896       activation_752[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_753 (BatchN (None, 10, 8, 128)   512         conv2d_834[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_753 (Activation)     (None, 10, 8, 128)   0           batch_normalization_753[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_835 (Conv2D)             (None, 10, 8, 128)   147584      activation_753[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_754 (BatchN (None, 10, 8, 128)   512         conv2d_835[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_754 (Activation)     (None, 10, 8, 128)   0           batch_normalization_754[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_836 (Conv2D)             (None, 10, 8, 256)   33024       activation_754[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_242 (Add)                   (None, 10, 8, 256)   0           add_241[0][0]                    \n",
      "                                                                 conv2d_836[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_755 (BatchN (None, 10, 8, 256)   1024        add_242[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_755 (Activation)     (None, 10, 8, 256)   0           batch_normalization_755[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_26 (AveragePo (None, 1, 1, 256)    0           activation_755[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 256)          0           average_pooling2d_26[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 10)           2570        flatten_26[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[0.11764706 0.37254903 0.4117647  ... 0.11764706 0.37254903\n",
      "    0.4117647 ]\n",
      "   [0.12156863 0.38039216 0.41960785 ... 0.12156863 0.38039216\n",
      "    0.41960785]\n",
      "   [0.11764706 0.3764706  0.41568628 ... 0.11764706 0.3764706\n",
      "    0.41568628]\n",
      "   ...\n",
      "   [0.10196079 0.35686275 0.40392157 ... 0.10196079 0.35686275\n",
      "    0.40392157]\n",
      "   [0.09803922 0.36078432 0.4        ... 0.09803922 0.36078432\n",
      "    0.4       ]\n",
      "   [0.09411765 0.35686275 0.39607844 ... 0.09411765 0.35686275\n",
      "    0.39607844]]\n",
      "\n",
      "  [[0.10588235 0.43529412 0.47843137 ... 0.10588235 0.43529412\n",
      "    0.47843137]\n",
      "   [0.10588235 0.43529412 0.48235294 ... 0.10588235 0.43529412\n",
      "    0.48235294]\n",
      "   [0.10980392 0.43529412 0.48235294 ... 0.10980392 0.43529412\n",
      "    0.48235294]\n",
      "   ...\n",
      "   [0.08627451 0.41960785 0.46666667 ... 0.08627451 0.41960785\n",
      "    0.46666667]\n",
      "   [0.08627451 0.42352942 0.4627451  ... 0.08627451 0.42352942\n",
      "    0.4627451 ]\n",
      "   [0.09019608 0.42352942 0.46666667 ... 0.09019608 0.42352942\n",
      "    0.46666667]]\n",
      "\n",
      "  [[0.09411765 0.4117647  0.45490196 ... 0.09411765 0.4117647\n",
      "    0.45490196]\n",
      "   [0.09803922 0.41960785 0.4627451  ... 0.09803922 0.41960785\n",
      "    0.4627451 ]\n",
      "   [0.10980392 0.42352942 0.46666667 ... 0.10980392 0.42352942\n",
      "    0.46666667]\n",
      "   ...\n",
      "   [0.10196079 0.41960785 0.45882353 ... 0.10196079 0.41960785\n",
      "    0.45882353]\n",
      "   [0.09019608 0.4117647  0.4509804  ... 0.09019608 0.4117647\n",
      "    0.4509804 ]\n",
      "   [0.08627451 0.40784314 0.45490196 ... 0.08627451 0.40784314\n",
      "    0.45490196]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.02745098 0.11372549 0.13333334 ... 0.02745098 0.11372549\n",
      "    0.13333334]\n",
      "   [0.34117648 0.2627451  0.21960784 ... 0.34117648 0.2627451\n",
      "    0.21960784]\n",
      "   [0.7019608  0.5019608  0.46666667 ... 0.7019608  0.5019608\n",
      "    0.46666667]\n",
      "   ...\n",
      "   [0.7647059  0.5176471  0.52156866 ... 0.7647059  0.5176471\n",
      "    0.52156866]\n",
      "   [0.46666667 0.3019608  0.28235295 ... 0.46666667 0.3019608\n",
      "    0.28235295]\n",
      "   [0.19607843 0.13725491 0.13725491 ... 0.19607843 0.13725491\n",
      "    0.13725491]]\n",
      "\n",
      "  [[0.00392157 0.16078432 0.19215687 ... 0.00392157 0.16078432\n",
      "    0.19215687]\n",
      "   [0.28235295 0.23921569 0.20392157 ... 0.28235295 0.23921569\n",
      "    0.20392157]\n",
      "   [0.6627451  0.4627451  0.41960785 ... 0.6627451  0.4627451\n",
      "    0.41960785]\n",
      "   ...\n",
      "   [0.7294118  0.5137255  0.49411765 ... 0.7294118  0.5137255\n",
      "    0.49411765]\n",
      "   [0.38039216 0.25490198 0.22745098 ... 0.38039216 0.25490198\n",
      "    0.22745098]\n",
      "   [0.11764706 0.10588235 0.12156863 ... 0.11764706 0.10588235\n",
      "    0.12156863]]\n",
      "\n",
      "  [[0.01960784 0.21960784 0.25490198 ... 0.01960784 0.21960784\n",
      "    0.25490198]\n",
      "   [0.24705882 0.23529412 0.20784314 ... 0.24705882 0.23529412\n",
      "    0.20784314]\n",
      "   [0.6313726  0.44313726 0.40392157 ... 0.6313726  0.44313726\n",
      "    0.40392157]\n",
      "   ...\n",
      "   [0.6901961  0.48235294 0.45882353 ... 0.6901961  0.48235294\n",
      "    0.45882353]\n",
      "   [0.29803923 0.22745098 0.21176471 ... 0.29803923 0.22745098\n",
      "    0.21176471]\n",
      "   [0.07450981 0.13725491 0.16470589 ... 0.07450981 0.13725491\n",
      "    0.16470589]]]\n",
      "\n",
      "\n",
      " [[[0.03529412 0.03529412 0.00784314 ... 0.03529412 0.03529412\n",
      "    0.00784314]\n",
      "   [0.02352941 0.02352941 0.00784314 ... 0.02352941 0.02352941\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01176471 0.00392157 ... 0.01568628 0.01176471\n",
      "    0.00392157]\n",
      "   ...\n",
      "   [0.         0.00392157 0.00392157 ... 0.         0.00392157\n",
      "    0.00392157]\n",
      "   [0.00392157 0.00784314 0.00784314 ... 0.00392157 0.00784314\n",
      "    0.00784314]\n",
      "   [0.03529412 0.03921569 0.02745098 ... 0.03529412 0.03921569\n",
      "    0.02745098]]\n",
      "\n",
      "  [[0.03529412 0.03137255 0.01568628 ... 0.03529412 0.03137255\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03529412 0.01568628 ... 0.03529412 0.03529412\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03137255 0.01176471 ... 0.03529412 0.03137255\n",
      "    0.01176471]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157 ... 0.00392157 0.00392157\n",
      "    0.00392157]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]\n",
      "   [0.01568628 0.01960784 0.01176471 ... 0.01568628 0.01960784\n",
      "    0.01176471]]\n",
      "\n",
      "  [[0.10196079 0.09019608 0.05490196 ... 0.10196079 0.09019608\n",
      "    0.05490196]\n",
      "   [0.12941177 0.10588235 0.05882353 ... 0.12941177 0.10588235\n",
      "    0.05882353]\n",
      "   [0.1764706  0.14509805 0.09411765 ... 0.1764706  0.14509805\n",
      "    0.09411765]\n",
      "   ...\n",
      "   [0.00784314 0.01176471 0.00784314 ... 0.00784314 0.01176471\n",
      "    0.00784314]\n",
      "   [0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.7372549  0.69803923 0.6313726  ... 0.7372549  0.69803923\n",
      "    0.6313726 ]\n",
      "   [0.74509805 0.7058824  0.63529414 ... 0.74509805 0.7058824\n",
      "    0.63529414]\n",
      "   [0.73333335 0.69411767 0.61960787 ... 0.73333335 0.69411767\n",
      "    0.61960787]\n",
      "   ...\n",
      "   [0.5882353  0.5568628  0.4862745  ... 0.5882353  0.5568628\n",
      "    0.4862745 ]\n",
      "   [0.5686275  0.5294118  0.4627451  ... 0.5686275  0.5294118\n",
      "    0.4627451 ]\n",
      "   [0.5686275  0.5294118  0.45882353 ... 0.5686275  0.5294118\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.77254903 0.7294118  0.6666667  ... 0.77254903 0.7294118\n",
      "    0.6666667 ]\n",
      "   [0.7607843  0.72156864 0.65882355 ... 0.7607843  0.72156864\n",
      "    0.65882355]\n",
      "   [0.7529412  0.70980394 0.6431373  ... 0.7529412  0.70980394\n",
      "    0.6431373 ]\n",
      "   ...\n",
      "   [0.5803922  0.54509807 0.47058824 ... 0.5803922  0.54509807\n",
      "    0.47058824]\n",
      "   [0.58431375 0.5411765  0.46666667 ... 0.58431375 0.5411765\n",
      "    0.46666667]\n",
      "   [0.5764706  0.53333336 0.45882353 ... 0.5764706  0.53333336\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.7882353  0.7490196  0.68235296 ... 0.7882353  0.7490196\n",
      "    0.68235296]\n",
      "   [0.7764706  0.7372549  0.6745098  ... 0.7764706  0.7372549\n",
      "    0.6745098 ]\n",
      "   [0.7607843  0.7176471  0.654902   ... 0.7607843  0.7176471\n",
      "    0.654902  ]\n",
      "   ...\n",
      "   [0.5882353  0.54509807 0.47058824 ... 0.5882353  0.54509807\n",
      "    0.47058824]\n",
      "   [0.59607846 0.5529412  0.4745098  ... 0.59607846 0.5529412\n",
      "    0.4745098 ]\n",
      "   [0.59607846 0.5568628  0.4862745  ... 0.59607846 0.5568628\n",
      "    0.4862745 ]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9529412  0.9647059  0.9490196  ... 0.9529412  0.9647059\n",
      "    0.9490196 ]\n",
      "   [0.8392157  0.8627451  0.8039216  ... 0.8392157  0.8627451\n",
      "    0.8039216 ]\n",
      "   ...\n",
      "   [0.41568628 0.47058824 0.38431373 ... 0.41568628 0.47058824\n",
      "    0.38431373]\n",
      "   [0.8235294  0.84313726 0.8039216  ... 0.8235294  0.84313726\n",
      "    0.8039216 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9647059  0.972549   0.9607843  ... 0.9647059  0.972549\n",
      "    0.9607843 ]\n",
      "   [0.8509804  0.8862745  0.8352941  ... 0.8509804  0.8862745\n",
      "    0.8352941 ]\n",
      "   ...\n",
      "   [0.42352942 0.48235294 0.3882353  ... 0.42352942 0.48235294\n",
      "    0.3882353 ]\n",
      "   [0.827451   0.84705883 0.80784315 ... 0.827451   0.84705883\n",
      "    0.80784315]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.972549   0.9843137  0.972549   ... 0.972549   0.9843137\n",
      "    0.972549  ]\n",
      "   [0.8666667  0.9098039  0.8666667  ... 0.8666667  0.9098039\n",
      "    0.8666667 ]\n",
      "   ...\n",
      "   [0.43137255 0.49019608 0.39215687 ... 0.43137255 0.49019608\n",
      "    0.39215687]\n",
      "   [0.83137256 0.8509804  0.80784315 ... 0.83137256 0.8509804\n",
      "    0.80784315]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.9764706  0.98039216 0.9764706  ... 0.9764706  0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.9098039  0.8980392  0.8666667  ... 0.9098039  0.8980392\n",
      "    0.8666667 ]\n",
      "   ...\n",
      "   [0.3372549  0.39607844 0.37254903 ... 0.3372549  0.39607844\n",
      "    0.37254903]\n",
      "   [0.8        0.8235294  0.8156863  ... 0.8        0.8235294\n",
      "    0.8156863 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.98039216 0.98039216 0.9764706  ... 0.98039216 0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.91764706 0.9254902  0.90588236 ... 0.91764706 0.9254902\n",
      "    0.90588236]\n",
      "   ...\n",
      "   [0.4        0.43529412 0.39607844 ... 0.4        0.43529412\n",
      "    0.39607844]\n",
      "   [0.7921569  0.8156863  0.8117647  ... 0.7921569  0.8156863\n",
      "    0.8117647 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.98039216 0.98039216 0.9764706  ... 0.98039216 0.98039216\n",
      "    0.9764706 ]\n",
      "   [0.92941177 0.9411765  0.92156863 ... 0.92941177 0.9411765\n",
      "    0.92156863]\n",
      "   ...\n",
      "   [0.5019608  0.49019608 0.43137255 ... 0.5019608  0.49019608\n",
      "    0.43137255]\n",
      "   [0.78039217 0.80784315 0.8039216  ... 0.78039217 0.80784315\n",
      "    0.8039216 ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.49803922 0.49803922 0.5176471  ... 0.49803922 0.49803922\n",
      "    0.5176471 ]\n",
      "   [0.5058824  0.5019608  0.52156866 ... 0.5058824  0.5019608\n",
      "    0.52156866]\n",
      "   [0.50980395 0.5058824  0.5254902  ... 0.50980395 0.5058824\n",
      "    0.5254902 ]\n",
      "   ...\n",
      "   [0.5686275  0.57254905 0.5882353  ... 0.5686275  0.57254905\n",
      "    0.5882353 ]\n",
      "   [0.57254905 0.5764706  0.5921569  ... 0.57254905 0.5764706\n",
      "    0.5921569 ]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]]\n",
      "\n",
      "  [[0.50980395 0.50980395 0.5254902  ... 0.50980395 0.50980395\n",
      "    0.5254902 ]\n",
      "   [0.5137255  0.5137255  0.5294118  ... 0.5137255  0.5137255\n",
      "    0.5294118 ]\n",
      "   [0.5176471  0.5176471  0.53333336 ... 0.5176471  0.5176471\n",
      "    0.53333336]\n",
      "   ...\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5803922  0.58431375 0.6        ... 0.5803922  0.58431375\n",
      "    0.6       ]]\n",
      "\n",
      "  [[0.5176471  0.5176471  0.5372549  ... 0.5176471  0.5176471\n",
      "    0.5372549 ]\n",
      "   [0.52156866 0.5254902  0.5411765  ... 0.52156866 0.5254902\n",
      "    0.5411765 ]\n",
      "   [0.5176471  0.52156866 0.5372549  ... 0.5176471  0.52156866\n",
      "    0.5372549 ]\n",
      "   ...\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.5882353  0.5921569  0.60784316 ... 0.5882353  0.5921569\n",
      "    0.60784316]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8901961  0.8392157  0.8039216  ... 0.8901961  0.8392157\n",
      "    0.8039216 ]\n",
      "   [0.91764706 0.8745098  0.8392157  ... 0.91764706 0.8745098\n",
      "    0.8392157 ]\n",
      "   [0.8627451  0.79607844 0.7490196  ... 0.8627451  0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9372549  0.90588236 0.8784314  ... 0.9372549  0.90588236\n",
      "    0.8784314 ]\n",
      "   [0.90588236 0.85882354 0.8235294  ... 0.90588236 0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.87058824 0.8117647  0.7764706  ... 0.87058824 0.8117647\n",
      "    0.7764706 ]]\n",
      "\n",
      "  [[0.88235295 0.8235294  0.78431374 ... 0.88235295 0.8235294\n",
      "    0.78431374]\n",
      "   [0.89411765 0.8392157  0.8        ... 0.89411765 0.8392157\n",
      "    0.8       ]\n",
      "   [0.87058824 0.8039216  0.7607843  ... 0.87058824 0.8039216\n",
      "    0.7607843 ]\n",
      "   ...\n",
      "   [0.92156863 0.8862745  0.8509804  ... 0.92156863 0.8862745\n",
      "    0.8509804 ]\n",
      "   [0.9098039  0.87058824 0.8352941  ... 0.9098039  0.87058824\n",
      "    0.8352941 ]\n",
      "   [0.8745098  0.8156863  0.77254903 ... 0.8745098  0.8156863\n",
      "    0.77254903]]\n",
      "\n",
      "  [[0.8627451  0.7921569  0.7411765  ... 0.8627451  0.7921569\n",
      "    0.7411765 ]\n",
      "   [0.8666667  0.8039216  0.7529412  ... 0.8666667  0.8039216\n",
      "    0.7529412 ]\n",
      "   [0.85882354 0.79607844 0.7490196  ... 0.85882354 0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9098039  0.85882354 0.8235294  ... 0.9098039  0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.90588236 0.85490197 0.8156863  ... 0.90588236 0.85490197\n",
      "    0.8156863 ]\n",
      "   [0.8784314  0.8156863  0.7647059  ... 0.8784314  0.8156863\n",
      "    0.7647059 ]]]\n",
      "\n",
      "\n",
      " [[[0.65882355 0.5411765  0.34901962 ... 0.65882355 0.5411765\n",
      "    0.34901962]\n",
      "   [0.79607844 0.6862745  0.44313726 ... 0.79607844 0.6862745\n",
      "    0.44313726]\n",
      "   [0.84705883 0.7372549  0.4862745  ... 0.84705883 0.7372549\n",
      "    0.4862745 ]\n",
      "   ...\n",
      "   [0.9019608  0.78431374 0.57254905 ... 0.9019608  0.78431374\n",
      "    0.57254905]\n",
      "   [0.8784314  0.77254903 0.5803922  ... 0.8784314  0.77254903\n",
      "    0.5803922 ]\n",
      "   [0.8666667  0.7607843  0.58431375 ... 0.8666667  0.7607843\n",
      "    0.58431375]]\n",
      "\n",
      "  [[0.6313726  0.5058824  0.27450982 ... 0.6313726  0.5058824\n",
      "    0.27450982]\n",
      "   [0.7372549  0.61960787 0.3764706  ... 0.7372549  0.61960787\n",
      "    0.3764706 ]\n",
      "   [0.83137256 0.72156864 0.4627451  ... 0.83137256 0.72156864\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.8784314  0.7607843  0.5568628  ... 0.8784314  0.7607843\n",
      "    0.5568628 ]\n",
      "   [0.85882354 0.7490196  0.56078434 ... 0.85882354 0.7490196\n",
      "    0.56078434]\n",
      "   [0.85490197 0.7529412  0.5647059  ... 0.85490197 0.7529412\n",
      "    0.5647059 ]]\n",
      "\n",
      "  [[0.6        0.46666667 0.22352941 ... 0.6        0.46666667\n",
      "    0.22352941]\n",
      "   [0.6313726  0.49803922 0.27058825 ... 0.6313726  0.49803922\n",
      "    0.27058825]\n",
      "   [0.80784315 0.69803923 0.44705883 ... 0.80784315 0.69803923\n",
      "    0.44705883]\n",
      "   ...\n",
      "   [0.8352941  0.72156864 0.5372549  ... 0.8352941  0.72156864\n",
      "    0.5372549 ]\n",
      "   [0.83137256 0.7254902  0.5411765  ... 0.83137256 0.7254902\n",
      "    0.5411765 ]\n",
      "   [0.8352941  0.73333335 0.5372549  ... 0.8352941  0.73333335\n",
      "    0.5372549 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6039216  0.3764706  0.27058825 ... 0.6039216  0.3764706\n",
      "    0.27058825]\n",
      "   [0.627451   0.40784314 0.31764707 ... 0.627451   0.40784314\n",
      "    0.31764707]\n",
      "   [0.67058825 0.47058824 0.4        ... 0.67058825 0.47058824\n",
      "    0.4       ]\n",
      "   ...\n",
      "   [0.76862746 0.58431375 0.4745098  ... 0.76862746 0.58431375\n",
      "    0.4745098 ]\n",
      "   [0.7058824  0.49803922 0.38431373 ... 0.7058824  0.49803922\n",
      "    0.38431373]\n",
      "   [0.65882355 0.44313726 0.32156864 ... 0.65882355 0.44313726\n",
      "    0.32156864]]\n",
      "\n",
      "  [[0.6        0.38039216 0.26666668 ... 0.6        0.38039216\n",
      "    0.26666668]\n",
      "   [0.63529414 0.41568628 0.32156864 ... 0.63529414 0.41568628\n",
      "    0.32156864]\n",
      "   [0.7372549  0.56078434 0.4627451  ... 0.7372549  0.56078434\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.83137256 0.654902   0.54901963 ... 0.83137256 0.654902\n",
      "    0.54901963]\n",
      "   [0.77254903 0.5647059  0.44313726 ... 0.77254903 0.5647059\n",
      "    0.44313726]\n",
      "   [0.7176471  0.5019608  0.38039216 ... 0.7176471  0.5019608\n",
      "    0.38039216]]\n",
      "\n",
      "  [[0.627451   0.41960785 0.29411766 ... 0.627451   0.41960785\n",
      "    0.29411766]\n",
      "   [0.68235296 0.46666667 0.3882353  ... 0.68235296 0.46666667\n",
      "    0.3882353 ]\n",
      "   [0.8509804  0.7294118  0.49019608 ... 0.8509804  0.7294118\n",
      "    0.49019608]\n",
      "   ...\n",
      "   [0.89411765 0.72156864 0.6117647  ... 0.89411765 0.72156864\n",
      "    0.6117647 ]\n",
      "   [0.8392157  0.627451   0.50980395 ... 0.8392157  0.627451\n",
      "    0.50980395]\n",
      "   [0.76862746 0.56078434 0.43529412 ... 0.76862746 0.56078434\n",
      "    0.43529412]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.99215686 0.99215686 0.99215686 ... 0.99215686 0.99215686\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6509804  0.6117647  0.60784316 ... 0.6509804  0.6117647\n",
      "    0.60784316]\n",
      "   [0.6431373  0.6        0.5921569  ... 0.6431373  0.6\n",
      "    0.5921569 ]\n",
      "   [0.63529414 0.5921569  0.5803922  ... 0.63529414 0.5921569\n",
      "    0.5803922 ]\n",
      "   ...\n",
      "   [0.80784315 0.69803923 0.67058825 ... 0.80784315 0.69803923\n",
      "    0.67058825]\n",
      "   [0.80784315 0.7176471  0.6901961  ... 0.80784315 0.7176471\n",
      "    0.6901961 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]]\n",
      "\n",
      "  [[0.64705884 0.59607846 0.59607846 ... 0.64705884 0.59607846\n",
      "    0.59607846]\n",
      "   [0.63529414 0.5921569  0.58431375 ... 0.63529414 0.5921569\n",
      "    0.58431375]\n",
      "   [0.62352943 0.5803922  0.57254905 ... 0.62352943 0.5803922\n",
      "    0.57254905]\n",
      "   ...\n",
      "   [0.8        0.69803923 0.6745098  ... 0.8        0.69803923\n",
      "    0.6745098 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]\n",
      "   [0.7921569  0.7019608  0.68235296 ... 0.7921569  0.7019608\n",
      "    0.68235296]]\n",
      "\n",
      "  [[0.6431373  0.59607846 0.5921569  ... 0.6431373  0.59607846\n",
      "    0.5921569 ]\n",
      "   [0.6313726  0.5882353  0.5803922  ... 0.6313726  0.5882353\n",
      "    0.5803922 ]\n",
      "   [0.627451   0.57254905 0.5686275  ... 0.627451   0.57254905\n",
      "    0.5686275 ]\n",
      "   ...\n",
      "   [0.7921569  0.6901961  0.6784314  ... 0.7921569  0.6901961\n",
      "    0.6784314 ]\n",
      "   [0.80784315 0.7058824  0.6901961  ... 0.80784315 0.7058824\n",
      "    0.6901961 ]\n",
      "   [0.80784315 0.7176471  0.7019608  ... 0.80784315 0.7176471\n",
      "    0.7019608 ]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (74, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (74, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 7s 3s/step - loss: 2.7675 - accuracy: 0.0541 - val_loss: 2.7748 - val_accuracy: 0.9643\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/100\n",
      "Learning rate:  1e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.7779 - accuracy: 0.0405 - val_loss: 2.6040 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/100\n",
      "Learning rate:  2e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.7372 - accuracy: 0.0946 - val_loss: 2.5062 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/100\n",
      "Learning rate:  3e-05\n",
      "2/2 [==============================] - 2s 549ms/step - loss: 2.7404 - accuracy: 0.0946 - val_loss: 2.4250 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/100\n",
      "Learning rate:  4e-05\n",
      "2/2 [==============================] - 2s 522ms/step - loss: 2.7260 - accuracy: 0.0541 - val_loss: 2.3552 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/100\n",
      "Learning rate:  5e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.6768 - accuracy: 0.1757 - val_loss: 2.2648 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/100\n",
      "Learning rate:  6e-05\n",
      "2/2 [==============================] - 2s 593ms/step - loss: 2.6423 - accuracy: 0.1622 - val_loss: 2.1764 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/100\n",
      "Learning rate:  7.000000000000001e-05\n",
      "2/2 [==============================] - 2s 497ms/step - loss: 2.5950 - accuracy: 0.2703 - val_loss: 2.0646 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/100\n",
      "Learning rate:  8e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.4971 - accuracy: 0.3243 - val_loss: 1.9568 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/100\n",
      "Learning rate:  8.999999999999999e-05\n",
      "2/2 [==============================] - 3s 2s/step - loss: 2.4368 - accuracy: 0.4054 - val_loss: 1.8181 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/100\n",
      "Learning rate:  0.0001\n",
      "2/2 [==============================] - 2s 488ms/step - loss: 2.3958 - accuracy: 0.4730 - val_loss: 1.6823 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/100\n",
      "Learning rate:  0.00011\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.2915 - accuracy: 0.5811 - val_loss: 1.5633 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/100\n",
      "Learning rate:  0.00012\n",
      "2/2 [==============================] - 2s 524ms/step - loss: 2.2230 - accuracy: 0.5811 - val_loss: 1.4372 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/100\n",
      "Learning rate:  0.00013000000000000002\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.1147 - accuracy: 0.6486 - val_loss: 1.3030 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/100\n",
      "Learning rate:  0.00014000000000000001\n",
      "2/2 [==============================] - 2s 507ms/step - loss: 2.0600 - accuracy: 0.7162 - val_loss: 1.1866 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/100\n",
      "Learning rate:  0.00015\n",
      "2/2 [==============================] - 2s 537ms/step - loss: 1.9852 - accuracy: 0.7162 - val_loss: 1.0802 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/100\n",
      "Learning rate:  0.00016\n",
      "2/2 [==============================] - 2s 498ms/step - loss: 1.8739 - accuracy: 0.7432 - val_loss: 0.9920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/100\n",
      "Learning rate:  0.00017\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.7982 - accuracy: 0.7432 - val_loss: 0.9448 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/100\n",
      "Learning rate:  0.00017999999999999998\n",
      "2/2 [==============================] - 2s 512ms/step - loss: 1.7269 - accuracy: 0.7703 - val_loss: 0.8975 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/100\n",
      "Learning rate:  0.00019\n",
      "2/2 [==============================] - 2s 475ms/step - loss: 1.6628 - accuracy: 0.7838 - val_loss: 0.8488 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 21/100\n",
      "Learning rate:  0.0002\n",
      "2/2 [==============================] - 2s 546ms/step - loss: 1.5977 - accuracy: 0.8108 - val_loss: 0.8096 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 22/100\n",
      "Learning rate:  0.00021\n",
      "2/2 [==============================] - 3s 659ms/step - loss: 1.5250 - accuracy: 0.7838 - val_loss: 0.7832 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 23/100\n",
      "Learning rate:  0.00022\n",
      "2/2 [==============================] - 2s 513ms/step - loss: 1.4642 - accuracy: 0.8108 - val_loss: 0.7602 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 24/100\n",
      "Learning rate:  0.00023\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.4076 - accuracy: 0.7838 - val_loss: 0.7412 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 25/100\n",
      "Learning rate:  0.00024\n",
      "2/2 [==============================] - 2s 476ms/step - loss: 1.3573 - accuracy: 0.8378 - val_loss: 0.7230 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 26/100\n",
      "Learning rate:  0.00025\n",
      "2/2 [==============================] - 3s 2s/step - loss: 1.3076 - accuracy: 0.7973 - val_loss: 0.7067 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 27/100\n",
      "Learning rate:  0.00026000000000000003\n",
      "2/2 [==============================] - 2s 541ms/step - loss: 1.2970 - accuracy: 0.8243 - val_loss: 0.6932 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 28/100\n",
      "Learning rate:  0.00027\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.2134 - accuracy: 0.8514 - val_loss: 0.6880 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 29/100\n",
      "Learning rate:  0.00028000000000000003\n",
      "2/2 [==============================] - 2s 482ms/step - loss: 1.1843 - accuracy: 0.8243 - val_loss: 0.6833 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 30/100\n",
      "Learning rate:  0.00029\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.1259 - accuracy: 0.8378 - val_loss: 0.6888 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 31/100\n",
      "Learning rate:  0.0003\n",
      "2/2 [==============================] - 2s 516ms/step - loss: 1.1058 - accuracy: 0.8649 - val_loss: 0.6929 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 32/100\n",
      "Learning rate:  0.00031\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0877 - accuracy: 0.8514 - val_loss: 0.6811 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 33/100\n",
      "Learning rate:  0.00032\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9882 - accuracy: 0.9324 - val_loss: 0.6708 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 34/100\n",
      "Learning rate:  0.00033\n",
      "2/2 [==============================] - 3s 3s/step - loss: 1.0221 - accuracy: 0.9054 - val_loss: 0.6616 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 35/100\n",
      "Learning rate:  0.00034\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9890 - accuracy: 0.8919 - val_loss: 0.6473 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 36/100\n",
      "Learning rate:  0.00035\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9417 - accuracy: 0.9324 - val_loss: 0.6499 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 37/100\n",
      "Learning rate:  0.00035999999999999997\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9649 - accuracy: 0.9054 - val_loss: 0.6583 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 38/100\n",
      "Learning rate:  0.00037\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9243 - accuracy: 0.9459 - val_loss: 0.6742 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 39/100\n",
      "Learning rate:  0.00038\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9062 - accuracy: 0.9324 - val_loss: 0.6705 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 40/100\n",
      "Learning rate:  0.00039000000000000005\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8679 - accuracy: 0.9324 - val_loss: 0.6723 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 41/100\n",
      "Learning rate:  0.0004\n",
      "2/2 [==============================] - 2s 535ms/step - loss: 0.8560 - accuracy: 0.9865 - val_loss: 0.6760 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 42/100\n",
      "Learning rate:  0.00041\n",
      "2/2 [==============================] - 2s 505ms/step - loss: 0.8779 - accuracy: 0.9459 - val_loss: 0.6674 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 43/100\n",
      "Learning rate:  0.00042\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8189 - accuracy: 0.9595 - val_loss: 0.6419 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 44/100\n",
      "Learning rate:  0.00043\n",
      "2/2 [==============================] - 2s 515ms/step - loss: 0.7779 - accuracy: 0.9595 - val_loss: 0.6348 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 45/100\n",
      "Learning rate:  0.00044\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7880 - accuracy: 0.9459 - val_loss: 0.6365 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 46/100\n",
      "Learning rate:  0.00045000000000000004\n",
      "2/2 [==============================] - 2s 515ms/step - loss: 0.7968 - accuracy: 0.9459 - val_loss: 0.6323 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 47/100\n",
      "Learning rate:  0.00046\n",
      "2/2 [==============================] - 2s 533ms/step - loss: 0.7880 - accuracy: 0.9595 - val_loss: 0.6269 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 48/100\n",
      "Learning rate:  0.00047\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7955 - accuracy: 0.9459 - val_loss: 0.6191 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 49/100\n",
      "Learning rate:  0.00048\n",
      "2/2 [==============================] - 2s 509ms/step - loss: 0.7603 - accuracy: 0.9324 - val_loss: 0.6177 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 50/100\n",
      "Learning rate:  0.00049\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7605 - accuracy: 0.9459 - val_loss: 0.6411 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 51/100\n",
      "Learning rate:  0.0005\n",
      "2/2 [==============================] - 2s 521ms/step - loss: 0.7065 - accuracy: 0.9730 - val_loss: 0.6841 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 52/100\n",
      "Learning rate:  0.00051\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7443 - accuracy: 0.9730 - val_loss: 0.7203 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 53/100\n",
      "Learning rate:  0.0005200000000000001\n",
      "2/2 [==============================] - 3s 601ms/step - loss: 0.7403 - accuracy: 0.9595 - val_loss: 0.6596 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 54/100\n",
      "Learning rate:  0.0005300000000000001\n",
      "2/2 [==============================] - 3s 525ms/step - loss: 0.7111 - accuracy: 0.9865 - val_loss: 0.6153 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 55/100\n",
      "Learning rate:  0.00054\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8840 - accuracy: 0.8784 - val_loss: 0.6022 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 56/100\n",
      "Learning rate:  0.00055\n",
      "2/2 [==============================] - 2s 506ms/step - loss: 0.8165 - accuracy: 0.8919 - val_loss: 0.6005 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 57/100\n",
      "Learning rate:  0.0005600000000000001\n",
      "2/2 [==============================] - 2s 504ms/step - loss: 0.8571 - accuracy: 0.9054 - val_loss: 0.6010 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 58/100\n",
      "Learning rate:  0.00057\n",
      "2/2 [==============================] - 2s 549ms/step - loss: 0.7611 - accuracy: 0.9595 - val_loss: 0.6029 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 59/100\n",
      "Learning rate:  0.00058\n",
      "2/2 [==============================] - 2s 561ms/step - loss: 0.7005 - accuracy: 0.9865 - val_loss: 0.6048 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 60/100\n",
      "Learning rate:  0.00059\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7501 - accuracy: 0.9595 - val_loss: 0.5988 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 61/100\n",
      "Learning rate:  0.0006\n",
      "2/2 [==============================] - 2s 548ms/step - loss: 0.6851 - accuracy: 0.9865 - val_loss: 0.5976 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 62/100\n",
      "Learning rate:  0.00061\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7320 - accuracy: 0.9459 - val_loss: 0.6006 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 63/100\n",
      "Learning rate:  0.00062\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7029 - accuracy: 0.9865 - val_loss: 0.6059 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 64/100\n",
      "Learning rate:  0.00063\n",
      "2/2 [==============================] - 2s 518ms/step - loss: 0.7029 - accuracy: 0.9730 - val_loss: 0.6097 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 65/100\n",
      "Learning rate:  0.00064\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7062 - accuracy: 0.9865 - val_loss: 0.6141 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 66/100\n",
      "Learning rate:  0.0006500000000000001\n",
      "2/2 [==============================] - 3s 650ms/step - loss: 0.6660 - accuracy: 1.0000 - val_loss: 0.6100 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 67/100\n",
      "Learning rate:  0.00066\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6572 - accuracy: 0.9865 - val_loss: 0.5997 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 68/100\n",
      "Learning rate:  0.00067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 2s/step - loss: 0.6484 - accuracy: 1.0000 - val_loss: 0.5954 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 69/100\n",
      "Learning rate:  0.00068\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6455 - accuracy: 1.0000 - val_loss: 0.5943 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 70/100\n",
      "Learning rate:  0.00069\n",
      "2/2 [==============================] - 2s 564ms/step - loss: 0.6899 - accuracy: 0.9730 - val_loss: 0.5948 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 71/100\n",
      "Learning rate:  0.0007\n",
      "2/2 [==============================] - 2s 531ms/step - loss: 0.6547 - accuracy: 1.0000 - val_loss: 0.5962 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 72/100\n",
      "Learning rate:  0.00071\n",
      "2/2 [==============================] - 2s 532ms/step - loss: 0.6432 - accuracy: 1.0000 - val_loss: 0.6021 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 73/100\n",
      "Learning rate:  0.0007199999999999999\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6423 - accuracy: 1.0000 - val_loss: 0.6142 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 74/100\n",
      "Learning rate:  0.00073\n",
      "2/2 [==============================] - 2s 481ms/step - loss: 0.7044 - accuracy: 0.9595 - val_loss: 0.6122 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 75/100\n",
      "Learning rate:  0.00074\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6978 - accuracy: 0.9865 - val_loss: 0.6074 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 76/100\n",
      "Learning rate:  0.00075\n",
      "2/2 [==============================] - 2s 506ms/step - loss: 0.7083 - accuracy: 0.9459 - val_loss: 0.5942 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 77/100\n",
      "Learning rate:  0.00076\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6501 - accuracy: 0.9865 - val_loss: 0.5874 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 78/100\n",
      "Learning rate:  0.0007700000000000001\n",
      "2/2 [==============================] - 2s 528ms/step - loss: 0.7691 - accuracy: 0.9595 - val_loss: 0.5860 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 79/100\n",
      "Learning rate:  0.0007800000000000001\n",
      "2/2 [==============================] - 2s 481ms/step - loss: 0.6506 - accuracy: 0.9730 - val_loss: 0.5853 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 80/100\n",
      "Learning rate:  0.00079\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7341 - accuracy: 0.9459 - val_loss: 0.5847 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 81/100\n",
      "Learning rate:  0.0008\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7899 - accuracy: 0.9054 - val_loss: 0.5856 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 82/100\n",
      "Learning rate:  8.1e-05\n",
      "2/2 [==============================] - 2s 480ms/step - loss: 0.7922 - accuracy: 0.9730 - val_loss: 0.5856 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 83/100\n",
      "Learning rate:  8.2e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6562 - accuracy: 0.9865 - val_loss: 0.5859 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 84/100\n",
      "Learning rate:  8.3e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6927 - accuracy: 0.9730 - val_loss: 0.5863 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 85/100\n",
      "Learning rate:  8.4e-05\n",
      "2/2 [==============================] - 2s 483ms/step - loss: 0.6915 - accuracy: 0.9459 - val_loss: 0.5867 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 86/100\n",
      "Learning rate:  8.5e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6343 - accuracy: 1.0000 - val_loss: 0.5872 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 87/100\n",
      "Learning rate:  8.6e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6767 - accuracy: 0.9865 - val_loss: 0.5871 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 88/100\n",
      "Learning rate:  8.7e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6184 - accuracy: 1.0000 - val_loss: 0.5869 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 89/100\n",
      "Learning rate:  8.800000000000001e-05\n",
      "2/2 [==============================] - 2s 603ms/step - loss: 0.6204 - accuracy: 1.0000 - val_loss: 0.5866 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 90/100\n",
      "Learning rate:  8.900000000000001e-05\n",
      "2/2 [==============================] - 2s 501ms/step - loss: 0.6328 - accuracy: 0.9865 - val_loss: 0.5864 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 91/100\n",
      "Learning rate:  9e-05\n",
      "2/2 [==============================] - 2s 484ms/step - loss: 0.6236 - accuracy: 1.0000 - val_loss: 0.5862 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 92/100\n",
      "Learning rate:  9.1e-05\n",
      "2/2 [==============================] - 2s 504ms/step - loss: 0.6902 - accuracy: 0.9595 - val_loss: 0.5860 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 93/100\n",
      "Learning rate:  9.200000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6214 - accuracy: 1.0000 - val_loss: 0.5857 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 94/100\n",
      "Learning rate:  9.300000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6232 - accuracy: 1.0000 - val_loss: 0.5857 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 95/100\n",
      "Learning rate:  9.4e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6944 - accuracy: 0.9730 - val_loss: 0.5847 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 96/100\n",
      "Learning rate:  9.5e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6400 - accuracy: 0.9865 - val_loss: 0.5844 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 97/100\n",
      "Learning rate:  9.6e-05\n",
      "2/2 [==============================] - 2s 499ms/step - loss: 0.6597 - accuracy: 0.9730 - val_loss: 0.5844 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 98/100\n",
      "Learning rate:  9.7e-05\n",
      "2/2 [==============================] - 2s 533ms/step - loss: 0.6575 - accuracy: 0.9730 - val_loss: 0.5846 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 99/100\n",
      "Learning rate:  9.8e-05\n",
      "2/2 [==============================] - 2s 510ms/step - loss: 0.6276 - accuracy: 0.9865 - val_loss: 0.5853 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 100/100\n",
      "Learning rate:  9.900000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6494 - accuracy: 0.9730 - val_loss: 0.5869 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 1.7402 - accuracy: 0.7500\n",
      "Test loss: 1.7402490377426147\n",
      "Test accuracy: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision:  0.8472222222222222\n",
      "Recall:  0.75\n",
      "F1 Score:  0.747765006385696\n",
      "27\n",
      "27\n",
      "[[[[243 243 243 ... 243 243 243]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  [[242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  [[242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[243 242 244 ... 243 242 244]\n",
      "   [245 242 248 ... 245 242 248]\n",
      "   [250 250 253 ... 250 250 253]\n",
      "   ...\n",
      "   [214 218 239 ... 214 218 239]\n",
      "   [205 209 233 ... 205 209 233]\n",
      "   [213 215 234 ... 213 215 234]]\n",
      "\n",
      "  [[238 236 241 ... 238 236 241]\n",
      "   [235 230 238 ... 235 230 238]\n",
      "   [240 236 243 ... 240 236 243]\n",
      "   ...\n",
      "   [210 214 238 ... 210 214 238]\n",
      "   [202 206 232 ... 202 206 232]\n",
      "   [206 208 231 ... 206 208 231]]\n",
      "\n",
      "  [[226 219 227 ... 226 219 227]\n",
      "   [221 211 220 ... 221 211 220]\n",
      "   [219 210 222 ... 219 210 222]\n",
      "   ...\n",
      "   [207 212 237 ... 207 212 237]\n",
      "   [207 212 236 ... 207 212 236]\n",
      "   [207 210 233 ... 207 210 233]]]\n",
      "\n",
      "\n",
      " [[[ 76  84  95 ...  76  84  95]\n",
      "   [ 15  15  20 ...  15  15  20]\n",
      "   [  8   7  11 ...   8   7  11]\n",
      "   ...\n",
      "   [ 16  14  25 ...  16  14  25]\n",
      "   [ 13  13  22 ...  13  13  22]\n",
      "   [  8   9  13 ...   8   9  13]]\n",
      "\n",
      "  [[ 88  96 107 ...  88  96 107]\n",
      "   [ 17  17  22 ...  17  17  22]\n",
      "   [  6   5   9 ...   6   5   9]\n",
      "   ...\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 15  13  24 ...  15  13  24]\n",
      "   [ 10  10  16 ...  10  10  16]]\n",
      "\n",
      "  [[ 86  91  98 ...  86  91  98]\n",
      "   [ 14  13  17 ...  14  13  17]\n",
      "   [  5   4   7 ...   5   4   7]\n",
      "   ...\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 12  11  18 ...  12  11  18]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[157 162 174 ... 157 162 174]\n",
      "   [159 165 176 ... 159 165 176]\n",
      "   [155 158 159 ... 155 158 159]\n",
      "   ...\n",
      "   [126 136 168 ... 126 136 168]\n",
      "   [145 153 178 ... 145 153 178]\n",
      "   [150 157 180 ... 150 157 180]]\n",
      "\n",
      "  [[158 162 174 ... 158 162 174]\n",
      "   [158 164 177 ... 158 164 177]\n",
      "   [160 167 179 ... 160 167 179]\n",
      "   ...\n",
      "   [134 144 172 ... 134 144 172]\n",
      "   [143 151 177 ... 143 151 177]\n",
      "   [151 158 180 ... 151 158 180]]\n",
      "\n",
      "  [[160 164 176 ... 160 164 176]\n",
      "   [158 164 177 ... 158 164 177]\n",
      "   [163 169 181 ... 163 169 181]\n",
      "   ...\n",
      "   [141 150 174 ... 141 150 174]\n",
      "   [140 149 176 ... 140 149 176]\n",
      "   [150 158 180 ... 150 158 180]]]\n",
      "\n",
      "\n",
      " [[[  9   9   2 ...   9   9   2]\n",
      "   [  6   6   2 ...   6   6   2]\n",
      "   [  4   3   1 ...   4   3   1]\n",
      "   ...\n",
      "   [  0   1   1 ...   0   1   1]\n",
      "   [  1   2   2 ...   1   2   2]\n",
      "   [  9  10   7 ...   9  10   7]]\n",
      "\n",
      "  [[  9   8   4 ...   9   8   4]\n",
      "   [  9   9   4 ...   9   9   4]\n",
      "   [  9   8   3 ...   9   8   3]\n",
      "   ...\n",
      "   [  1   1   1 ...   1   1   1]\n",
      "   [  4   4   3 ...   4   4   3]\n",
      "   [  4   5   3 ...   4   5   3]]\n",
      "\n",
      "  [[ 26  23  14 ...  26  23  14]\n",
      "   [ 33  27  15 ...  33  27  15]\n",
      "   [ 45  37  24 ...  45  37  24]\n",
      "   ...\n",
      "   [  2   3   2 ...   2   3   2]\n",
      "   [  2   2   2 ...   2   2   2]\n",
      "   [  4   4   3 ...   4   4   3]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[188 178 161 ... 188 178 161]\n",
      "   [190 180 162 ... 190 180 162]\n",
      "   [187 177 158 ... 187 177 158]\n",
      "   ...\n",
      "   [150 142 124 ... 150 142 124]\n",
      "   [145 135 118 ... 145 135 118]\n",
      "   [145 135 117 ... 145 135 117]]\n",
      "\n",
      "  [[197 186 170 ... 197 186 170]\n",
      "   [194 184 168 ... 194 184 168]\n",
      "   [192 181 164 ... 192 181 164]\n",
      "   ...\n",
      "   [148 139 120 ... 148 139 120]\n",
      "   [149 138 119 ... 149 138 119]\n",
      "   [147 136 117 ... 147 136 117]]\n",
      "\n",
      "  [[201 191 174 ... 201 191 174]\n",
      "   [198 188 172 ... 198 188 172]\n",
      "   [194 183 167 ... 194 183 167]\n",
      "   ...\n",
      "   [150 139 120 ... 150 139 120]\n",
      "   [152 141 121 ... 152 141 121]\n",
      "   [152 142 124 ... 152 142 124]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (77, 40, 30, 9)\n",
      "77 train samples\n",
      "26 test samples\n",
      "y_train shape: (77, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_27\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_28 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_837 (Conv2D)             (None, 40, 30, 16)   1312        input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_756 (BatchN (None, 40, 30, 16)   64          conv2d_837[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_756 (Activation)     (None, 40, 30, 16)   0           batch_normalization_756[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_838 (Conv2D)             (None, 40, 30, 16)   272         activation_756[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_757 (BatchN (None, 40, 30, 16)   64          conv2d_838[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_757 (Activation)     (None, 40, 30, 16)   0           batch_normalization_757[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_839 (Conv2D)             (None, 40, 30, 16)   2320        activation_757[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_758 (BatchN (None, 40, 30, 16)   64          conv2d_839[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_758 (Activation)     (None, 40, 30, 16)   0           batch_normalization_758[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_841 (Conv2D)             (None, 40, 30, 64)   1088        activation_756[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_840 (Conv2D)             (None, 40, 30, 64)   1088        activation_758[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_243 (Add)                   (None, 40, 30, 64)   0           conv2d_841[0][0]                 \n",
      "                                                                 conv2d_840[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_759 (BatchN (None, 40, 30, 64)   256         add_243[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_759 (Activation)     (None, 40, 30, 64)   0           batch_normalization_759[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_842 (Conv2D)             (None, 40, 30, 16)   1040        activation_759[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_760 (BatchN (None, 40, 30, 16)   64          conv2d_842[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_760 (Activation)     (None, 40, 30, 16)   0           batch_normalization_760[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_843 (Conv2D)             (None, 40, 30, 16)   2320        activation_760[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_761 (BatchN (None, 40, 30, 16)   64          conv2d_843[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_761 (Activation)     (None, 40, 30, 16)   0           batch_normalization_761[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_844 (Conv2D)             (None, 40, 30, 64)   1088        activation_761[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_244 (Add)                   (None, 40, 30, 64)   0           add_243[0][0]                    \n",
      "                                                                 conv2d_844[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_762 (BatchN (None, 40, 30, 64)   256         add_244[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_762 (Activation)     (None, 40, 30, 64)   0           batch_normalization_762[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_845 (Conv2D)             (None, 40, 30, 16)   1040        activation_762[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_763 (BatchN (None, 40, 30, 16)   64          conv2d_845[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_763 (Activation)     (None, 40, 30, 16)   0           batch_normalization_763[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_846 (Conv2D)             (None, 40, 30, 16)   2320        activation_763[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_764 (BatchN (None, 40, 30, 16)   64          conv2d_846[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_764 (Activation)     (None, 40, 30, 16)   0           batch_normalization_764[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_847 (Conv2D)             (None, 40, 30, 64)   1088        activation_764[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_245 (Add)                   (None, 40, 30, 64)   0           add_244[0][0]                    \n",
      "                                                                 conv2d_847[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_765 (BatchN (None, 40, 30, 64)   256         add_245[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_765 (Activation)     (None, 40, 30, 64)   0           batch_normalization_765[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_848 (Conv2D)             (None, 20, 15, 64)   4160        activation_765[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_766 (BatchN (None, 20, 15, 64)   256         conv2d_848[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_766 (Activation)     (None, 20, 15, 64)   0           batch_normalization_766[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_849 (Conv2D)             (None, 20, 15, 64)   36928       activation_766[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_767 (BatchN (None, 20, 15, 64)   256         conv2d_849[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_767 (Activation)     (None, 20, 15, 64)   0           batch_normalization_767[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_851 (Conv2D)             (None, 20, 15, 128)  8320        add_245[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_850 (Conv2D)             (None, 20, 15, 128)  8320        activation_767[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_246 (Add)                   (None, 20, 15, 128)  0           conv2d_851[0][0]                 \n",
      "                                                                 conv2d_850[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_768 (BatchN (None, 20, 15, 128)  512         add_246[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_768 (Activation)     (None, 20, 15, 128)  0           batch_normalization_768[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_852 (Conv2D)             (None, 20, 15, 64)   8256        activation_768[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_769 (BatchN (None, 20, 15, 64)   256         conv2d_852[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_769 (Activation)     (None, 20, 15, 64)   0           batch_normalization_769[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_853 (Conv2D)             (None, 20, 15, 64)   36928       activation_769[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_770 (BatchN (None, 20, 15, 64)   256         conv2d_853[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_770 (Activation)     (None, 20, 15, 64)   0           batch_normalization_770[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_854 (Conv2D)             (None, 20, 15, 128)  8320        activation_770[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_247 (Add)                   (None, 20, 15, 128)  0           add_246[0][0]                    \n",
      "                                                                 conv2d_854[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_771 (BatchN (None, 20, 15, 128)  512         add_247[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_771 (Activation)     (None, 20, 15, 128)  0           batch_normalization_771[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_855 (Conv2D)             (None, 20, 15, 64)   8256        activation_771[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_772 (BatchN (None, 20, 15, 64)   256         conv2d_855[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_772 (Activation)     (None, 20, 15, 64)   0           batch_normalization_772[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_856 (Conv2D)             (None, 20, 15, 64)   36928       activation_772[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_773 (BatchN (None, 20, 15, 64)   256         conv2d_856[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_773 (Activation)     (None, 20, 15, 64)   0           batch_normalization_773[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_857 (Conv2D)             (None, 20, 15, 128)  8320        activation_773[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_248 (Add)                   (None, 20, 15, 128)  0           add_247[0][0]                    \n",
      "                                                                 conv2d_857[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_774 (BatchN (None, 20, 15, 128)  512         add_248[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_774 (Activation)     (None, 20, 15, 128)  0           batch_normalization_774[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_858 (Conv2D)             (None, 10, 8, 128)   16512       activation_774[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_775 (BatchN (None, 10, 8, 128)   512         conv2d_858[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_775 (Activation)     (None, 10, 8, 128)   0           batch_normalization_775[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_859 (Conv2D)             (None, 10, 8, 128)   147584      activation_775[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_776 (BatchN (None, 10, 8, 128)   512         conv2d_859[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_776 (Activation)     (None, 10, 8, 128)   0           batch_normalization_776[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_861 (Conv2D)             (None, 10, 8, 256)   33024       add_248[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_860 (Conv2D)             (None, 10, 8, 256)   33024       activation_776[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_249 (Add)                   (None, 10, 8, 256)   0           conv2d_861[0][0]                 \n",
      "                                                                 conv2d_860[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_777 (BatchN (None, 10, 8, 256)   1024        add_249[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_777 (Activation)     (None, 10, 8, 256)   0           batch_normalization_777[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_862 (Conv2D)             (None, 10, 8, 128)   32896       activation_777[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_778 (BatchN (None, 10, 8, 128)   512         conv2d_862[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_778 (Activation)     (None, 10, 8, 128)   0           batch_normalization_778[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_863 (Conv2D)             (None, 10, 8, 128)   147584      activation_778[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_779 (BatchN (None, 10, 8, 128)   512         conv2d_863[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_779 (Activation)     (None, 10, 8, 128)   0           batch_normalization_779[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_864 (Conv2D)             (None, 10, 8, 256)   33024       activation_779[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_250 (Add)                   (None, 10, 8, 256)   0           add_249[0][0]                    \n",
      "                                                                 conv2d_864[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_780 (BatchN (None, 10, 8, 256)   1024        add_250[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_780 (Activation)     (None, 10, 8, 256)   0           batch_normalization_780[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_865 (Conv2D)             (None, 10, 8, 128)   32896       activation_780[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_781 (BatchN (None, 10, 8, 128)   512         conv2d_865[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_781 (Activation)     (None, 10, 8, 128)   0           batch_normalization_781[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_866 (Conv2D)             (None, 10, 8, 128)   147584      activation_781[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_782 (BatchN (None, 10, 8, 128)   512         conv2d_866[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_782 (Activation)     (None, 10, 8, 128)   0           batch_normalization_782[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_867 (Conv2D)             (None, 10, 8, 256)   33024       activation_782[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_251 (Add)                   (None, 10, 8, 256)   0           add_250[0][0]                    \n",
      "                                                                 conv2d_867[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_783 (BatchN (None, 10, 8, 256)   1024        add_251[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_783 (Activation)     (None, 10, 8, 256)   0           batch_normalization_783[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_27 (AveragePo (None, 1, 1, 256)    0           activation_783[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 256)          0           average_pooling2d_27[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 10)           2570        flatten_27[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[0.9529412  0.9529412  0.9529412  ... 0.9529412  0.9529412\n",
      "    0.9529412 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  [[0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  [[0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.9529412  0.9490196  0.95686275 ... 0.9529412  0.9490196\n",
      "    0.95686275]\n",
      "   [0.9607843  0.9490196  0.972549   ... 0.9607843  0.9490196\n",
      "    0.972549  ]\n",
      "   [0.98039216 0.98039216 0.99215686 ... 0.98039216 0.98039216\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [0.8392157  0.85490197 0.9372549  ... 0.8392157  0.85490197\n",
      "    0.9372549 ]\n",
      "   [0.8039216  0.81960785 0.9137255  ... 0.8039216  0.81960785\n",
      "    0.9137255 ]\n",
      "   [0.8352941  0.84313726 0.91764706 ... 0.8352941  0.84313726\n",
      "    0.91764706]]\n",
      "\n",
      "  [[0.93333334 0.9254902  0.94509804 ... 0.93333334 0.9254902\n",
      "    0.94509804]\n",
      "   [0.92156863 0.9019608  0.93333334 ... 0.92156863 0.9019608\n",
      "    0.93333334]\n",
      "   [0.9411765  0.9254902  0.9529412  ... 0.9411765  0.9254902\n",
      "    0.9529412 ]\n",
      "   ...\n",
      "   [0.8235294  0.8392157  0.93333334 ... 0.8235294  0.8392157\n",
      "    0.93333334]\n",
      "   [0.7921569  0.80784315 0.9098039  ... 0.7921569  0.80784315\n",
      "    0.9098039 ]\n",
      "   [0.80784315 0.8156863  0.90588236 ... 0.80784315 0.8156863\n",
      "    0.90588236]]\n",
      "\n",
      "  [[0.8862745  0.85882354 0.8901961  ... 0.8862745  0.85882354\n",
      "    0.8901961 ]\n",
      "   [0.8666667  0.827451   0.8627451  ... 0.8666667  0.827451\n",
      "    0.8627451 ]\n",
      "   [0.85882354 0.8235294  0.87058824 ... 0.85882354 0.8235294\n",
      "    0.87058824]\n",
      "   ...\n",
      "   [0.8117647  0.83137256 0.92941177 ... 0.8117647  0.83137256\n",
      "    0.92941177]\n",
      "   [0.8117647  0.83137256 0.9254902  ... 0.8117647  0.83137256\n",
      "    0.9254902 ]\n",
      "   [0.8117647  0.8235294  0.9137255  ... 0.8117647  0.8235294\n",
      "    0.9137255 ]]]\n",
      "\n",
      "\n",
      " [[[0.29803923 0.32941177 0.37254903 ... 0.29803923 0.32941177\n",
      "    0.37254903]\n",
      "   [0.05882353 0.05882353 0.07843138 ... 0.05882353 0.05882353\n",
      "    0.07843138]\n",
      "   [0.03137255 0.02745098 0.04313726 ... 0.03137255 0.02745098\n",
      "    0.04313726]\n",
      "   ...\n",
      "   [0.0627451  0.05490196 0.09803922 ... 0.0627451  0.05490196\n",
      "    0.09803922]\n",
      "   [0.05098039 0.05098039 0.08627451 ... 0.05098039 0.05098039\n",
      "    0.08627451]\n",
      "   [0.03137255 0.03529412 0.05098039 ... 0.03137255 0.03529412\n",
      "    0.05098039]]\n",
      "\n",
      "  [[0.34509805 0.3764706  0.41960785 ... 0.34509805 0.3764706\n",
      "    0.41960785]\n",
      "   [0.06666667 0.06666667 0.08627451 ... 0.06666667 0.06666667\n",
      "    0.08627451]\n",
      "   [0.02352941 0.01960784 0.03529412 ... 0.02352941 0.01960784\n",
      "    0.03529412]\n",
      "   ...\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.05882353 0.05098039 0.09411765 ... 0.05882353 0.05098039\n",
      "    0.09411765]\n",
      "   [0.03921569 0.03921569 0.0627451  ... 0.03921569 0.03921569\n",
      "    0.0627451 ]]\n",
      "\n",
      "  [[0.3372549  0.35686275 0.38431373 ... 0.3372549  0.35686275\n",
      "    0.38431373]\n",
      "   [0.05490196 0.05098039 0.06666667 ... 0.05490196 0.05098039\n",
      "    0.06666667]\n",
      "   [0.01960784 0.01568628 0.02745098 ... 0.01960784 0.01568628\n",
      "    0.02745098]\n",
      "   ...\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.04705882 0.04313726 0.07058824 ... 0.04705882 0.04313726\n",
      "    0.07058824]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6156863  0.63529414 0.68235296 ... 0.6156863  0.63529414\n",
      "    0.68235296]\n",
      "   [0.62352943 0.64705884 0.6901961  ... 0.62352943 0.64705884\n",
      "    0.6901961 ]\n",
      "   [0.60784316 0.61960787 0.62352943 ... 0.60784316 0.61960787\n",
      "    0.62352943]\n",
      "   ...\n",
      "   [0.49411765 0.53333336 0.65882355 ... 0.49411765 0.53333336\n",
      "    0.65882355]\n",
      "   [0.5686275  0.6        0.69803923 ... 0.5686275  0.6\n",
      "    0.69803923]\n",
      "   [0.5882353  0.6156863  0.7058824  ... 0.5882353  0.6156863\n",
      "    0.7058824 ]]\n",
      "\n",
      "  [[0.61960787 0.63529414 0.68235296 ... 0.61960787 0.63529414\n",
      "    0.68235296]\n",
      "   [0.61960787 0.6431373  0.69411767 ... 0.61960787 0.6431373\n",
      "    0.69411767]\n",
      "   [0.627451   0.654902   0.7019608  ... 0.627451   0.654902\n",
      "    0.7019608 ]\n",
      "   ...\n",
      "   [0.5254902  0.5647059  0.6745098  ... 0.5254902  0.5647059\n",
      "    0.6745098 ]\n",
      "   [0.56078434 0.5921569  0.69411767 ... 0.56078434 0.5921569\n",
      "    0.69411767]\n",
      "   [0.5921569  0.61960787 0.7058824  ... 0.5921569  0.61960787\n",
      "    0.7058824 ]]\n",
      "\n",
      "  [[0.627451   0.6431373  0.6901961  ... 0.627451   0.6431373\n",
      "    0.6901961 ]\n",
      "   [0.61960787 0.6431373  0.69411767 ... 0.61960787 0.6431373\n",
      "    0.69411767]\n",
      "   [0.6392157  0.6627451  0.70980394 ... 0.6392157  0.6627451\n",
      "    0.70980394]\n",
      "   ...\n",
      "   [0.5529412  0.5882353  0.68235296 ... 0.5529412  0.5882353\n",
      "    0.68235296]\n",
      "   [0.54901963 0.58431375 0.6901961  ... 0.54901963 0.58431375\n",
      "    0.6901961 ]\n",
      "   [0.5882353  0.61960787 0.7058824  ... 0.5882353  0.61960787\n",
      "    0.7058824 ]]]\n",
      "\n",
      "\n",
      " [[[0.03529412 0.03529412 0.00784314 ... 0.03529412 0.03529412\n",
      "    0.00784314]\n",
      "   [0.02352941 0.02352941 0.00784314 ... 0.02352941 0.02352941\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01176471 0.00392157 ... 0.01568628 0.01176471\n",
      "    0.00392157]\n",
      "   ...\n",
      "   [0.         0.00392157 0.00392157 ... 0.         0.00392157\n",
      "    0.00392157]\n",
      "   [0.00392157 0.00784314 0.00784314 ... 0.00392157 0.00784314\n",
      "    0.00784314]\n",
      "   [0.03529412 0.03921569 0.02745098 ... 0.03529412 0.03921569\n",
      "    0.02745098]]\n",
      "\n",
      "  [[0.03529412 0.03137255 0.01568628 ... 0.03529412 0.03137255\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03529412 0.01568628 ... 0.03529412 0.03529412\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03137255 0.01176471 ... 0.03529412 0.03137255\n",
      "    0.01176471]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157 ... 0.00392157 0.00392157\n",
      "    0.00392157]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]\n",
      "   [0.01568628 0.01960784 0.01176471 ... 0.01568628 0.01960784\n",
      "    0.01176471]]\n",
      "\n",
      "  [[0.10196079 0.09019608 0.05490196 ... 0.10196079 0.09019608\n",
      "    0.05490196]\n",
      "   [0.12941177 0.10588235 0.05882353 ... 0.12941177 0.10588235\n",
      "    0.05882353]\n",
      "   [0.1764706  0.14509805 0.09411765 ... 0.1764706  0.14509805\n",
      "    0.09411765]\n",
      "   ...\n",
      "   [0.00784314 0.01176471 0.00784314 ... 0.00784314 0.01176471\n",
      "    0.00784314]\n",
      "   [0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.7372549  0.69803923 0.6313726  ... 0.7372549  0.69803923\n",
      "    0.6313726 ]\n",
      "   [0.74509805 0.7058824  0.63529414 ... 0.74509805 0.7058824\n",
      "    0.63529414]\n",
      "   [0.73333335 0.69411767 0.61960787 ... 0.73333335 0.69411767\n",
      "    0.61960787]\n",
      "   ...\n",
      "   [0.5882353  0.5568628  0.4862745  ... 0.5882353  0.5568628\n",
      "    0.4862745 ]\n",
      "   [0.5686275  0.5294118  0.4627451  ... 0.5686275  0.5294118\n",
      "    0.4627451 ]\n",
      "   [0.5686275  0.5294118  0.45882353 ... 0.5686275  0.5294118\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.77254903 0.7294118  0.6666667  ... 0.77254903 0.7294118\n",
      "    0.6666667 ]\n",
      "   [0.7607843  0.72156864 0.65882355 ... 0.7607843  0.72156864\n",
      "    0.65882355]\n",
      "   [0.7529412  0.70980394 0.6431373  ... 0.7529412  0.70980394\n",
      "    0.6431373 ]\n",
      "   ...\n",
      "   [0.5803922  0.54509807 0.47058824 ... 0.5803922  0.54509807\n",
      "    0.47058824]\n",
      "   [0.58431375 0.5411765  0.46666667 ... 0.58431375 0.5411765\n",
      "    0.46666667]\n",
      "   [0.5764706  0.53333336 0.45882353 ... 0.5764706  0.53333336\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.7882353  0.7490196  0.68235296 ... 0.7882353  0.7490196\n",
      "    0.68235296]\n",
      "   [0.7764706  0.7372549  0.6745098  ... 0.7764706  0.7372549\n",
      "    0.6745098 ]\n",
      "   [0.7607843  0.7176471  0.654902   ... 0.7607843  0.7176471\n",
      "    0.654902  ]\n",
      "   ...\n",
      "   [0.5882353  0.54509807 0.47058824 ... 0.5882353  0.54509807\n",
      "    0.47058824]\n",
      "   [0.59607846 0.5529412  0.4745098  ... 0.59607846 0.5529412\n",
      "    0.4745098 ]\n",
      "   [0.59607846 0.5568628  0.4862745  ... 0.59607846 0.5568628\n",
      "    0.4862745 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.49803922 0.49803922 0.5176471  ... 0.49803922 0.49803922\n",
      "    0.5176471 ]\n",
      "   [0.5058824  0.5019608  0.52156866 ... 0.5058824  0.5019608\n",
      "    0.52156866]\n",
      "   [0.50980395 0.5058824  0.5254902  ... 0.50980395 0.5058824\n",
      "    0.5254902 ]\n",
      "   ...\n",
      "   [0.5686275  0.57254905 0.5882353  ... 0.5686275  0.57254905\n",
      "    0.5882353 ]\n",
      "   [0.57254905 0.5764706  0.5921569  ... 0.57254905 0.5764706\n",
      "    0.5921569 ]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]]\n",
      "\n",
      "  [[0.50980395 0.50980395 0.5254902  ... 0.50980395 0.50980395\n",
      "    0.5254902 ]\n",
      "   [0.5137255  0.5137255  0.5294118  ... 0.5137255  0.5137255\n",
      "    0.5294118 ]\n",
      "   [0.5176471  0.5176471  0.53333336 ... 0.5176471  0.5176471\n",
      "    0.53333336]\n",
      "   ...\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5803922  0.58431375 0.6        ... 0.5803922  0.58431375\n",
      "    0.6       ]]\n",
      "\n",
      "  [[0.5176471  0.5176471  0.5372549  ... 0.5176471  0.5176471\n",
      "    0.5372549 ]\n",
      "   [0.52156866 0.5254902  0.5411765  ... 0.52156866 0.5254902\n",
      "    0.5411765 ]\n",
      "   [0.5176471  0.52156866 0.5372549  ... 0.5176471  0.52156866\n",
      "    0.5372549 ]\n",
      "   ...\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.5882353  0.5921569  0.60784316 ... 0.5882353  0.5921569\n",
      "    0.60784316]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8901961  0.8392157  0.8039216  ... 0.8901961  0.8392157\n",
      "    0.8039216 ]\n",
      "   [0.91764706 0.8745098  0.8392157  ... 0.91764706 0.8745098\n",
      "    0.8392157 ]\n",
      "   [0.8627451  0.79607844 0.7490196  ... 0.8627451  0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9372549  0.90588236 0.8784314  ... 0.9372549  0.90588236\n",
      "    0.8784314 ]\n",
      "   [0.90588236 0.85882354 0.8235294  ... 0.90588236 0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.87058824 0.8117647  0.7764706  ... 0.87058824 0.8117647\n",
      "    0.7764706 ]]\n",
      "\n",
      "  [[0.88235295 0.8235294  0.78431374 ... 0.88235295 0.8235294\n",
      "    0.78431374]\n",
      "   [0.89411765 0.8392157  0.8        ... 0.89411765 0.8392157\n",
      "    0.8       ]\n",
      "   [0.87058824 0.8039216  0.7607843  ... 0.87058824 0.8039216\n",
      "    0.7607843 ]\n",
      "   ...\n",
      "   [0.92156863 0.8862745  0.8509804  ... 0.92156863 0.8862745\n",
      "    0.8509804 ]\n",
      "   [0.9098039  0.87058824 0.8352941  ... 0.9098039  0.87058824\n",
      "    0.8352941 ]\n",
      "   [0.8745098  0.8156863  0.77254903 ... 0.8745098  0.8156863\n",
      "    0.77254903]]\n",
      "\n",
      "  [[0.8627451  0.7921569  0.7411765  ... 0.8627451  0.7921569\n",
      "    0.7411765 ]\n",
      "   [0.8666667  0.8039216  0.7529412  ... 0.8666667  0.8039216\n",
      "    0.7529412 ]\n",
      "   [0.85882354 0.79607844 0.7490196  ... 0.85882354 0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9098039  0.85882354 0.8235294  ... 0.9098039  0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.90588236 0.85490197 0.8156863  ... 0.90588236 0.85490197\n",
      "    0.8156863 ]\n",
      "   [0.8784314  0.8156863  0.7647059  ... 0.8784314  0.8156863\n",
      "    0.7647059 ]]]\n",
      "\n",
      "\n",
      " [[[0.65882355 0.5411765  0.34901962 ... 0.65882355 0.5411765\n",
      "    0.34901962]\n",
      "   [0.79607844 0.6862745  0.44313726 ... 0.79607844 0.6862745\n",
      "    0.44313726]\n",
      "   [0.84705883 0.7372549  0.4862745  ... 0.84705883 0.7372549\n",
      "    0.4862745 ]\n",
      "   ...\n",
      "   [0.9019608  0.78431374 0.57254905 ... 0.9019608  0.78431374\n",
      "    0.57254905]\n",
      "   [0.8784314  0.77254903 0.5803922  ... 0.8784314  0.77254903\n",
      "    0.5803922 ]\n",
      "   [0.8666667  0.7607843  0.58431375 ... 0.8666667  0.7607843\n",
      "    0.58431375]]\n",
      "\n",
      "  [[0.6313726  0.5058824  0.27450982 ... 0.6313726  0.5058824\n",
      "    0.27450982]\n",
      "   [0.7372549  0.61960787 0.3764706  ... 0.7372549  0.61960787\n",
      "    0.3764706 ]\n",
      "   [0.83137256 0.72156864 0.4627451  ... 0.83137256 0.72156864\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.8784314  0.7607843  0.5568628  ... 0.8784314  0.7607843\n",
      "    0.5568628 ]\n",
      "   [0.85882354 0.7490196  0.56078434 ... 0.85882354 0.7490196\n",
      "    0.56078434]\n",
      "   [0.85490197 0.7529412  0.5647059  ... 0.85490197 0.7529412\n",
      "    0.5647059 ]]\n",
      "\n",
      "  [[0.6        0.46666667 0.22352941 ... 0.6        0.46666667\n",
      "    0.22352941]\n",
      "   [0.6313726  0.49803922 0.27058825 ... 0.6313726  0.49803922\n",
      "    0.27058825]\n",
      "   [0.80784315 0.69803923 0.44705883 ... 0.80784315 0.69803923\n",
      "    0.44705883]\n",
      "   ...\n",
      "   [0.8352941  0.72156864 0.5372549  ... 0.8352941  0.72156864\n",
      "    0.5372549 ]\n",
      "   [0.83137256 0.7254902  0.5411765  ... 0.83137256 0.7254902\n",
      "    0.5411765 ]\n",
      "   [0.8352941  0.73333335 0.5372549  ... 0.8352941  0.73333335\n",
      "    0.5372549 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6039216  0.3764706  0.27058825 ... 0.6039216  0.3764706\n",
      "    0.27058825]\n",
      "   [0.627451   0.40784314 0.31764707 ... 0.627451   0.40784314\n",
      "    0.31764707]\n",
      "   [0.67058825 0.47058824 0.4        ... 0.67058825 0.47058824\n",
      "    0.4       ]\n",
      "   ...\n",
      "   [0.76862746 0.58431375 0.4745098  ... 0.76862746 0.58431375\n",
      "    0.4745098 ]\n",
      "   [0.7058824  0.49803922 0.38431373 ... 0.7058824  0.49803922\n",
      "    0.38431373]\n",
      "   [0.65882355 0.44313726 0.32156864 ... 0.65882355 0.44313726\n",
      "    0.32156864]]\n",
      "\n",
      "  [[0.6        0.38039216 0.26666668 ... 0.6        0.38039216\n",
      "    0.26666668]\n",
      "   [0.63529414 0.41568628 0.32156864 ... 0.63529414 0.41568628\n",
      "    0.32156864]\n",
      "   [0.7372549  0.56078434 0.4627451  ... 0.7372549  0.56078434\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.83137256 0.654902   0.54901963 ... 0.83137256 0.654902\n",
      "    0.54901963]\n",
      "   [0.77254903 0.5647059  0.44313726 ... 0.77254903 0.5647059\n",
      "    0.44313726]\n",
      "   [0.7176471  0.5019608  0.38039216 ... 0.7176471  0.5019608\n",
      "    0.38039216]]\n",
      "\n",
      "  [[0.627451   0.41960785 0.29411766 ... 0.627451   0.41960785\n",
      "    0.29411766]\n",
      "   [0.68235296 0.46666667 0.3882353  ... 0.68235296 0.46666667\n",
      "    0.3882353 ]\n",
      "   [0.8509804  0.7294118  0.49019608 ... 0.8509804  0.7294118\n",
      "    0.49019608]\n",
      "   ...\n",
      "   [0.89411765 0.72156864 0.6117647  ... 0.89411765 0.72156864\n",
      "    0.6117647 ]\n",
      "   [0.8392157  0.627451   0.50980395 ... 0.8392157  0.627451\n",
      "    0.50980395]\n",
      "   [0.76862746 0.56078434 0.43529412 ... 0.76862746 0.56078434\n",
      "    0.43529412]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.99215686 0.99215686 0.99215686 ... 0.99215686 0.99215686\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6509804  0.6117647  0.60784316 ... 0.6509804  0.6117647\n",
      "    0.60784316]\n",
      "   [0.6431373  0.6        0.5921569  ... 0.6431373  0.6\n",
      "    0.5921569 ]\n",
      "   [0.63529414 0.5921569  0.5803922  ... 0.63529414 0.5921569\n",
      "    0.5803922 ]\n",
      "   ...\n",
      "   [0.80784315 0.69803923 0.67058825 ... 0.80784315 0.69803923\n",
      "    0.67058825]\n",
      "   [0.80784315 0.7176471  0.6901961  ... 0.80784315 0.7176471\n",
      "    0.6901961 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]]\n",
      "\n",
      "  [[0.64705884 0.59607846 0.59607846 ... 0.64705884 0.59607846\n",
      "    0.59607846]\n",
      "   [0.63529414 0.5921569  0.58431375 ... 0.63529414 0.5921569\n",
      "    0.58431375]\n",
      "   [0.62352943 0.5803922  0.57254905 ... 0.62352943 0.5803922\n",
      "    0.57254905]\n",
      "   ...\n",
      "   [0.8        0.69803923 0.6745098  ... 0.8        0.69803923\n",
      "    0.6745098 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]\n",
      "   [0.7921569  0.7019608  0.68235296 ... 0.7921569  0.7019608\n",
      "    0.68235296]]\n",
      "\n",
      "  [[0.6431373  0.59607846 0.5921569  ... 0.6431373  0.59607846\n",
      "    0.5921569 ]\n",
      "   [0.6313726  0.5882353  0.5803922  ... 0.6313726  0.5882353\n",
      "    0.5803922 ]\n",
      "   [0.627451   0.57254905 0.5686275  ... 0.627451   0.57254905\n",
      "    0.5686275 ]\n",
      "   ...\n",
      "   [0.7921569  0.6901961  0.6784314  ... 0.7921569  0.6901961\n",
      "    0.6784314 ]\n",
      "   [0.80784315 0.7058824  0.6901961  ... 0.80784315 0.7058824\n",
      "    0.6901961 ]\n",
      "   [0.80784315 0.7176471  0.7019608  ... 0.80784315 0.7176471\n",
      "    0.7019608 ]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (77, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (77, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 3.5412 - accuracy: 0.0000e+0032\n",
      "32\n",
      "[[[[243 243 243 ... 243 243 243]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  [[242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  [[242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[243 242 244 ... 243 242 244]\n",
      "   [245 242 248 ... 245 242 248]\n",
      "   [250 250 253 ... 250 250 253]\n",
      "   ...\n",
      "   [214 218 239 ... 214 218 239]\n",
      "   [205 209 233 ... 205 209 233]\n",
      "   [213 215 234 ... 213 215 234]]\n",
      "\n",
      "  [[238 236 241 ... 238 236 241]\n",
      "   [235 230 238 ... 235 230 238]\n",
      "   [240 236 243 ... 240 236 243]\n",
      "   ...\n",
      "   [210 214 238 ... 210 214 238]\n",
      "   [202 206 232 ... 202 206 232]\n",
      "   [206 208 231 ... 206 208 231]]\n",
      "\n",
      "  [[226 219 227 ... 226 219 227]\n",
      "   [221 211 220 ... 221 211 220]\n",
      "   [219 210 222 ... 219 210 222]\n",
      "   ...\n",
      "   [207 212 237 ... 207 212 237]\n",
      "   [207 212 236 ... 207 212 236]\n",
      "   [207 210 233 ... 207 210 233]]]\n",
      "\n",
      "\n",
      " [[[  9   9   2 ...   9   9   2]\n",
      "   [  6   6   2 ...   6   6   2]\n",
      "   [  4   3   1 ...   4   3   1]\n",
      "   ...\n",
      "   [  0   1   1 ...   0   1   1]\n",
      "   [  1   2   2 ...   1   2   2]\n",
      "   [  9  10   7 ...   9  10   7]]\n",
      "\n",
      "  [[  9   8   4 ...   9   8   4]\n",
      "   [  9   9   4 ...   9   9   4]\n",
      "   [  9   8   3 ...   9   8   3]\n",
      "   ...\n",
      "   [  1   1   1 ...   1   1   1]\n",
      "   [  4   4   3 ...   4   4   3]\n",
      "   [  4   5   3 ...   4   5   3]]\n",
      "\n",
      "  [[ 26  23  14 ...  26  23  14]\n",
      "   [ 33  27  15 ...  33  27  15]\n",
      "   [ 45  37  24 ...  45  37  24]\n",
      "   ...\n",
      "   [  2   3   2 ...   2   3   2]\n",
      "   [  2   2   2 ...   2   2   2]\n",
      "   [  4   4   3 ...   4   4   3]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[188 178 161 ... 188 178 161]\n",
      "   [190 180 162 ... 190 180 162]\n",
      "   [187 177 158 ... 187 177 158]\n",
      "   ...\n",
      "   [150 142 124 ... 150 142 124]\n",
      "   [145 135 118 ... 145 135 118]\n",
      "   [145 135 117 ... 145 135 117]]\n",
      "\n",
      "  [[197 186 170 ... 197 186 170]\n",
      "   [194 184 168 ... 194 184 168]\n",
      "   [192 181 164 ... 192 181 164]\n",
      "   ...\n",
      "   [148 139 120 ... 148 139 120]\n",
      "   [149 138 119 ... 149 138 119]\n",
      "   [147 136 117 ... 147 136 117]]\n",
      "\n",
      "  [[201 191 174 ... 201 191 174]\n",
      "   [198 188 172 ... 198 188 172]\n",
      "   [194 183 167 ... 194 183 167]\n",
      "   ...\n",
      "   [150 139 120 ... 150 139 120]\n",
      "   [152 141 121 ... 152 141 121]\n",
      "   [152 142 124 ... 152 142 124]]]\n",
      "\n",
      "\n",
      " [[[198 203 206 ... 198 203 206]\n",
      "   [192 196 197 ... 192 196 197]\n",
      "   [154 154 149 ... 154 154 149]\n",
      "   ...\n",
      "   [201 208 206 ... 201 208 206]\n",
      "   [199 207 205 ... 199 207 205]\n",
      "   [192 202 201 ... 192 202 201]]\n",
      "\n",
      "  [[200 204 207 ... 200 204 207]\n",
      "   [184 187 188 ... 184 187 188]\n",
      "   [109 107 101 ... 109 107 101]\n",
      "   ...\n",
      "   [199 206 206 ... 199 206 206]\n",
      "   [201 211 209 ... 201 211 209]\n",
      "   [196 207 205 ... 196 207 205]]\n",
      "\n",
      "  [[207 211 215 ... 207 211 215]\n",
      "   [168 171 170 ... 168 171 170]\n",
      "   [ 62  58  50 ...  62  58  50]\n",
      "   ...\n",
      "   [193 200 200 ... 193 200 200]\n",
      "   [198 207 206 ... 198 207 206]\n",
      "   [198 209 207 ... 198 209 207]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[217 216 217 ... 217 216 217]\n",
      "   [217 214 212 ... 217 214 212]\n",
      "   [223 219 217 ... 223 219 217]\n",
      "   ...\n",
      "   [165 114 111 ... 165 114 111]\n",
      "   [174 125 121 ... 174 125 121]\n",
      "   [175 127 123 ... 175 127 123]]\n",
      "\n",
      "  [[218 217 217 ... 218 217 217]\n",
      "   [218 216 217 ... 218 216 217]\n",
      "   [222 220 218 ... 222 220 218]\n",
      "   ...\n",
      "   [163 113 109 ... 163 113 109]\n",
      "   [171 123 119 ... 171 123 119]\n",
      "   [171 124 119 ... 171 124 119]]\n",
      "\n",
      "  [[216 216 216 ... 216 216 216]\n",
      "   [219 218 217 ... 219 218 217]\n",
      "   [222 221 220 ... 222 221 220]\n",
      "   ...\n",
      "   [162 113 109 ... 162 113 109]\n",
      "   [167 119 115 ... 167 119 115]\n",
      "   [166 120 115 ... 166 120 115]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (67, 40, 30, 9)\n",
      "67 train samples\n",
      "31 test samples\n",
      "y_train shape: (67, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_28\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_29 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_868 (Conv2D)             (None, 40, 30, 16)   1312        input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_784 (BatchN (None, 40, 30, 16)   64          conv2d_868[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_784 (Activation)     (None, 40, 30, 16)   0           batch_normalization_784[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_869 (Conv2D)             (None, 40, 30, 16)   272         activation_784[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_785 (BatchN (None, 40, 30, 16)   64          conv2d_869[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_785 (Activation)     (None, 40, 30, 16)   0           batch_normalization_785[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_870 (Conv2D)             (None, 40, 30, 16)   2320        activation_785[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_786 (BatchN (None, 40, 30, 16)   64          conv2d_870[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_786 (Activation)     (None, 40, 30, 16)   0           batch_normalization_786[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_872 (Conv2D)             (None, 40, 30, 64)   1088        activation_784[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_871 (Conv2D)             (None, 40, 30, 64)   1088        activation_786[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_252 (Add)                   (None, 40, 30, 64)   0           conv2d_872[0][0]                 \n",
      "                                                                 conv2d_871[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_787 (BatchN (None, 40, 30, 64)   256         add_252[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_787 (Activation)     (None, 40, 30, 64)   0           batch_normalization_787[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_873 (Conv2D)             (None, 40, 30, 16)   1040        activation_787[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_788 (BatchN (None, 40, 30, 16)   64          conv2d_873[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_788 (Activation)     (None, 40, 30, 16)   0           batch_normalization_788[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_874 (Conv2D)             (None, 40, 30, 16)   2320        activation_788[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_789 (BatchN (None, 40, 30, 16)   64          conv2d_874[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_789 (Activation)     (None, 40, 30, 16)   0           batch_normalization_789[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_875 (Conv2D)             (None, 40, 30, 64)   1088        activation_789[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_253 (Add)                   (None, 40, 30, 64)   0           add_252[0][0]                    \n",
      "                                                                 conv2d_875[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_790 (BatchN (None, 40, 30, 64)   256         add_253[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_790 (Activation)     (None, 40, 30, 64)   0           batch_normalization_790[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_876 (Conv2D)             (None, 40, 30, 16)   1040        activation_790[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_791 (BatchN (None, 40, 30, 16)   64          conv2d_876[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_791 (Activation)     (None, 40, 30, 16)   0           batch_normalization_791[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_877 (Conv2D)             (None, 40, 30, 16)   2320        activation_791[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_792 (BatchN (None, 40, 30, 16)   64          conv2d_877[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_792 (Activation)     (None, 40, 30, 16)   0           batch_normalization_792[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_878 (Conv2D)             (None, 40, 30, 64)   1088        activation_792[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_254 (Add)                   (None, 40, 30, 64)   0           add_253[0][0]                    \n",
      "                                                                 conv2d_878[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_793 (BatchN (None, 40, 30, 64)   256         add_254[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_793 (Activation)     (None, 40, 30, 64)   0           batch_normalization_793[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_879 (Conv2D)             (None, 20, 15, 64)   4160        activation_793[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_794 (BatchN (None, 20, 15, 64)   256         conv2d_879[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_794 (Activation)     (None, 20, 15, 64)   0           batch_normalization_794[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_880 (Conv2D)             (None, 20, 15, 64)   36928       activation_794[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_795 (BatchN (None, 20, 15, 64)   256         conv2d_880[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_795 (Activation)     (None, 20, 15, 64)   0           batch_normalization_795[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_882 (Conv2D)             (None, 20, 15, 128)  8320        add_254[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_881 (Conv2D)             (None, 20, 15, 128)  8320        activation_795[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_255 (Add)                   (None, 20, 15, 128)  0           conv2d_882[0][0]                 \n",
      "                                                                 conv2d_881[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_796 (BatchN (None, 20, 15, 128)  512         add_255[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_796 (Activation)     (None, 20, 15, 128)  0           batch_normalization_796[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_883 (Conv2D)             (None, 20, 15, 64)   8256        activation_796[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_797 (BatchN (None, 20, 15, 64)   256         conv2d_883[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_797 (Activation)     (None, 20, 15, 64)   0           batch_normalization_797[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_884 (Conv2D)             (None, 20, 15, 64)   36928       activation_797[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_798 (BatchN (None, 20, 15, 64)   256         conv2d_884[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_798 (Activation)     (None, 20, 15, 64)   0           batch_normalization_798[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_885 (Conv2D)             (None, 20, 15, 128)  8320        activation_798[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_256 (Add)                   (None, 20, 15, 128)  0           add_255[0][0]                    \n",
      "                                                                 conv2d_885[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_799 (BatchN (None, 20, 15, 128)  512         add_256[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_799 (Activation)     (None, 20, 15, 128)  0           batch_normalization_799[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_886 (Conv2D)             (None, 20, 15, 64)   8256        activation_799[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_800 (BatchN (None, 20, 15, 64)   256         conv2d_886[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_800 (Activation)     (None, 20, 15, 64)   0           batch_normalization_800[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_887 (Conv2D)             (None, 20, 15, 64)   36928       activation_800[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_801 (BatchN (None, 20, 15, 64)   256         conv2d_887[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_801 (Activation)     (None, 20, 15, 64)   0           batch_normalization_801[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_888 (Conv2D)             (None, 20, 15, 128)  8320        activation_801[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_257 (Add)                   (None, 20, 15, 128)  0           add_256[0][0]                    \n",
      "                                                                 conv2d_888[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_802 (BatchN (None, 20, 15, 128)  512         add_257[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_802 (Activation)     (None, 20, 15, 128)  0           batch_normalization_802[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_889 (Conv2D)             (None, 10, 8, 128)   16512       activation_802[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_803 (BatchN (None, 10, 8, 128)   512         conv2d_889[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_803 (Activation)     (None, 10, 8, 128)   0           batch_normalization_803[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_890 (Conv2D)             (None, 10, 8, 128)   147584      activation_803[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_804 (BatchN (None, 10, 8, 128)   512         conv2d_890[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_804 (Activation)     (None, 10, 8, 128)   0           batch_normalization_804[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_892 (Conv2D)             (None, 10, 8, 256)   33024       add_257[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_891 (Conv2D)             (None, 10, 8, 256)   33024       activation_804[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_258 (Add)                   (None, 10, 8, 256)   0           conv2d_892[0][0]                 \n",
      "                                                                 conv2d_891[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_805 (BatchN (None, 10, 8, 256)   1024        add_258[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_805 (Activation)     (None, 10, 8, 256)   0           batch_normalization_805[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_893 (Conv2D)             (None, 10, 8, 128)   32896       activation_805[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_806 (BatchN (None, 10, 8, 128)   512         conv2d_893[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_806 (Activation)     (None, 10, 8, 128)   0           batch_normalization_806[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_894 (Conv2D)             (None, 10, 8, 128)   147584      activation_806[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_807 (BatchN (None, 10, 8, 128)   512         conv2d_894[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_807 (Activation)     (None, 10, 8, 128)   0           batch_normalization_807[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_895 (Conv2D)             (None, 10, 8, 256)   33024       activation_807[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_259 (Add)                   (None, 10, 8, 256)   0           add_258[0][0]                    \n",
      "                                                                 conv2d_895[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_808 (BatchN (None, 10, 8, 256)   1024        add_259[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_808 (Activation)     (None, 10, 8, 256)   0           batch_normalization_808[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_896 (Conv2D)             (None, 10, 8, 128)   32896       activation_808[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_809 (BatchN (None, 10, 8, 128)   512         conv2d_896[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_809 (Activation)     (None, 10, 8, 128)   0           batch_normalization_809[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_897 (Conv2D)             (None, 10, 8, 128)   147584      activation_809[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_810 (BatchN (None, 10, 8, 128)   512         conv2d_897[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_810 (Activation)     (None, 10, 8, 128)   0           batch_normalization_810[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_898 (Conv2D)             (None, 10, 8, 256)   33024       activation_810[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_260 (Add)                   (None, 10, 8, 256)   0           add_259[0][0]                    \n",
      "                                                                 conv2d_898[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_811 (BatchN (None, 10, 8, 256)   1024        add_260[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_811 (Activation)     (None, 10, 8, 256)   0           batch_normalization_811[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_28 (AveragePo (None, 1, 1, 256)    0           activation_811[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 256)          0           average_pooling2d_28[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 10)           2570        flatten_28[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[0.9529412  0.9529412  0.9529412  ... 0.9529412  0.9529412\n",
      "    0.9529412 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  [[0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  [[0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.9529412  0.9490196  0.95686275 ... 0.9529412  0.9490196\n",
      "    0.95686275]\n",
      "   [0.9607843  0.9490196  0.972549   ... 0.9607843  0.9490196\n",
      "    0.972549  ]\n",
      "   [0.98039216 0.98039216 0.99215686 ... 0.98039216 0.98039216\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [0.8392157  0.85490197 0.9372549  ... 0.8392157  0.85490197\n",
      "    0.9372549 ]\n",
      "   [0.8039216  0.81960785 0.9137255  ... 0.8039216  0.81960785\n",
      "    0.9137255 ]\n",
      "   [0.8352941  0.84313726 0.91764706 ... 0.8352941  0.84313726\n",
      "    0.91764706]]\n",
      "\n",
      "  [[0.93333334 0.9254902  0.94509804 ... 0.93333334 0.9254902\n",
      "    0.94509804]\n",
      "   [0.92156863 0.9019608  0.93333334 ... 0.92156863 0.9019608\n",
      "    0.93333334]\n",
      "   [0.9411765  0.9254902  0.9529412  ... 0.9411765  0.9254902\n",
      "    0.9529412 ]\n",
      "   ...\n",
      "   [0.8235294  0.8392157  0.93333334 ... 0.8235294  0.8392157\n",
      "    0.93333334]\n",
      "   [0.7921569  0.80784315 0.9098039  ... 0.7921569  0.80784315\n",
      "    0.9098039 ]\n",
      "   [0.80784315 0.8156863  0.90588236 ... 0.80784315 0.8156863\n",
      "    0.90588236]]\n",
      "\n",
      "  [[0.8862745  0.85882354 0.8901961  ... 0.8862745  0.85882354\n",
      "    0.8901961 ]\n",
      "   [0.8666667  0.827451   0.8627451  ... 0.8666667  0.827451\n",
      "    0.8627451 ]\n",
      "   [0.85882354 0.8235294  0.87058824 ... 0.85882354 0.8235294\n",
      "    0.87058824]\n",
      "   ...\n",
      "   [0.8117647  0.83137256 0.92941177 ... 0.8117647  0.83137256\n",
      "    0.92941177]\n",
      "   [0.8117647  0.83137256 0.9254902  ... 0.8117647  0.83137256\n",
      "    0.9254902 ]\n",
      "   [0.8117647  0.8235294  0.9137255  ... 0.8117647  0.8235294\n",
      "    0.9137255 ]]]\n",
      "\n",
      "\n",
      " [[[0.03529412 0.03529412 0.00784314 ... 0.03529412 0.03529412\n",
      "    0.00784314]\n",
      "   [0.02352941 0.02352941 0.00784314 ... 0.02352941 0.02352941\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01176471 0.00392157 ... 0.01568628 0.01176471\n",
      "    0.00392157]\n",
      "   ...\n",
      "   [0.         0.00392157 0.00392157 ... 0.         0.00392157\n",
      "    0.00392157]\n",
      "   [0.00392157 0.00784314 0.00784314 ... 0.00392157 0.00784314\n",
      "    0.00784314]\n",
      "   [0.03529412 0.03921569 0.02745098 ... 0.03529412 0.03921569\n",
      "    0.02745098]]\n",
      "\n",
      "  [[0.03529412 0.03137255 0.01568628 ... 0.03529412 0.03137255\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03529412 0.01568628 ... 0.03529412 0.03529412\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03137255 0.01176471 ... 0.03529412 0.03137255\n",
      "    0.01176471]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157 ... 0.00392157 0.00392157\n",
      "    0.00392157]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]\n",
      "   [0.01568628 0.01960784 0.01176471 ... 0.01568628 0.01960784\n",
      "    0.01176471]]\n",
      "\n",
      "  [[0.10196079 0.09019608 0.05490196 ... 0.10196079 0.09019608\n",
      "    0.05490196]\n",
      "   [0.12941177 0.10588235 0.05882353 ... 0.12941177 0.10588235\n",
      "    0.05882353]\n",
      "   [0.1764706  0.14509805 0.09411765 ... 0.1764706  0.14509805\n",
      "    0.09411765]\n",
      "   ...\n",
      "   [0.00784314 0.01176471 0.00784314 ... 0.00784314 0.01176471\n",
      "    0.00784314]\n",
      "   [0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.7372549  0.69803923 0.6313726  ... 0.7372549  0.69803923\n",
      "    0.6313726 ]\n",
      "   [0.74509805 0.7058824  0.63529414 ... 0.74509805 0.7058824\n",
      "    0.63529414]\n",
      "   [0.73333335 0.69411767 0.61960787 ... 0.73333335 0.69411767\n",
      "    0.61960787]\n",
      "   ...\n",
      "   [0.5882353  0.5568628  0.4862745  ... 0.5882353  0.5568628\n",
      "    0.4862745 ]\n",
      "   [0.5686275  0.5294118  0.4627451  ... 0.5686275  0.5294118\n",
      "    0.4627451 ]\n",
      "   [0.5686275  0.5294118  0.45882353 ... 0.5686275  0.5294118\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.77254903 0.7294118  0.6666667  ... 0.77254903 0.7294118\n",
      "    0.6666667 ]\n",
      "   [0.7607843  0.72156864 0.65882355 ... 0.7607843  0.72156864\n",
      "    0.65882355]\n",
      "   [0.7529412  0.70980394 0.6431373  ... 0.7529412  0.70980394\n",
      "    0.6431373 ]\n",
      "   ...\n",
      "   [0.5803922  0.54509807 0.47058824 ... 0.5803922  0.54509807\n",
      "    0.47058824]\n",
      "   [0.58431375 0.5411765  0.46666667 ... 0.58431375 0.5411765\n",
      "    0.46666667]\n",
      "   [0.5764706  0.53333336 0.45882353 ... 0.5764706  0.53333336\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.7882353  0.7490196  0.68235296 ... 0.7882353  0.7490196\n",
      "    0.68235296]\n",
      "   [0.7764706  0.7372549  0.6745098  ... 0.7764706  0.7372549\n",
      "    0.6745098 ]\n",
      "   [0.7607843  0.7176471  0.654902   ... 0.7607843  0.7176471\n",
      "    0.654902  ]\n",
      "   ...\n",
      "   [0.5882353  0.54509807 0.47058824 ... 0.5882353  0.54509807\n",
      "    0.47058824]\n",
      "   [0.59607846 0.5529412  0.4745098  ... 0.59607846 0.5529412\n",
      "    0.4745098 ]\n",
      "   [0.59607846 0.5568628  0.4862745  ... 0.59607846 0.5568628\n",
      "    0.4862745 ]]]\n",
      "\n",
      "\n",
      " [[[0.7764706  0.79607844 0.80784315 ... 0.7764706  0.79607844\n",
      "    0.80784315]\n",
      "   [0.7529412  0.76862746 0.77254903 ... 0.7529412  0.76862746\n",
      "    0.77254903]\n",
      "   [0.6039216  0.6039216  0.58431375 ... 0.6039216  0.6039216\n",
      "    0.58431375]\n",
      "   ...\n",
      "   [0.7882353  0.8156863  0.80784315 ... 0.7882353  0.8156863\n",
      "    0.80784315]\n",
      "   [0.78039217 0.8117647  0.8039216  ... 0.78039217 0.8117647\n",
      "    0.8039216 ]\n",
      "   [0.7529412  0.7921569  0.7882353  ... 0.7529412  0.7921569\n",
      "    0.7882353 ]]\n",
      "\n",
      "  [[0.78431374 0.8        0.8117647  ... 0.78431374 0.8\n",
      "    0.8117647 ]\n",
      "   [0.72156864 0.73333335 0.7372549  ... 0.72156864 0.73333335\n",
      "    0.7372549 ]\n",
      "   [0.42745098 0.41960785 0.39607844 ... 0.42745098 0.41960785\n",
      "    0.39607844]\n",
      "   ...\n",
      "   [0.78039217 0.80784315 0.80784315 ... 0.78039217 0.80784315\n",
      "    0.80784315]\n",
      "   [0.7882353  0.827451   0.81960785 ... 0.7882353  0.827451\n",
      "    0.81960785]\n",
      "   [0.76862746 0.8117647  0.8039216  ... 0.76862746 0.8117647\n",
      "    0.8039216 ]]\n",
      "\n",
      "  [[0.8117647  0.827451   0.84313726 ... 0.8117647  0.827451\n",
      "    0.84313726]\n",
      "   [0.65882355 0.67058825 0.6666667  ... 0.65882355 0.67058825\n",
      "    0.6666667 ]\n",
      "   [0.24313726 0.22745098 0.19607843 ... 0.24313726 0.22745098\n",
      "    0.19607843]\n",
      "   ...\n",
      "   [0.75686276 0.78431374 0.78431374 ... 0.75686276 0.78431374\n",
      "    0.78431374]\n",
      "   [0.7764706  0.8117647  0.80784315 ... 0.7764706  0.8117647\n",
      "    0.80784315]\n",
      "   [0.7764706  0.81960785 0.8117647  ... 0.7764706  0.81960785\n",
      "    0.8117647 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8509804  0.84705883 0.8509804  ... 0.8509804  0.84705883\n",
      "    0.8509804 ]\n",
      "   [0.8509804  0.8392157  0.83137256 ... 0.8509804  0.8392157\n",
      "    0.83137256]\n",
      "   [0.8745098  0.85882354 0.8509804  ... 0.8745098  0.85882354\n",
      "    0.8509804 ]\n",
      "   ...\n",
      "   [0.64705884 0.44705883 0.43529412 ... 0.64705884 0.44705883\n",
      "    0.43529412]\n",
      "   [0.68235296 0.49019608 0.4745098  ... 0.68235296 0.49019608\n",
      "    0.4745098 ]\n",
      "   [0.6862745  0.49803922 0.48235294 ... 0.6862745  0.49803922\n",
      "    0.48235294]]\n",
      "\n",
      "  [[0.85490197 0.8509804  0.8509804  ... 0.85490197 0.8509804\n",
      "    0.8509804 ]\n",
      "   [0.85490197 0.84705883 0.8509804  ... 0.85490197 0.84705883\n",
      "    0.8509804 ]\n",
      "   [0.87058824 0.8627451  0.85490197 ... 0.87058824 0.8627451\n",
      "    0.85490197]\n",
      "   ...\n",
      "   [0.6392157  0.44313726 0.42745098 ... 0.6392157  0.44313726\n",
      "    0.42745098]\n",
      "   [0.67058825 0.48235294 0.46666667 ... 0.67058825 0.48235294\n",
      "    0.46666667]\n",
      "   [0.67058825 0.4862745  0.46666667 ... 0.67058825 0.4862745\n",
      "    0.46666667]]\n",
      "\n",
      "  [[0.84705883 0.84705883 0.84705883 ... 0.84705883 0.84705883\n",
      "    0.84705883]\n",
      "   [0.85882354 0.85490197 0.8509804  ... 0.85882354 0.85490197\n",
      "    0.8509804 ]\n",
      "   [0.87058824 0.8666667  0.8627451  ... 0.87058824 0.8666667\n",
      "    0.8627451 ]\n",
      "   ...\n",
      "   [0.63529414 0.44313726 0.42745098 ... 0.63529414 0.44313726\n",
      "    0.42745098]\n",
      "   [0.654902   0.46666667 0.4509804  ... 0.654902   0.46666667\n",
      "    0.4509804 ]\n",
      "   [0.6509804  0.47058824 0.4509804  ... 0.6509804  0.47058824\n",
      "    0.4509804 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.49803922 0.49803922 0.5176471  ... 0.49803922 0.49803922\n",
      "    0.5176471 ]\n",
      "   [0.5058824  0.5019608  0.52156866 ... 0.5058824  0.5019608\n",
      "    0.52156866]\n",
      "   [0.50980395 0.5058824  0.5254902  ... 0.50980395 0.5058824\n",
      "    0.5254902 ]\n",
      "   ...\n",
      "   [0.5686275  0.57254905 0.5882353  ... 0.5686275  0.57254905\n",
      "    0.5882353 ]\n",
      "   [0.57254905 0.5764706  0.5921569  ... 0.57254905 0.5764706\n",
      "    0.5921569 ]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]]\n",
      "\n",
      "  [[0.50980395 0.50980395 0.5254902  ... 0.50980395 0.50980395\n",
      "    0.5254902 ]\n",
      "   [0.5137255  0.5137255  0.5294118  ... 0.5137255  0.5137255\n",
      "    0.5294118 ]\n",
      "   [0.5176471  0.5176471  0.53333336 ... 0.5176471  0.5176471\n",
      "    0.53333336]\n",
      "   ...\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5803922  0.58431375 0.6        ... 0.5803922  0.58431375\n",
      "    0.6       ]]\n",
      "\n",
      "  [[0.5176471  0.5176471  0.5372549  ... 0.5176471  0.5176471\n",
      "    0.5372549 ]\n",
      "   [0.52156866 0.5254902  0.5411765  ... 0.52156866 0.5254902\n",
      "    0.5411765 ]\n",
      "   [0.5176471  0.52156866 0.5372549  ... 0.5176471  0.52156866\n",
      "    0.5372549 ]\n",
      "   ...\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.5882353  0.5921569  0.60784316 ... 0.5882353  0.5921569\n",
      "    0.60784316]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8901961  0.8392157  0.8039216  ... 0.8901961  0.8392157\n",
      "    0.8039216 ]\n",
      "   [0.91764706 0.8745098  0.8392157  ... 0.91764706 0.8745098\n",
      "    0.8392157 ]\n",
      "   [0.8627451  0.79607844 0.7490196  ... 0.8627451  0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9372549  0.90588236 0.8784314  ... 0.9372549  0.90588236\n",
      "    0.8784314 ]\n",
      "   [0.90588236 0.85882354 0.8235294  ... 0.90588236 0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.87058824 0.8117647  0.7764706  ... 0.87058824 0.8117647\n",
      "    0.7764706 ]]\n",
      "\n",
      "  [[0.88235295 0.8235294  0.78431374 ... 0.88235295 0.8235294\n",
      "    0.78431374]\n",
      "   [0.89411765 0.8392157  0.8        ... 0.89411765 0.8392157\n",
      "    0.8       ]\n",
      "   [0.87058824 0.8039216  0.7607843  ... 0.87058824 0.8039216\n",
      "    0.7607843 ]\n",
      "   ...\n",
      "   [0.92156863 0.8862745  0.8509804  ... 0.92156863 0.8862745\n",
      "    0.8509804 ]\n",
      "   [0.9098039  0.87058824 0.8352941  ... 0.9098039  0.87058824\n",
      "    0.8352941 ]\n",
      "   [0.8745098  0.8156863  0.77254903 ... 0.8745098  0.8156863\n",
      "    0.77254903]]\n",
      "\n",
      "  [[0.8627451  0.7921569  0.7411765  ... 0.8627451  0.7921569\n",
      "    0.7411765 ]\n",
      "   [0.8666667  0.8039216  0.7529412  ... 0.8666667  0.8039216\n",
      "    0.7529412 ]\n",
      "   [0.85882354 0.79607844 0.7490196  ... 0.85882354 0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9098039  0.85882354 0.8235294  ... 0.9098039  0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.90588236 0.85490197 0.8156863  ... 0.90588236 0.85490197\n",
      "    0.8156863 ]\n",
      "   [0.8784314  0.8156863  0.7647059  ... 0.8784314  0.8156863\n",
      "    0.7647059 ]]]\n",
      "\n",
      "\n",
      " [[[0.65882355 0.5411765  0.34901962 ... 0.65882355 0.5411765\n",
      "    0.34901962]\n",
      "   [0.79607844 0.6862745  0.44313726 ... 0.79607844 0.6862745\n",
      "    0.44313726]\n",
      "   [0.84705883 0.7372549  0.4862745  ... 0.84705883 0.7372549\n",
      "    0.4862745 ]\n",
      "   ...\n",
      "   [0.9019608  0.78431374 0.57254905 ... 0.9019608  0.78431374\n",
      "    0.57254905]\n",
      "   [0.8784314  0.77254903 0.5803922  ... 0.8784314  0.77254903\n",
      "    0.5803922 ]\n",
      "   [0.8666667  0.7607843  0.58431375 ... 0.8666667  0.7607843\n",
      "    0.58431375]]\n",
      "\n",
      "  [[0.6313726  0.5058824  0.27450982 ... 0.6313726  0.5058824\n",
      "    0.27450982]\n",
      "   [0.7372549  0.61960787 0.3764706  ... 0.7372549  0.61960787\n",
      "    0.3764706 ]\n",
      "   [0.83137256 0.72156864 0.4627451  ... 0.83137256 0.72156864\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.8784314  0.7607843  0.5568628  ... 0.8784314  0.7607843\n",
      "    0.5568628 ]\n",
      "   [0.85882354 0.7490196  0.56078434 ... 0.85882354 0.7490196\n",
      "    0.56078434]\n",
      "   [0.85490197 0.7529412  0.5647059  ... 0.85490197 0.7529412\n",
      "    0.5647059 ]]\n",
      "\n",
      "  [[0.6        0.46666667 0.22352941 ... 0.6        0.46666667\n",
      "    0.22352941]\n",
      "   [0.6313726  0.49803922 0.27058825 ... 0.6313726  0.49803922\n",
      "    0.27058825]\n",
      "   [0.80784315 0.69803923 0.44705883 ... 0.80784315 0.69803923\n",
      "    0.44705883]\n",
      "   ...\n",
      "   [0.8352941  0.72156864 0.5372549  ... 0.8352941  0.72156864\n",
      "    0.5372549 ]\n",
      "   [0.83137256 0.7254902  0.5411765  ... 0.83137256 0.7254902\n",
      "    0.5411765 ]\n",
      "   [0.8352941  0.73333335 0.5372549  ... 0.8352941  0.73333335\n",
      "    0.5372549 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6039216  0.3764706  0.27058825 ... 0.6039216  0.3764706\n",
      "    0.27058825]\n",
      "   [0.627451   0.40784314 0.31764707 ... 0.627451   0.40784314\n",
      "    0.31764707]\n",
      "   [0.67058825 0.47058824 0.4        ... 0.67058825 0.47058824\n",
      "    0.4       ]\n",
      "   ...\n",
      "   [0.76862746 0.58431375 0.4745098  ... 0.76862746 0.58431375\n",
      "    0.4745098 ]\n",
      "   [0.7058824  0.49803922 0.38431373 ... 0.7058824  0.49803922\n",
      "    0.38431373]\n",
      "   [0.65882355 0.44313726 0.32156864 ... 0.65882355 0.44313726\n",
      "    0.32156864]]\n",
      "\n",
      "  [[0.6        0.38039216 0.26666668 ... 0.6        0.38039216\n",
      "    0.26666668]\n",
      "   [0.63529414 0.41568628 0.32156864 ... 0.63529414 0.41568628\n",
      "    0.32156864]\n",
      "   [0.7372549  0.56078434 0.4627451  ... 0.7372549  0.56078434\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.83137256 0.654902   0.54901963 ... 0.83137256 0.654902\n",
      "    0.54901963]\n",
      "   [0.77254903 0.5647059  0.44313726 ... 0.77254903 0.5647059\n",
      "    0.44313726]\n",
      "   [0.7176471  0.5019608  0.38039216 ... 0.7176471  0.5019608\n",
      "    0.38039216]]\n",
      "\n",
      "  [[0.627451   0.41960785 0.29411766 ... 0.627451   0.41960785\n",
      "    0.29411766]\n",
      "   [0.68235296 0.46666667 0.3882353  ... 0.68235296 0.46666667\n",
      "    0.3882353 ]\n",
      "   [0.8509804  0.7294118  0.49019608 ... 0.8509804  0.7294118\n",
      "    0.49019608]\n",
      "   ...\n",
      "   [0.89411765 0.72156864 0.6117647  ... 0.89411765 0.72156864\n",
      "    0.6117647 ]\n",
      "   [0.8392157  0.627451   0.50980395 ... 0.8392157  0.627451\n",
      "    0.50980395]\n",
      "   [0.76862746 0.56078434 0.43529412 ... 0.76862746 0.56078434\n",
      "    0.43529412]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.99215686 0.99215686 0.99215686 ... 0.99215686 0.99215686\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6509804  0.6117647  0.60784316 ... 0.6509804  0.6117647\n",
      "    0.60784316]\n",
      "   [0.6431373  0.6        0.5921569  ... 0.6431373  0.6\n",
      "    0.5921569 ]\n",
      "   [0.63529414 0.5921569  0.5803922  ... 0.63529414 0.5921569\n",
      "    0.5803922 ]\n",
      "   ...\n",
      "   [0.80784315 0.69803923 0.67058825 ... 0.80784315 0.69803923\n",
      "    0.67058825]\n",
      "   [0.80784315 0.7176471  0.6901961  ... 0.80784315 0.7176471\n",
      "    0.6901961 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]]\n",
      "\n",
      "  [[0.64705884 0.59607846 0.59607846 ... 0.64705884 0.59607846\n",
      "    0.59607846]\n",
      "   [0.63529414 0.5921569  0.58431375 ... 0.63529414 0.5921569\n",
      "    0.58431375]\n",
      "   [0.62352943 0.5803922  0.57254905 ... 0.62352943 0.5803922\n",
      "    0.57254905]\n",
      "   ...\n",
      "   [0.8        0.69803923 0.6745098  ... 0.8        0.69803923\n",
      "    0.6745098 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]\n",
      "   [0.7921569  0.7019608  0.68235296 ... 0.7921569  0.7019608\n",
      "    0.68235296]]\n",
      "\n",
      "  [[0.6431373  0.59607846 0.5921569  ... 0.6431373  0.59607846\n",
      "    0.5921569 ]\n",
      "   [0.6313726  0.5882353  0.5803922  ... 0.6313726  0.5882353\n",
      "    0.5803922 ]\n",
      "   [0.627451   0.57254905 0.5686275  ... 0.627451   0.57254905\n",
      "    0.5686275 ]\n",
      "   ...\n",
      "   [0.7921569  0.6901961  0.6784314  ... 0.7921569  0.6901961\n",
      "    0.6784314 ]\n",
      "   [0.80784315 0.7058824  0.6901961  ... 0.80784315 0.7058824\n",
      "    0.6901961 ]\n",
      "   [0.80784315 0.7176471  0.7019608  ... 0.80784315 0.7176471\n",
      "    0.7019608 ]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (67, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (67, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 3.1438 - accuracy: 0.0000e+0029\n",
      "29\n",
      "[[[[243 243 243 ... 243 243 243]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  [[242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  [[242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   ...\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]\n",
      "   [242 242 242 ... 242 242 242]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[243 242 244 ... 243 242 244]\n",
      "   [245 242 248 ... 245 242 248]\n",
      "   [250 250 253 ... 250 250 253]\n",
      "   ...\n",
      "   [214 218 239 ... 214 218 239]\n",
      "   [205 209 233 ... 205 209 233]\n",
      "   [213 215 234 ... 213 215 234]]\n",
      "\n",
      "  [[238 236 241 ... 238 236 241]\n",
      "   [235 230 238 ... 235 230 238]\n",
      "   [240 236 243 ... 240 236 243]\n",
      "   ...\n",
      "   [210 214 238 ... 210 214 238]\n",
      "   [202 206 232 ... 202 206 232]\n",
      "   [206 208 231 ... 206 208 231]]\n",
      "\n",
      "  [[226 219 227 ... 226 219 227]\n",
      "   [221 211 220 ... 221 211 220]\n",
      "   [219 210 222 ... 219 210 222]\n",
      "   ...\n",
      "   [207 212 237 ... 207 212 237]\n",
      "   [207 212 236 ... 207 212 236]\n",
      "   [207 210 233 ... 207 210 233]]]\n",
      "\n",
      "\n",
      " [[[ 76  84  95 ...  76  84  95]\n",
      "   [ 15  15  20 ...  15  15  20]\n",
      "   [  8   7  11 ...   8   7  11]\n",
      "   ...\n",
      "   [ 16  14  25 ...  16  14  25]\n",
      "   [ 13  13  22 ...  13  13  22]\n",
      "   [  8   9  13 ...   8   9  13]]\n",
      "\n",
      "  [[ 88  96 107 ...  88  96 107]\n",
      "   [ 17  17  22 ...  17  17  22]\n",
      "   [  6   5   9 ...   6   5   9]\n",
      "   ...\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 15  13  24 ...  15  13  24]\n",
      "   [ 10  10  16 ...  10  10  16]]\n",
      "\n",
      "  [[ 86  91  98 ...  86  91  98]\n",
      "   [ 14  13  17 ...  14  13  17]\n",
      "   [  5   4   7 ...   5   4   7]\n",
      "   ...\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 12  11  18 ...  12  11  18]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[157 162 174 ... 157 162 174]\n",
      "   [159 165 176 ... 159 165 176]\n",
      "   [155 158 159 ... 155 158 159]\n",
      "   ...\n",
      "   [126 136 168 ... 126 136 168]\n",
      "   [145 153 178 ... 145 153 178]\n",
      "   [150 157 180 ... 150 157 180]]\n",
      "\n",
      "  [[158 162 174 ... 158 162 174]\n",
      "   [158 164 177 ... 158 164 177]\n",
      "   [160 167 179 ... 160 167 179]\n",
      "   ...\n",
      "   [134 144 172 ... 134 144 172]\n",
      "   [143 151 177 ... 143 151 177]\n",
      "   [151 158 180 ... 151 158 180]]\n",
      "\n",
      "  [[160 164 176 ... 160 164 176]\n",
      "   [158 164 177 ... 158 164 177]\n",
      "   [163 169 181 ... 163 169 181]\n",
      "   ...\n",
      "   [141 150 174 ... 141 150 174]\n",
      "   [140 149 176 ... 140 149 176]\n",
      "   [150 158 180 ... 150 158 180]]]\n",
      "\n",
      "\n",
      " [[[  9   9   2 ...   9   9   2]\n",
      "   [  6   6   2 ...   6   6   2]\n",
      "   [  4   3   1 ...   4   3   1]\n",
      "   ...\n",
      "   [  0   1   1 ...   0   1   1]\n",
      "   [  1   2   2 ...   1   2   2]\n",
      "   [  9  10   7 ...   9  10   7]]\n",
      "\n",
      "  [[  9   8   4 ...   9   8   4]\n",
      "   [  9   9   4 ...   9   9   4]\n",
      "   [  9   8   3 ...   9   8   3]\n",
      "   ...\n",
      "   [  1   1   1 ...   1   1   1]\n",
      "   [  4   4   3 ...   4   4   3]\n",
      "   [  4   5   3 ...   4   5   3]]\n",
      "\n",
      "  [[ 26  23  14 ...  26  23  14]\n",
      "   [ 33  27  15 ...  33  27  15]\n",
      "   [ 45  37  24 ...  45  37  24]\n",
      "   ...\n",
      "   [  2   3   2 ...   2   3   2]\n",
      "   [  2   2   2 ...   2   2   2]\n",
      "   [  4   4   3 ...   4   4   3]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[188 178 161 ... 188 178 161]\n",
      "   [190 180 162 ... 190 180 162]\n",
      "   [187 177 158 ... 187 177 158]\n",
      "   ...\n",
      "   [150 142 124 ... 150 142 124]\n",
      "   [145 135 118 ... 145 135 118]\n",
      "   [145 135 117 ... 145 135 117]]\n",
      "\n",
      "  [[197 186 170 ... 197 186 170]\n",
      "   [194 184 168 ... 194 184 168]\n",
      "   [192 181 164 ... 192 181 164]\n",
      "   ...\n",
      "   [148 139 120 ... 148 139 120]\n",
      "   [149 138 119 ... 149 138 119]\n",
      "   [147 136 117 ... 147 136 117]]\n",
      "\n",
      "  [[201 191 174 ... 201 191 174]\n",
      "   [198 188 172 ... 198 188 172]\n",
      "   [194 183 167 ... 194 183 167]\n",
      "   ...\n",
      "   [150 139 120 ... 150 139 120]\n",
      "   [152 141 121 ... 152 141 121]\n",
      "   [152 142 124 ... 152 142 124]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (72, 40, 30, 9)\n",
      "72 train samples\n",
      "29 test samples\n",
      "y_train shape: (72, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_29\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_30 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_899 (Conv2D)             (None, 40, 30, 16)   1312        input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_812 (BatchN (None, 40, 30, 16)   64          conv2d_899[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_812 (Activation)     (None, 40, 30, 16)   0           batch_normalization_812[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_900 (Conv2D)             (None, 40, 30, 16)   272         activation_812[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_813 (BatchN (None, 40, 30, 16)   64          conv2d_900[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_813 (Activation)     (None, 40, 30, 16)   0           batch_normalization_813[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_901 (Conv2D)             (None, 40, 30, 16)   2320        activation_813[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_814 (BatchN (None, 40, 30, 16)   64          conv2d_901[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_814 (Activation)     (None, 40, 30, 16)   0           batch_normalization_814[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_903 (Conv2D)             (None, 40, 30, 64)   1088        activation_812[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_902 (Conv2D)             (None, 40, 30, 64)   1088        activation_814[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_261 (Add)                   (None, 40, 30, 64)   0           conv2d_903[0][0]                 \n",
      "                                                                 conv2d_902[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_815 (BatchN (None, 40, 30, 64)   256         add_261[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_815 (Activation)     (None, 40, 30, 64)   0           batch_normalization_815[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_904 (Conv2D)             (None, 40, 30, 16)   1040        activation_815[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_816 (BatchN (None, 40, 30, 16)   64          conv2d_904[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_816 (Activation)     (None, 40, 30, 16)   0           batch_normalization_816[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_905 (Conv2D)             (None, 40, 30, 16)   2320        activation_816[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_817 (BatchN (None, 40, 30, 16)   64          conv2d_905[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_817 (Activation)     (None, 40, 30, 16)   0           batch_normalization_817[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_906 (Conv2D)             (None, 40, 30, 64)   1088        activation_817[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_262 (Add)                   (None, 40, 30, 64)   0           add_261[0][0]                    \n",
      "                                                                 conv2d_906[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_818 (BatchN (None, 40, 30, 64)   256         add_262[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_818 (Activation)     (None, 40, 30, 64)   0           batch_normalization_818[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_907 (Conv2D)             (None, 40, 30, 16)   1040        activation_818[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_819 (BatchN (None, 40, 30, 16)   64          conv2d_907[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_819 (Activation)     (None, 40, 30, 16)   0           batch_normalization_819[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_908 (Conv2D)             (None, 40, 30, 16)   2320        activation_819[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_820 (BatchN (None, 40, 30, 16)   64          conv2d_908[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_820 (Activation)     (None, 40, 30, 16)   0           batch_normalization_820[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_909 (Conv2D)             (None, 40, 30, 64)   1088        activation_820[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_263 (Add)                   (None, 40, 30, 64)   0           add_262[0][0]                    \n",
      "                                                                 conv2d_909[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_821 (BatchN (None, 40, 30, 64)   256         add_263[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_821 (Activation)     (None, 40, 30, 64)   0           batch_normalization_821[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_910 (Conv2D)             (None, 20, 15, 64)   4160        activation_821[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_822 (BatchN (None, 20, 15, 64)   256         conv2d_910[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_822 (Activation)     (None, 20, 15, 64)   0           batch_normalization_822[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_911 (Conv2D)             (None, 20, 15, 64)   36928       activation_822[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_823 (BatchN (None, 20, 15, 64)   256         conv2d_911[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_823 (Activation)     (None, 20, 15, 64)   0           batch_normalization_823[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_913 (Conv2D)             (None, 20, 15, 128)  8320        add_263[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_912 (Conv2D)             (None, 20, 15, 128)  8320        activation_823[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_264 (Add)                   (None, 20, 15, 128)  0           conv2d_913[0][0]                 \n",
      "                                                                 conv2d_912[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_824 (BatchN (None, 20, 15, 128)  512         add_264[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_824 (Activation)     (None, 20, 15, 128)  0           batch_normalization_824[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_914 (Conv2D)             (None, 20, 15, 64)   8256        activation_824[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_825 (BatchN (None, 20, 15, 64)   256         conv2d_914[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_825 (Activation)     (None, 20, 15, 64)   0           batch_normalization_825[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_915 (Conv2D)             (None, 20, 15, 64)   36928       activation_825[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_826 (BatchN (None, 20, 15, 64)   256         conv2d_915[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_826 (Activation)     (None, 20, 15, 64)   0           batch_normalization_826[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_916 (Conv2D)             (None, 20, 15, 128)  8320        activation_826[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_265 (Add)                   (None, 20, 15, 128)  0           add_264[0][0]                    \n",
      "                                                                 conv2d_916[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_827 (BatchN (None, 20, 15, 128)  512         add_265[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_827 (Activation)     (None, 20, 15, 128)  0           batch_normalization_827[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_917 (Conv2D)             (None, 20, 15, 64)   8256        activation_827[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_828 (BatchN (None, 20, 15, 64)   256         conv2d_917[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_828 (Activation)     (None, 20, 15, 64)   0           batch_normalization_828[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_918 (Conv2D)             (None, 20, 15, 64)   36928       activation_828[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_829 (BatchN (None, 20, 15, 64)   256         conv2d_918[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_829 (Activation)     (None, 20, 15, 64)   0           batch_normalization_829[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_919 (Conv2D)             (None, 20, 15, 128)  8320        activation_829[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_266 (Add)                   (None, 20, 15, 128)  0           add_265[0][0]                    \n",
      "                                                                 conv2d_919[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_830 (BatchN (None, 20, 15, 128)  512         add_266[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_830 (Activation)     (None, 20, 15, 128)  0           batch_normalization_830[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_920 (Conv2D)             (None, 10, 8, 128)   16512       activation_830[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_831 (BatchN (None, 10, 8, 128)   512         conv2d_920[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_831 (Activation)     (None, 10, 8, 128)   0           batch_normalization_831[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_921 (Conv2D)             (None, 10, 8, 128)   147584      activation_831[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_832 (BatchN (None, 10, 8, 128)   512         conv2d_921[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_832 (Activation)     (None, 10, 8, 128)   0           batch_normalization_832[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_923 (Conv2D)             (None, 10, 8, 256)   33024       add_266[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_922 (Conv2D)             (None, 10, 8, 256)   33024       activation_832[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_267 (Add)                   (None, 10, 8, 256)   0           conv2d_923[0][0]                 \n",
      "                                                                 conv2d_922[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_833 (BatchN (None, 10, 8, 256)   1024        add_267[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_833 (Activation)     (None, 10, 8, 256)   0           batch_normalization_833[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_924 (Conv2D)             (None, 10, 8, 128)   32896       activation_833[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_834 (BatchN (None, 10, 8, 128)   512         conv2d_924[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_834 (Activation)     (None, 10, 8, 128)   0           batch_normalization_834[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_925 (Conv2D)             (None, 10, 8, 128)   147584      activation_834[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_835 (BatchN (None, 10, 8, 128)   512         conv2d_925[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_835 (Activation)     (None, 10, 8, 128)   0           batch_normalization_835[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_926 (Conv2D)             (None, 10, 8, 256)   33024       activation_835[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_268 (Add)                   (None, 10, 8, 256)   0           add_267[0][0]                    \n",
      "                                                                 conv2d_926[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_836 (BatchN (None, 10, 8, 256)   1024        add_268[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_836 (Activation)     (None, 10, 8, 256)   0           batch_normalization_836[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_927 (Conv2D)             (None, 10, 8, 128)   32896       activation_836[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_837 (BatchN (None, 10, 8, 128)   512         conv2d_927[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_837 (Activation)     (None, 10, 8, 128)   0           batch_normalization_837[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_928 (Conv2D)             (None, 10, 8, 128)   147584      activation_837[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_838 (BatchN (None, 10, 8, 128)   512         conv2d_928[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_838 (Activation)     (None, 10, 8, 128)   0           batch_normalization_838[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_929 (Conv2D)             (None, 10, 8, 256)   33024       activation_838[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_269 (Add)                   (None, 10, 8, 256)   0           add_268[0][0]                    \n",
      "                                                                 conv2d_929[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_839 (BatchN (None, 10, 8, 256)   1024        add_269[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_839 (Activation)     (None, 10, 8, 256)   0           batch_normalization_839[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_29 (AveragePo (None, 1, 1, 256)    0           activation_839[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 256)          0           average_pooling2d_29[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 10)           2570        flatten_29[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[0.9529412  0.9529412  0.9529412  ... 0.9529412  0.9529412\n",
      "    0.9529412 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  [[0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  [[0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196  ... 0.9490196  0.9490196\n",
      "    0.9490196 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.9529412  0.9490196  0.95686275 ... 0.9529412  0.9490196\n",
      "    0.95686275]\n",
      "   [0.9607843  0.9490196  0.972549   ... 0.9607843  0.9490196\n",
      "    0.972549  ]\n",
      "   [0.98039216 0.98039216 0.99215686 ... 0.98039216 0.98039216\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [0.8392157  0.85490197 0.9372549  ... 0.8392157  0.85490197\n",
      "    0.9372549 ]\n",
      "   [0.8039216  0.81960785 0.9137255  ... 0.8039216  0.81960785\n",
      "    0.9137255 ]\n",
      "   [0.8352941  0.84313726 0.91764706 ... 0.8352941  0.84313726\n",
      "    0.91764706]]\n",
      "\n",
      "  [[0.93333334 0.9254902  0.94509804 ... 0.93333334 0.9254902\n",
      "    0.94509804]\n",
      "   [0.92156863 0.9019608  0.93333334 ... 0.92156863 0.9019608\n",
      "    0.93333334]\n",
      "   [0.9411765  0.9254902  0.9529412  ... 0.9411765  0.9254902\n",
      "    0.9529412 ]\n",
      "   ...\n",
      "   [0.8235294  0.8392157  0.93333334 ... 0.8235294  0.8392157\n",
      "    0.93333334]\n",
      "   [0.7921569  0.80784315 0.9098039  ... 0.7921569  0.80784315\n",
      "    0.9098039 ]\n",
      "   [0.80784315 0.8156863  0.90588236 ... 0.80784315 0.8156863\n",
      "    0.90588236]]\n",
      "\n",
      "  [[0.8862745  0.85882354 0.8901961  ... 0.8862745  0.85882354\n",
      "    0.8901961 ]\n",
      "   [0.8666667  0.827451   0.8627451  ... 0.8666667  0.827451\n",
      "    0.8627451 ]\n",
      "   [0.85882354 0.8235294  0.87058824 ... 0.85882354 0.8235294\n",
      "    0.87058824]\n",
      "   ...\n",
      "   [0.8117647  0.83137256 0.92941177 ... 0.8117647  0.83137256\n",
      "    0.92941177]\n",
      "   [0.8117647  0.83137256 0.9254902  ... 0.8117647  0.83137256\n",
      "    0.9254902 ]\n",
      "   [0.8117647  0.8235294  0.9137255  ... 0.8117647  0.8235294\n",
      "    0.9137255 ]]]\n",
      "\n",
      "\n",
      " [[[0.29803923 0.32941177 0.37254903 ... 0.29803923 0.32941177\n",
      "    0.37254903]\n",
      "   [0.05882353 0.05882353 0.07843138 ... 0.05882353 0.05882353\n",
      "    0.07843138]\n",
      "   [0.03137255 0.02745098 0.04313726 ... 0.03137255 0.02745098\n",
      "    0.04313726]\n",
      "   ...\n",
      "   [0.0627451  0.05490196 0.09803922 ... 0.0627451  0.05490196\n",
      "    0.09803922]\n",
      "   [0.05098039 0.05098039 0.08627451 ... 0.05098039 0.05098039\n",
      "    0.08627451]\n",
      "   [0.03137255 0.03529412 0.05098039 ... 0.03137255 0.03529412\n",
      "    0.05098039]]\n",
      "\n",
      "  [[0.34509805 0.3764706  0.41960785 ... 0.34509805 0.3764706\n",
      "    0.41960785]\n",
      "   [0.06666667 0.06666667 0.08627451 ... 0.06666667 0.06666667\n",
      "    0.08627451]\n",
      "   [0.02352941 0.01960784 0.03529412 ... 0.02352941 0.01960784\n",
      "    0.03529412]\n",
      "   ...\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.05882353 0.05098039 0.09411765 ... 0.05882353 0.05098039\n",
      "    0.09411765]\n",
      "   [0.03921569 0.03921569 0.0627451  ... 0.03921569 0.03921569\n",
      "    0.0627451 ]]\n",
      "\n",
      "  [[0.3372549  0.35686275 0.38431373 ... 0.3372549  0.35686275\n",
      "    0.38431373]\n",
      "   [0.05490196 0.05098039 0.06666667 ... 0.05490196 0.05098039\n",
      "    0.06666667]\n",
      "   [0.01960784 0.01568628 0.02745098 ... 0.01960784 0.01568628\n",
      "    0.02745098]\n",
      "   ...\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.04705882 0.04313726 0.07058824 ... 0.04705882 0.04313726\n",
      "    0.07058824]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6156863  0.63529414 0.68235296 ... 0.6156863  0.63529414\n",
      "    0.68235296]\n",
      "   [0.62352943 0.64705884 0.6901961  ... 0.62352943 0.64705884\n",
      "    0.6901961 ]\n",
      "   [0.60784316 0.61960787 0.62352943 ... 0.60784316 0.61960787\n",
      "    0.62352943]\n",
      "   ...\n",
      "   [0.49411765 0.53333336 0.65882355 ... 0.49411765 0.53333336\n",
      "    0.65882355]\n",
      "   [0.5686275  0.6        0.69803923 ... 0.5686275  0.6\n",
      "    0.69803923]\n",
      "   [0.5882353  0.6156863  0.7058824  ... 0.5882353  0.6156863\n",
      "    0.7058824 ]]\n",
      "\n",
      "  [[0.61960787 0.63529414 0.68235296 ... 0.61960787 0.63529414\n",
      "    0.68235296]\n",
      "   [0.61960787 0.6431373  0.69411767 ... 0.61960787 0.6431373\n",
      "    0.69411767]\n",
      "   [0.627451   0.654902   0.7019608  ... 0.627451   0.654902\n",
      "    0.7019608 ]\n",
      "   ...\n",
      "   [0.5254902  0.5647059  0.6745098  ... 0.5254902  0.5647059\n",
      "    0.6745098 ]\n",
      "   [0.56078434 0.5921569  0.69411767 ... 0.56078434 0.5921569\n",
      "    0.69411767]\n",
      "   [0.5921569  0.61960787 0.7058824  ... 0.5921569  0.61960787\n",
      "    0.7058824 ]]\n",
      "\n",
      "  [[0.627451   0.6431373  0.6901961  ... 0.627451   0.6431373\n",
      "    0.6901961 ]\n",
      "   [0.61960787 0.6431373  0.69411767 ... 0.61960787 0.6431373\n",
      "    0.69411767]\n",
      "   [0.6392157  0.6627451  0.70980394 ... 0.6392157  0.6627451\n",
      "    0.70980394]\n",
      "   ...\n",
      "   [0.5529412  0.5882353  0.68235296 ... 0.5529412  0.5882353\n",
      "    0.68235296]\n",
      "   [0.54901963 0.58431375 0.6901961  ... 0.54901963 0.58431375\n",
      "    0.6901961 ]\n",
      "   [0.5882353  0.61960787 0.7058824  ... 0.5882353  0.61960787\n",
      "    0.7058824 ]]]\n",
      "\n",
      "\n",
      " [[[0.03529412 0.03529412 0.00784314 ... 0.03529412 0.03529412\n",
      "    0.00784314]\n",
      "   [0.02352941 0.02352941 0.00784314 ... 0.02352941 0.02352941\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01176471 0.00392157 ... 0.01568628 0.01176471\n",
      "    0.00392157]\n",
      "   ...\n",
      "   [0.         0.00392157 0.00392157 ... 0.         0.00392157\n",
      "    0.00392157]\n",
      "   [0.00392157 0.00784314 0.00784314 ... 0.00392157 0.00784314\n",
      "    0.00784314]\n",
      "   [0.03529412 0.03921569 0.02745098 ... 0.03529412 0.03921569\n",
      "    0.02745098]]\n",
      "\n",
      "  [[0.03529412 0.03137255 0.01568628 ... 0.03529412 0.03137255\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03529412 0.01568628 ... 0.03529412 0.03529412\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03137255 0.01176471 ... 0.03529412 0.03137255\n",
      "    0.01176471]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157 ... 0.00392157 0.00392157\n",
      "    0.00392157]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]\n",
      "   [0.01568628 0.01960784 0.01176471 ... 0.01568628 0.01960784\n",
      "    0.01176471]]\n",
      "\n",
      "  [[0.10196079 0.09019608 0.05490196 ... 0.10196079 0.09019608\n",
      "    0.05490196]\n",
      "   [0.12941177 0.10588235 0.05882353 ... 0.12941177 0.10588235\n",
      "    0.05882353]\n",
      "   [0.1764706  0.14509805 0.09411765 ... 0.1764706  0.14509805\n",
      "    0.09411765]\n",
      "   ...\n",
      "   [0.00784314 0.01176471 0.00784314 ... 0.00784314 0.01176471\n",
      "    0.00784314]\n",
      "   [0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.7372549  0.69803923 0.6313726  ... 0.7372549  0.69803923\n",
      "    0.6313726 ]\n",
      "   [0.74509805 0.7058824  0.63529414 ... 0.74509805 0.7058824\n",
      "    0.63529414]\n",
      "   [0.73333335 0.69411767 0.61960787 ... 0.73333335 0.69411767\n",
      "    0.61960787]\n",
      "   ...\n",
      "   [0.5882353  0.5568628  0.4862745  ... 0.5882353  0.5568628\n",
      "    0.4862745 ]\n",
      "   [0.5686275  0.5294118  0.4627451  ... 0.5686275  0.5294118\n",
      "    0.4627451 ]\n",
      "   [0.5686275  0.5294118  0.45882353 ... 0.5686275  0.5294118\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.77254903 0.7294118  0.6666667  ... 0.77254903 0.7294118\n",
      "    0.6666667 ]\n",
      "   [0.7607843  0.72156864 0.65882355 ... 0.7607843  0.72156864\n",
      "    0.65882355]\n",
      "   [0.7529412  0.70980394 0.6431373  ... 0.7529412  0.70980394\n",
      "    0.6431373 ]\n",
      "   ...\n",
      "   [0.5803922  0.54509807 0.47058824 ... 0.5803922  0.54509807\n",
      "    0.47058824]\n",
      "   [0.58431375 0.5411765  0.46666667 ... 0.58431375 0.5411765\n",
      "    0.46666667]\n",
      "   [0.5764706  0.53333336 0.45882353 ... 0.5764706  0.53333336\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.7882353  0.7490196  0.68235296 ... 0.7882353  0.7490196\n",
      "    0.68235296]\n",
      "   [0.7764706  0.7372549  0.6745098  ... 0.7764706  0.7372549\n",
      "    0.6745098 ]\n",
      "   [0.7607843  0.7176471  0.654902   ... 0.7607843  0.7176471\n",
      "    0.654902  ]\n",
      "   ...\n",
      "   [0.5882353  0.54509807 0.47058824 ... 0.5882353  0.54509807\n",
      "    0.47058824]\n",
      "   [0.59607846 0.5529412  0.4745098  ... 0.59607846 0.5529412\n",
      "    0.4745098 ]\n",
      "   [0.59607846 0.5568628  0.4862745  ... 0.59607846 0.5568628\n",
      "    0.4862745 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157 ... 0.00392157 0.00392157\n",
      "    0.00392157]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.61960787 0.62352943 0.6784314  ... 0.61960787 0.62352943\n",
      "    0.6784314 ]\n",
      "   [0.6784314  0.6862745  0.73333335 ... 0.6784314  0.6862745\n",
      "    0.73333335]\n",
      "   [0.5019608  0.5058824  0.5568628  ... 0.5019608  0.5058824\n",
      "    0.5568628 ]\n",
      "   ...\n",
      "   [0.25490198 0.26666668 0.30588236 ... 0.25490198 0.26666668\n",
      "    0.30588236]\n",
      "   [0.16078432 0.16078432 0.21176471 ... 0.16078432 0.16078432\n",
      "    0.21176471]\n",
      "   [0.15686275 0.15294118 0.2        ... 0.15686275 0.15294118\n",
      "    0.2       ]]\n",
      "\n",
      "  [[0.59607846 0.6039216  0.65882355 ... 0.59607846 0.6039216\n",
      "    0.65882355]\n",
      "   [0.6        0.60784316 0.6627451  ... 0.6        0.60784316\n",
      "    0.6627451 ]\n",
      "   [0.4627451  0.46666667 0.52156866 ... 0.4627451  0.46666667\n",
      "    0.52156866]\n",
      "   ...\n",
      "   [0.16078432 0.16862746 0.19607843 ... 0.16078432 0.16862746\n",
      "    0.19607843]\n",
      "   [0.1254902  0.12156863 0.16470589 ... 0.1254902  0.12156863\n",
      "    0.16470589]\n",
      "   [0.12156863 0.12156863 0.16078432 ... 0.12156863 0.12156863\n",
      "    0.16078432]]\n",
      "\n",
      "  [[0.49411765 0.49019608 0.5568628  ... 0.49411765 0.49019608\n",
      "    0.5568628 ]\n",
      "   [0.5019608  0.49803922 0.5647059  ... 0.5019608  0.49803922\n",
      "    0.5647059 ]\n",
      "   [0.4745098  0.47058824 0.5294118  ... 0.4745098  0.47058824\n",
      "    0.5294118 ]\n",
      "   ...\n",
      "   [0.02745098 0.02352941 0.03137255 ... 0.02745098 0.02352941\n",
      "    0.03137255]\n",
      "   [0.09803922 0.09019608 0.1254902  ... 0.09803922 0.09019608\n",
      "    0.1254902 ]\n",
      "   [0.09411765 0.08627451 0.12156863 ... 0.09411765 0.08627451\n",
      "    0.12156863]]]\n",
      "\n",
      "\n",
      " [[[0.49803922 0.49803922 0.5176471  ... 0.49803922 0.49803922\n",
      "    0.5176471 ]\n",
      "   [0.5058824  0.5019608  0.52156866 ... 0.5058824  0.5019608\n",
      "    0.52156866]\n",
      "   [0.50980395 0.5058824  0.5254902  ... 0.50980395 0.5058824\n",
      "    0.5254902 ]\n",
      "   ...\n",
      "   [0.5686275  0.57254905 0.5882353  ... 0.5686275  0.57254905\n",
      "    0.5882353 ]\n",
      "   [0.57254905 0.5764706  0.5921569  ... 0.57254905 0.5764706\n",
      "    0.5921569 ]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]]\n",
      "\n",
      "  [[0.50980395 0.50980395 0.5254902  ... 0.50980395 0.50980395\n",
      "    0.5254902 ]\n",
      "   [0.5137255  0.5137255  0.5294118  ... 0.5137255  0.5137255\n",
      "    0.5294118 ]\n",
      "   [0.5176471  0.5176471  0.53333336 ... 0.5176471  0.5176471\n",
      "    0.53333336]\n",
      "   ...\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5803922  0.58431375 0.6        ... 0.5803922  0.58431375\n",
      "    0.6       ]]\n",
      "\n",
      "  [[0.5176471  0.5176471  0.5372549  ... 0.5176471  0.5176471\n",
      "    0.5372549 ]\n",
      "   [0.52156866 0.5254902  0.5411765  ... 0.52156866 0.5254902\n",
      "    0.5411765 ]\n",
      "   [0.5176471  0.52156866 0.5372549  ... 0.5176471  0.52156866\n",
      "    0.5372549 ]\n",
      "   ...\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.5882353  0.5921569  0.60784316 ... 0.5882353  0.5921569\n",
      "    0.60784316]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8901961  0.8392157  0.8039216  ... 0.8901961  0.8392157\n",
      "    0.8039216 ]\n",
      "   [0.91764706 0.8745098  0.8392157  ... 0.91764706 0.8745098\n",
      "    0.8392157 ]\n",
      "   [0.8627451  0.79607844 0.7490196  ... 0.8627451  0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9372549  0.90588236 0.8784314  ... 0.9372549  0.90588236\n",
      "    0.8784314 ]\n",
      "   [0.90588236 0.85882354 0.8235294  ... 0.90588236 0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.87058824 0.8117647  0.7764706  ... 0.87058824 0.8117647\n",
      "    0.7764706 ]]\n",
      "\n",
      "  [[0.88235295 0.8235294  0.78431374 ... 0.88235295 0.8235294\n",
      "    0.78431374]\n",
      "   [0.89411765 0.8392157  0.8        ... 0.89411765 0.8392157\n",
      "    0.8       ]\n",
      "   [0.87058824 0.8039216  0.7607843  ... 0.87058824 0.8039216\n",
      "    0.7607843 ]\n",
      "   ...\n",
      "   [0.92156863 0.8862745  0.8509804  ... 0.92156863 0.8862745\n",
      "    0.8509804 ]\n",
      "   [0.9098039  0.87058824 0.8352941  ... 0.9098039  0.87058824\n",
      "    0.8352941 ]\n",
      "   [0.8745098  0.8156863  0.77254903 ... 0.8745098  0.8156863\n",
      "    0.77254903]]\n",
      "\n",
      "  [[0.8627451  0.7921569  0.7411765  ... 0.8627451  0.7921569\n",
      "    0.7411765 ]\n",
      "   [0.8666667  0.8039216  0.7529412  ... 0.8666667  0.8039216\n",
      "    0.7529412 ]\n",
      "   [0.85882354 0.79607844 0.7490196  ... 0.85882354 0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9098039  0.85882354 0.8235294  ... 0.9098039  0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.90588236 0.85490197 0.8156863  ... 0.90588236 0.85490197\n",
      "    0.8156863 ]\n",
      "   [0.8784314  0.8156863  0.7647059  ... 0.8784314  0.8156863\n",
      "    0.7647059 ]]]\n",
      "\n",
      "\n",
      " [[[0.65882355 0.5411765  0.34901962 ... 0.65882355 0.5411765\n",
      "    0.34901962]\n",
      "   [0.79607844 0.6862745  0.44313726 ... 0.79607844 0.6862745\n",
      "    0.44313726]\n",
      "   [0.84705883 0.7372549  0.4862745  ... 0.84705883 0.7372549\n",
      "    0.4862745 ]\n",
      "   ...\n",
      "   [0.9019608  0.78431374 0.57254905 ... 0.9019608  0.78431374\n",
      "    0.57254905]\n",
      "   [0.8784314  0.77254903 0.5803922  ... 0.8784314  0.77254903\n",
      "    0.5803922 ]\n",
      "   [0.8666667  0.7607843  0.58431375 ... 0.8666667  0.7607843\n",
      "    0.58431375]]\n",
      "\n",
      "  [[0.6313726  0.5058824  0.27450982 ... 0.6313726  0.5058824\n",
      "    0.27450982]\n",
      "   [0.7372549  0.61960787 0.3764706  ... 0.7372549  0.61960787\n",
      "    0.3764706 ]\n",
      "   [0.83137256 0.72156864 0.4627451  ... 0.83137256 0.72156864\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.8784314  0.7607843  0.5568628  ... 0.8784314  0.7607843\n",
      "    0.5568628 ]\n",
      "   [0.85882354 0.7490196  0.56078434 ... 0.85882354 0.7490196\n",
      "    0.56078434]\n",
      "   [0.85490197 0.7529412  0.5647059  ... 0.85490197 0.7529412\n",
      "    0.5647059 ]]\n",
      "\n",
      "  [[0.6        0.46666667 0.22352941 ... 0.6        0.46666667\n",
      "    0.22352941]\n",
      "   [0.6313726  0.49803922 0.27058825 ... 0.6313726  0.49803922\n",
      "    0.27058825]\n",
      "   [0.80784315 0.69803923 0.44705883 ... 0.80784315 0.69803923\n",
      "    0.44705883]\n",
      "   ...\n",
      "   [0.8352941  0.72156864 0.5372549  ... 0.8352941  0.72156864\n",
      "    0.5372549 ]\n",
      "   [0.83137256 0.7254902  0.5411765  ... 0.83137256 0.7254902\n",
      "    0.5411765 ]\n",
      "   [0.8352941  0.73333335 0.5372549  ... 0.8352941  0.73333335\n",
      "    0.5372549 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6039216  0.3764706  0.27058825 ... 0.6039216  0.3764706\n",
      "    0.27058825]\n",
      "   [0.627451   0.40784314 0.31764707 ... 0.627451   0.40784314\n",
      "    0.31764707]\n",
      "   [0.67058825 0.47058824 0.4        ... 0.67058825 0.47058824\n",
      "    0.4       ]\n",
      "   ...\n",
      "   [0.76862746 0.58431375 0.4745098  ... 0.76862746 0.58431375\n",
      "    0.4745098 ]\n",
      "   [0.7058824  0.49803922 0.38431373 ... 0.7058824  0.49803922\n",
      "    0.38431373]\n",
      "   [0.65882355 0.44313726 0.32156864 ... 0.65882355 0.44313726\n",
      "    0.32156864]]\n",
      "\n",
      "  [[0.6        0.38039216 0.26666668 ... 0.6        0.38039216\n",
      "    0.26666668]\n",
      "   [0.63529414 0.41568628 0.32156864 ... 0.63529414 0.41568628\n",
      "    0.32156864]\n",
      "   [0.7372549  0.56078434 0.4627451  ... 0.7372549  0.56078434\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.83137256 0.654902   0.54901963 ... 0.83137256 0.654902\n",
      "    0.54901963]\n",
      "   [0.77254903 0.5647059  0.44313726 ... 0.77254903 0.5647059\n",
      "    0.44313726]\n",
      "   [0.7176471  0.5019608  0.38039216 ... 0.7176471  0.5019608\n",
      "    0.38039216]]\n",
      "\n",
      "  [[0.627451   0.41960785 0.29411766 ... 0.627451   0.41960785\n",
      "    0.29411766]\n",
      "   [0.68235296 0.46666667 0.3882353  ... 0.68235296 0.46666667\n",
      "    0.3882353 ]\n",
      "   [0.8509804  0.7294118  0.49019608 ... 0.8509804  0.7294118\n",
      "    0.49019608]\n",
      "   ...\n",
      "   [0.89411765 0.72156864 0.6117647  ... 0.89411765 0.72156864\n",
      "    0.6117647 ]\n",
      "   [0.8392157  0.627451   0.50980395 ... 0.8392157  0.627451\n",
      "    0.50980395]\n",
      "   [0.76862746 0.56078434 0.43529412 ... 0.76862746 0.56078434\n",
      "    0.43529412]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (72, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (72, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 7s 3s/step - loss: 3.3993 - accuracy: 0.0139 - val_loss: 2.8991 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/100\n",
      "Learning rate:  1e-05\n",
      "2/2 [==============================] - 2s 458ms/step - loss: 3.4049 - accuracy: 0.0139 - val_loss: 2.9131 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/100\n",
      "Learning rate:  2e-05\n",
      "2/2 [==============================] - 2s 451ms/step - loss: 3.3846 - accuracy: 0.0139 - val_loss: 2.9097 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/100\n",
      "Learning rate:  3e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 3.3526 - accuracy: 0.0000e+00 - val_loss: 2.9047 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/100\n",
      "Learning rate:  4e-05\n",
      "2/2 [==============================] - 2s 438ms/step - loss: 3.3539 - accuracy: 0.0139 - val_loss: 2.8926 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/100\n",
      "Learning rate:  5e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 3.2998 - accuracy: 0.0139 - val_loss: 2.8758 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/100\n",
      "Learning rate:  6e-05\n",
      "2/2 [==============================] - 2s 450ms/step - loss: 3.2598 - accuracy: 0.0139 - val_loss: 2.8513 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/100\n",
      "Learning rate:  7.000000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 3.2062 - accuracy: 0.0278 - val_loss: 2.8240 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/100\n",
      "Learning rate:  8e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 3.1212 - accuracy: 0.0417 - val_loss: 2.7921 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/100\n",
      "Learning rate:  8.999999999999999e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 3.0786 - accuracy: 0.1250 - val_loss: 2.7510 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/100\n",
      "Learning rate:  0.0001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 3.0047 - accuracy: 0.1944 - val_loss: 2.6980 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/100\n",
      "Learning rate:  0.00011\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.9252 - accuracy: 0.2222 - val_loss: 2.6338 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/100\n",
      "Learning rate:  0.00012\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.8425 - accuracy: 0.2917 - val_loss: 2.5529 - val_accuracy: 0.0345\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/100\n",
      "Learning rate:  0.00013000000000000002\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.7514 - accuracy: 0.3750 - val_loss: 2.4705 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/100\n",
      "Learning rate:  0.00014000000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.6325 - accuracy: 0.4444 - val_loss: 2.3647 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/100\n",
      "Learning rate:  0.00015\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.5312 - accuracy: 0.5139 - val_loss: 2.2598 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/100\n",
      "Learning rate:  0.00016\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.4318 - accuracy: 0.5694 - val_loss: 2.1510 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/100\n",
      "Learning rate:  0.00017\n",
      "2/2 [==============================] - 2s 451ms/step - loss: 2.3742 - accuracy: 0.6389 - val_loss: 2.0323 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/100\n",
      "Learning rate:  0.00017999999999999998\n",
      "2/2 [==============================] - 2s 446ms/step - loss: 2.2522 - accuracy: 0.5972 - val_loss: 1.9148 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/100\n",
      "Learning rate:  0.00019\n",
      "2/2 [==============================] - 2s 499ms/step - loss: 2.1750 - accuracy: 0.6806 - val_loss: 1.8169 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 21/100\n",
      "Learning rate:  0.0002\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.0296 - accuracy: 0.7083 - val_loss: 1.7278 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 22/100\n",
      "Learning rate:  0.00021\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.9122 - accuracy: 0.7639 - val_loss: 1.6278 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 23/100\n",
      "Learning rate:  0.00022\n",
      "2/2 [==============================] - 2s 463ms/step - loss: 1.8571 - accuracy: 0.6944 - val_loss: 1.5209 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 24/100\n",
      "Learning rate:  0.00023\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.7402 - accuracy: 0.6944 - val_loss: 1.4154 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 25/100\n",
      "Learning rate:  0.00024\n",
      "2/2 [==============================] - 2s 470ms/step - loss: 1.7170 - accuracy: 0.6806 - val_loss: 1.3403 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 26/100\n",
      "Learning rate:  0.00025\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.6179 - accuracy: 0.7222 - val_loss: 1.3232 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 27/100\n",
      "Learning rate:  0.00026000000000000003\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.4930 - accuracy: 0.7083 - val_loss: 1.3094 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 28/100\n",
      "Learning rate:  0.00027\n",
      "2/2 [==============================] - 2s 519ms/step - loss: 1.4735 - accuracy: 0.7500 - val_loss: 1.2744 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 29/100\n",
      "Learning rate:  0.00028000000000000003\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.3769 - accuracy: 0.7917 - val_loss: 1.1668 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 30/100\n",
      "Learning rate:  0.00029\n",
      "2/2 [==============================] - 3s 703ms/step - loss: 1.3548 - accuracy: 0.7500 - val_loss: 1.0872 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 31/100\n",
      "Learning rate:  0.0003\n",
      "2/2 [==============================] - 3s 3s/step - loss: 1.2895 - accuracy: 0.8056 - val_loss: 0.9903 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 32/100\n",
      "Learning rate:  0.00031\n",
      "2/2 [==============================] - 3s 3s/step - loss: 1.2447 - accuracy: 0.8472 - val_loss: 0.9371 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 33/100\n",
      "Learning rate:  0.00032\n",
      "2/2 [==============================] - 3s 568ms/step - loss: 1.2959 - accuracy: 0.7778 - val_loss: 0.9205 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 34/100\n",
      "Learning rate:  0.00033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 507ms/step - loss: 1.2320 - accuracy: 0.8194 - val_loss: 0.9575 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 35/100\n",
      "Learning rate:  0.00034\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.1885 - accuracy: 0.8333 - val_loss: 1.0201 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 36/100\n",
      "Learning rate:  0.00035\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.1318 - accuracy: 0.8333 - val_loss: 1.1112 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 37/100\n",
      "Learning rate:  0.00035999999999999997\n",
      "2/2 [==============================] - 2s 441ms/step - loss: 1.0904 - accuracy: 0.8611 - val_loss: 1.2270 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 38/100\n",
      "Learning rate:  0.00037\n",
      "2/2 [==============================] - 2s 444ms/step - loss: 1.1473 - accuracy: 0.8194 - val_loss: 1.3378 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 39/100\n",
      "Learning rate:  0.00038\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0833 - accuracy: 0.8056 - val_loss: 1.4965 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 40/100\n",
      "Learning rate:  0.00039000000000000005\n",
      "2/2 [==============================] - 2s 478ms/step - loss: 1.1081 - accuracy: 0.8472 - val_loss: 1.5940 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 41/100\n",
      "Learning rate:  0.0004\n",
      "2/2 [==============================] - 2s 454ms/step - loss: 1.0555 - accuracy: 0.8611 - val_loss: 1.6193 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 42/100\n",
      "Learning rate:  0.00041\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0354 - accuracy: 0.8333 - val_loss: 1.5741 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 43/100\n",
      "Learning rate:  0.00042\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0243 - accuracy: 0.8611 - val_loss: 1.4601 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 44/100\n",
      "Learning rate:  0.00043\n",
      "2/2 [==============================] - 2s 493ms/step - loss: 0.9694 - accuracy: 0.8889 - val_loss: 1.3657 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 45/100\n",
      "Learning rate:  0.00044\n",
      "2/2 [==============================] - 2s 475ms/step - loss: 1.0203 - accuracy: 0.8611 - val_loss: 1.2856 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 46/100\n",
      "Learning rate:  0.00045000000000000004\n",
      "2/2 [==============================] - 3s 642ms/step - loss: 1.0608 - accuracy: 0.8333 - val_loss: 1.2519 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 47/100\n",
      "Learning rate:  0.00046\n",
      "2/2 [==============================] - 3s 515ms/step - loss: 1.0384 - accuracy: 0.8472 - val_loss: 1.2815 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 48/100\n",
      "Learning rate:  0.00047\n",
      "2/2 [==============================] - 3s 571ms/step - loss: 1.0483 - accuracy: 0.8056 - val_loss: 1.4450 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 49/100\n",
      "Learning rate:  0.00048\n",
      "2/2 [==============================] - 3s 550ms/step - loss: 0.9560 - accuracy: 0.8750 - val_loss: 1.7553 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 50/100\n",
      "Learning rate:  0.00049\n",
      "2/2 [==============================] - 2s 513ms/step - loss: 0.9083 - accuracy: 0.9167 - val_loss: 2.2192 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 51/100\n",
      "Learning rate:  0.0005\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8793 - accuracy: 0.9028 - val_loss: 2.6415 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 52/100\n",
      "Learning rate:  0.00051\n",
      "2/2 [==============================] - 2s 513ms/step - loss: 0.8982 - accuracy: 0.9028 - val_loss: 2.8913 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 53/100\n",
      "Learning rate:  0.0005200000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8778 - accuracy: 0.9306 - val_loss: 3.1696 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 54/100\n",
      "Learning rate:  0.0005300000000000001\n",
      "2/2 [==============================] - 2s 515ms/step - loss: 0.8952 - accuracy: 0.9167 - val_loss: 3.1506 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 55/100\n",
      "Learning rate:  0.00054\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8948 - accuracy: 0.9167 - val_loss: 2.6361 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 56/100\n",
      "Learning rate:  0.00055\n",
      "2/2 [==============================] - 3s 524ms/step - loss: 0.8738 - accuracy: 0.9306 - val_loss: 2.2105 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 57/100\n",
      "Learning rate:  0.0005600000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8689 - accuracy: 0.9306 - val_loss: 1.7706 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 58/100\n",
      "Learning rate:  0.00057\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8082 - accuracy: 0.9583 - val_loss: 1.5671 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 59/100\n",
      "Learning rate:  0.00058\n",
      "2/2 [==============================] - 3s 542ms/step - loss: 0.8737 - accuracy: 0.9444 - val_loss: 1.6413 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 60/100\n",
      "Learning rate:  0.00059\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8142 - accuracy: 0.9444 - val_loss: 1.7732 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 61/100\n",
      "Learning rate:  0.0006\n",
      "2/2 [==============================] - 2s 574ms/step - loss: 0.7829 - accuracy: 0.9722 - val_loss: 2.2755 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 62/100\n",
      "Learning rate:  0.00061\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7956 - accuracy: 0.9306 - val_loss: 4.5645 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 63/100\n",
      "Learning rate:  0.00062\n",
      "2/2 [==============================] - 2s 514ms/step - loss: 0.7877 - accuracy: 0.9583 - val_loss: 6.2797 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 64/100\n",
      "Learning rate:  0.00063\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7633 - accuracy: 0.9583 - val_loss: 7.1650 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 65/100\n",
      "Learning rate:  0.00064\n",
      "2/2 [==============================] - 2s 454ms/step - loss: 0.8273 - accuracy: 0.9306 - val_loss: 7.0450 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 66/100\n",
      "Learning rate:  0.0006500000000000001\n",
      "2/2 [==============================] - 2s 456ms/step - loss: 0.8374 - accuracy: 0.9167 - val_loss: 6.5165 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 67/100\n",
      "Learning rate:  0.00066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 2s/step - loss: 0.7843 - accuracy: 0.9306 - val_loss: 6.0695 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 68/100\n",
      "Learning rate:  0.00067\n",
      "2/2 [==============================] - 2s 447ms/step - loss: 0.7594 - accuracy: 0.9444 - val_loss: 4.8038 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 69/100\n",
      "Learning rate:  0.00068\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7449 - accuracy: 0.9444 - val_loss: 3.0870 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 70/100\n",
      "Learning rate:  0.00069\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8094 - accuracy: 0.9167 - val_loss: 1.6448 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 71/100\n",
      "Learning rate:  0.0007\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8119 - accuracy: 0.9028 - val_loss: 3.2571 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 72/100\n",
      "Learning rate:  0.00071\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8012 - accuracy: 0.9167 - val_loss: 4.3458 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 73/100\n",
      "Learning rate:  0.0007199999999999999\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8525 - accuracy: 0.9167 - val_loss: 5.8768 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 74/100\n",
      "Learning rate:  0.00073\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9341 - accuracy: 0.8611 - val_loss: 5.0432 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 75/100\n",
      "Learning rate:  0.00074\n",
      "2/2 [==============================] - 2s 499ms/step - loss: 0.8649 - accuracy: 0.8889 - val_loss: 4.3198 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 76/100\n",
      "Learning rate:  0.00075\n",
      "2/2 [==============================] - 2s 533ms/step - loss: 0.8465 - accuracy: 0.9028 - val_loss: 4.0470 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 77/100\n",
      "Learning rate:  0.00076\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7448 - accuracy: 0.9722 - val_loss: 4.4487 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 78/100\n",
      "Learning rate:  0.0007700000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7499 - accuracy: 0.9306 - val_loss: 4.7102 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 79/100\n",
      "Learning rate:  0.0007800000000000001\n",
      "2/2 [==============================] - 2s 493ms/step - loss: 0.7910 - accuracy: 0.9583 - val_loss: 3.7288 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 80/100\n",
      "Learning rate:  0.00079\n",
      "2/2 [==============================] - 2s 455ms/step - loss: 0.7531 - accuracy: 0.9583 - val_loss: 2.4851 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 81/100\n",
      "Learning rate:  0.0008\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8870 - accuracy: 0.8750 - val_loss: 2.2981 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 82/100\n",
      "Learning rate:  8.1e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8473 - accuracy: 0.8750 - val_loss: 2.2695 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 83/100\n",
      "Learning rate:  8.2e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8079 - accuracy: 0.9306 - val_loss: 2.1837 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 84/100\n",
      "Learning rate:  8.3e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8316 - accuracy: 0.8889 - val_loss: 2.1040 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 85/100\n",
      "Learning rate:  8.4e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8305 - accuracy: 0.8889 - val_loss: 2.1203 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 86/100\n",
      "Learning rate:  8.5e-05\n",
      "2/2 [==============================] - 2s 456ms/step - loss: 0.7531 - accuracy: 0.9167 - val_loss: 2.1169 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 87/100\n",
      "Learning rate:  8.6e-05\n",
      "2/2 [==============================] - 2s 447ms/step - loss: 0.7612 - accuracy: 0.9583 - val_loss: 2.0962 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 88/100\n",
      "Learning rate:  8.7e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7902 - accuracy: 0.9028 - val_loss: 2.0797 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 89/100\n",
      "Learning rate:  8.800000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7516 - accuracy: 0.9444 - val_loss: 2.0762 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 90/100\n",
      "Learning rate:  8.900000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7330 - accuracy: 0.9722 - val_loss: 2.0576 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 91/100\n",
      "Learning rate:  9e-05\n",
      "2/2 [==============================] - 2s 447ms/step - loss: 0.7471 - accuracy: 0.9306 - val_loss: 2.0294 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 92/100\n",
      "Learning rate:  9.1e-05\n",
      "2/2 [==============================] - 2s 454ms/step - loss: 0.7404 - accuracy: 0.9583 - val_loss: 1.9907 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 93/100\n",
      "Learning rate:  9.200000000000001e-05\n",
      "2/2 [==============================] - 2s 601ms/step - loss: 0.7401 - accuracy: 0.9306 - val_loss: 1.9105 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 94/100\n",
      "Learning rate:  9.300000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8219 - accuracy: 0.9306 - val_loss: 1.9284 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 95/100\n",
      "Learning rate:  9.4e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7301 - accuracy: 0.9583 - val_loss: 1.8471 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 96/100\n",
      "Learning rate:  9.5e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7512 - accuracy: 0.9306 - val_loss: 1.7562 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 97/100\n",
      "Learning rate:  9.6e-05\n",
      "2/2 [==============================] - 2s 468ms/step - loss: 0.7096 - accuracy: 0.9722 - val_loss: 1.6459 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 98/100\n",
      "Learning rate:  9.7e-05\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.7135 - accuracy: 0.9722 - val_loss: 1.5471 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 99/100\n",
      "Learning rate:  9.8e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7113 - accuracy: 0.9722 - val_loss: 1.5052 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 100/100\n",
      "Learning rate:  9.900000000000001e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 2s/step - loss: 0.6891 - accuracy: 0.9583 - val_loss: 1.4146 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.7699 - accuracy: 0.9310\n",
      "Test loss: 0.7698709964752197\n",
      "Test accuracy: 0.931034505367279\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision:  0.9373040752351096\n",
      "Recall:  0.9310344827586207\n",
      "F1 Score:  0.9283661740558292\n",
      "26\n",
      "26\n",
      "[[[[ 76  84  95 ...  76  84  95]\n",
      "   [ 15  15  20 ...  15  15  20]\n",
      "   [  8   7  11 ...   8   7  11]\n",
      "   ...\n",
      "   [ 16  14  25 ...  16  14  25]\n",
      "   [ 13  13  22 ...  13  13  22]\n",
      "   [  8   9  13 ...   8   9  13]]\n",
      "\n",
      "  [[ 88  96 107 ...  88  96 107]\n",
      "   [ 17  17  22 ...  17  17  22]\n",
      "   [  6   5   9 ...   6   5   9]\n",
      "   ...\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 15  13  24 ...  15  13  24]\n",
      "   [ 10  10  16 ...  10  10  16]]\n",
      "\n",
      "  [[ 86  91  98 ...  86  91  98]\n",
      "   [ 14  13  17 ...  14  13  17]\n",
      "   [  5   4   7 ...   5   4   7]\n",
      "   ...\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 12  11  18 ...  12  11  18]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[157 162 174 ... 157 162 174]\n",
      "   [159 165 176 ... 159 165 176]\n",
      "   [155 158 159 ... 155 158 159]\n",
      "   ...\n",
      "   [126 136 168 ... 126 136 168]\n",
      "   [145 153 178 ... 145 153 178]\n",
      "   [150 157 180 ... 150 157 180]]\n",
      "\n",
      "  [[158 162 174 ... 158 162 174]\n",
      "   [158 164 177 ... 158 164 177]\n",
      "   [160 167 179 ... 160 167 179]\n",
      "   ...\n",
      "   [134 144 172 ... 134 144 172]\n",
      "   [143 151 177 ... 143 151 177]\n",
      "   [151 158 180 ... 151 158 180]]\n",
      "\n",
      "  [[160 164 176 ... 160 164 176]\n",
      "   [158 164 177 ... 158 164 177]\n",
      "   [163 169 181 ... 163 169 181]\n",
      "   ...\n",
      "   [141 150 174 ... 141 150 174]\n",
      "   [140 149 176 ... 140 149 176]\n",
      "   [150 158 180 ... 150 158 180]]]\n",
      "\n",
      "\n",
      " [[[ 30  95 105 ...  30  95 105]\n",
      "   [ 31  97 107 ...  31  97 107]\n",
      "   [ 30  96 106 ...  30  96 106]\n",
      "   ...\n",
      "   [ 26  91 103 ...  26  91 103]\n",
      "   [ 25  92 102 ...  25  92 102]\n",
      "   [ 24  91 101 ...  24  91 101]]\n",
      "\n",
      "  [[ 27 111 122 ...  27 111 122]\n",
      "   [ 27 111 123 ...  27 111 123]\n",
      "   [ 28 111 123 ...  28 111 123]\n",
      "   ...\n",
      "   [ 22 107 119 ...  22 107 119]\n",
      "   [ 22 108 118 ...  22 108 118]\n",
      "   [ 23 108 119 ...  23 108 119]]\n",
      "\n",
      "  [[ 24 105 116 ...  24 105 116]\n",
      "   [ 25 107 118 ...  25 107 118]\n",
      "   [ 28 108 119 ...  28 108 119]\n",
      "   ...\n",
      "   [ 26 107 117 ...  26 107 117]\n",
      "   [ 23 105 115 ...  23 105 115]\n",
      "   [ 22 104 116 ...  22 104 116]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  7  29  34 ...   7  29  34]\n",
      "   [ 87  67  56 ...  87  67  56]\n",
      "   [179 128 119 ... 179 128 119]\n",
      "   ...\n",
      "   [195 132 133 ... 195 132 133]\n",
      "   [119  77  72 ... 119  77  72]\n",
      "   [ 50  35  35 ...  50  35  35]]\n",
      "\n",
      "  [[  1  41  49 ...   1  41  49]\n",
      "   [ 72  61  52 ...  72  61  52]\n",
      "   [169 118 107 ... 169 118 107]\n",
      "   ...\n",
      "   [186 131 126 ... 186 131 126]\n",
      "   [ 97  65  58 ...  97  65  58]\n",
      "   [ 30  27  31 ...  30  27  31]]\n",
      "\n",
      "  [[  5  56  65 ...   5  56  65]\n",
      "   [ 63  60  53 ...  63  60  53]\n",
      "   [161 113 103 ... 161 113 103]\n",
      "   ...\n",
      "   [176 123 117 ... 176 123 117]\n",
      "   [ 76  58  54 ...  76  58  54]\n",
      "   [ 19  35  42 ...  19  35  42]]]\n",
      "\n",
      "\n",
      " [[[  9   9   2 ...   9   9   2]\n",
      "   [  6   6   2 ...   6   6   2]\n",
      "   [  4   3   1 ...   4   3   1]\n",
      "   ...\n",
      "   [  0   1   1 ...   0   1   1]\n",
      "   [  1   2   2 ...   1   2   2]\n",
      "   [  9  10   7 ...   9  10   7]]\n",
      "\n",
      "  [[  9   8   4 ...   9   8   4]\n",
      "   [  9   9   4 ...   9   9   4]\n",
      "   [  9   8   3 ...   9   8   3]\n",
      "   ...\n",
      "   [  1   1   1 ...   1   1   1]\n",
      "   [  4   4   3 ...   4   4   3]\n",
      "   [  4   5   3 ...   4   5   3]]\n",
      "\n",
      "  [[ 26  23  14 ...  26  23  14]\n",
      "   [ 33  27  15 ...  33  27  15]\n",
      "   [ 45  37  24 ...  45  37  24]\n",
      "   ...\n",
      "   [  2   3   2 ...   2   3   2]\n",
      "   [  2   2   2 ...   2   2   2]\n",
      "   [  4   4   3 ...   4   4   3]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[188 178 161 ... 188 178 161]\n",
      "   [190 180 162 ... 190 180 162]\n",
      "   [187 177 158 ... 187 177 158]\n",
      "   ...\n",
      "   [150 142 124 ... 150 142 124]\n",
      "   [145 135 118 ... 145 135 118]\n",
      "   [145 135 117 ... 145 135 117]]\n",
      "\n",
      "  [[197 186 170 ... 197 186 170]\n",
      "   [194 184 168 ... 194 184 168]\n",
      "   [192 181 164 ... 192 181 164]\n",
      "   ...\n",
      "   [148 139 120 ... 148 139 120]\n",
      "   [149 138 119 ... 149 138 119]\n",
      "   [147 136 117 ... 147 136 117]]\n",
      "\n",
      "  [[201 191 174 ... 201 191 174]\n",
      "   [198 188 172 ... 198 188 172]\n",
      "   [194 183 167 ... 194 183 167]\n",
      "   ...\n",
      "   [150 139 120 ... 150 139 120]\n",
      "   [152 141 121 ... 152 141 121]\n",
      "   [152 142 124 ... 152 142 124]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (78, 40, 30, 9)\n",
      "78 train samples\n",
      "26 test samples\n",
      "y_train shape: (78, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_30\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_31 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_930 (Conv2D)             (None, 40, 30, 16)   1312        input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_840 (BatchN (None, 40, 30, 16)   64          conv2d_930[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_840 (Activation)     (None, 40, 30, 16)   0           batch_normalization_840[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_931 (Conv2D)             (None, 40, 30, 16)   272         activation_840[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_841 (BatchN (None, 40, 30, 16)   64          conv2d_931[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_841 (Activation)     (None, 40, 30, 16)   0           batch_normalization_841[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_932 (Conv2D)             (None, 40, 30, 16)   2320        activation_841[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_842 (BatchN (None, 40, 30, 16)   64          conv2d_932[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_842 (Activation)     (None, 40, 30, 16)   0           batch_normalization_842[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_934 (Conv2D)             (None, 40, 30, 64)   1088        activation_840[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_933 (Conv2D)             (None, 40, 30, 64)   1088        activation_842[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_270 (Add)                   (None, 40, 30, 64)   0           conv2d_934[0][0]                 \n",
      "                                                                 conv2d_933[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_843 (BatchN (None, 40, 30, 64)   256         add_270[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_843 (Activation)     (None, 40, 30, 64)   0           batch_normalization_843[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_935 (Conv2D)             (None, 40, 30, 16)   1040        activation_843[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_844 (BatchN (None, 40, 30, 16)   64          conv2d_935[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_844 (Activation)     (None, 40, 30, 16)   0           batch_normalization_844[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_936 (Conv2D)             (None, 40, 30, 16)   2320        activation_844[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_845 (BatchN (None, 40, 30, 16)   64          conv2d_936[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_845 (Activation)     (None, 40, 30, 16)   0           batch_normalization_845[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_937 (Conv2D)             (None, 40, 30, 64)   1088        activation_845[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_271 (Add)                   (None, 40, 30, 64)   0           add_270[0][0]                    \n",
      "                                                                 conv2d_937[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_846 (BatchN (None, 40, 30, 64)   256         add_271[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_846 (Activation)     (None, 40, 30, 64)   0           batch_normalization_846[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_938 (Conv2D)             (None, 40, 30, 16)   1040        activation_846[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_847 (BatchN (None, 40, 30, 16)   64          conv2d_938[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_847 (Activation)     (None, 40, 30, 16)   0           batch_normalization_847[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_939 (Conv2D)             (None, 40, 30, 16)   2320        activation_847[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_848 (BatchN (None, 40, 30, 16)   64          conv2d_939[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_848 (Activation)     (None, 40, 30, 16)   0           batch_normalization_848[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_940 (Conv2D)             (None, 40, 30, 64)   1088        activation_848[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_272 (Add)                   (None, 40, 30, 64)   0           add_271[0][0]                    \n",
      "                                                                 conv2d_940[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_849 (BatchN (None, 40, 30, 64)   256         add_272[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_849 (Activation)     (None, 40, 30, 64)   0           batch_normalization_849[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_941 (Conv2D)             (None, 20, 15, 64)   4160        activation_849[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_850 (BatchN (None, 20, 15, 64)   256         conv2d_941[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_850 (Activation)     (None, 20, 15, 64)   0           batch_normalization_850[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_942 (Conv2D)             (None, 20, 15, 64)   36928       activation_850[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_851 (BatchN (None, 20, 15, 64)   256         conv2d_942[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_851 (Activation)     (None, 20, 15, 64)   0           batch_normalization_851[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_944 (Conv2D)             (None, 20, 15, 128)  8320        add_272[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_943 (Conv2D)             (None, 20, 15, 128)  8320        activation_851[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_273 (Add)                   (None, 20, 15, 128)  0           conv2d_944[0][0]                 \n",
      "                                                                 conv2d_943[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_852 (BatchN (None, 20, 15, 128)  512         add_273[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_852 (Activation)     (None, 20, 15, 128)  0           batch_normalization_852[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_945 (Conv2D)             (None, 20, 15, 64)   8256        activation_852[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_853 (BatchN (None, 20, 15, 64)   256         conv2d_945[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_853 (Activation)     (None, 20, 15, 64)   0           batch_normalization_853[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_946 (Conv2D)             (None, 20, 15, 64)   36928       activation_853[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_854 (BatchN (None, 20, 15, 64)   256         conv2d_946[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_854 (Activation)     (None, 20, 15, 64)   0           batch_normalization_854[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_947 (Conv2D)             (None, 20, 15, 128)  8320        activation_854[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_274 (Add)                   (None, 20, 15, 128)  0           add_273[0][0]                    \n",
      "                                                                 conv2d_947[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_855 (BatchN (None, 20, 15, 128)  512         add_274[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_855 (Activation)     (None, 20, 15, 128)  0           batch_normalization_855[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_948 (Conv2D)             (None, 20, 15, 64)   8256        activation_855[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_856 (BatchN (None, 20, 15, 64)   256         conv2d_948[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_856 (Activation)     (None, 20, 15, 64)   0           batch_normalization_856[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_949 (Conv2D)             (None, 20, 15, 64)   36928       activation_856[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_857 (BatchN (None, 20, 15, 64)   256         conv2d_949[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_857 (Activation)     (None, 20, 15, 64)   0           batch_normalization_857[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_950 (Conv2D)             (None, 20, 15, 128)  8320        activation_857[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_275 (Add)                   (None, 20, 15, 128)  0           add_274[0][0]                    \n",
      "                                                                 conv2d_950[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_858 (BatchN (None, 20, 15, 128)  512         add_275[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_858 (Activation)     (None, 20, 15, 128)  0           batch_normalization_858[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_951 (Conv2D)             (None, 10, 8, 128)   16512       activation_858[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_859 (BatchN (None, 10, 8, 128)   512         conv2d_951[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_859 (Activation)     (None, 10, 8, 128)   0           batch_normalization_859[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_952 (Conv2D)             (None, 10, 8, 128)   147584      activation_859[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_860 (BatchN (None, 10, 8, 128)   512         conv2d_952[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_860 (Activation)     (None, 10, 8, 128)   0           batch_normalization_860[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_954 (Conv2D)             (None, 10, 8, 256)   33024       add_275[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_953 (Conv2D)             (None, 10, 8, 256)   33024       activation_860[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_276 (Add)                   (None, 10, 8, 256)   0           conv2d_954[0][0]                 \n",
      "                                                                 conv2d_953[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_861 (BatchN (None, 10, 8, 256)   1024        add_276[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_861 (Activation)     (None, 10, 8, 256)   0           batch_normalization_861[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_955 (Conv2D)             (None, 10, 8, 128)   32896       activation_861[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_862 (BatchN (None, 10, 8, 128)   512         conv2d_955[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_862 (Activation)     (None, 10, 8, 128)   0           batch_normalization_862[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_956 (Conv2D)             (None, 10, 8, 128)   147584      activation_862[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_863 (BatchN (None, 10, 8, 128)   512         conv2d_956[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_863 (Activation)     (None, 10, 8, 128)   0           batch_normalization_863[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_957 (Conv2D)             (None, 10, 8, 256)   33024       activation_863[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_277 (Add)                   (None, 10, 8, 256)   0           add_276[0][0]                    \n",
      "                                                                 conv2d_957[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_864 (BatchN (None, 10, 8, 256)   1024        add_277[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_864 (Activation)     (None, 10, 8, 256)   0           batch_normalization_864[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_958 (Conv2D)             (None, 10, 8, 128)   32896       activation_864[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_865 (BatchN (None, 10, 8, 128)   512         conv2d_958[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_865 (Activation)     (None, 10, 8, 128)   0           batch_normalization_865[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_959 (Conv2D)             (None, 10, 8, 128)   147584      activation_865[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_866 (BatchN (None, 10, 8, 128)   512         conv2d_959[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_866 (Activation)     (None, 10, 8, 128)   0           batch_normalization_866[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_960 (Conv2D)             (None, 10, 8, 256)   33024       activation_866[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_278 (Add)                   (None, 10, 8, 256)   0           add_277[0][0]                    \n",
      "                                                                 conv2d_960[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_867 (BatchN (None, 10, 8, 256)   1024        add_278[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_867 (Activation)     (None, 10, 8, 256)   0           batch_normalization_867[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_30 (AveragePo (None, 1, 1, 256)    0           activation_867[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 256)          0           average_pooling2d_30[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 10)           2570        flatten_30[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[0.29803923 0.32941177 0.37254903 ... 0.29803923 0.32941177\n",
      "    0.37254903]\n",
      "   [0.05882353 0.05882353 0.07843138 ... 0.05882353 0.05882353\n",
      "    0.07843138]\n",
      "   [0.03137255 0.02745098 0.04313726 ... 0.03137255 0.02745098\n",
      "    0.04313726]\n",
      "   ...\n",
      "   [0.0627451  0.05490196 0.09803922 ... 0.0627451  0.05490196\n",
      "    0.09803922]\n",
      "   [0.05098039 0.05098039 0.08627451 ... 0.05098039 0.05098039\n",
      "    0.08627451]\n",
      "   [0.03137255 0.03529412 0.05098039 ... 0.03137255 0.03529412\n",
      "    0.05098039]]\n",
      "\n",
      "  [[0.34509805 0.3764706  0.41960785 ... 0.34509805 0.3764706\n",
      "    0.41960785]\n",
      "   [0.06666667 0.06666667 0.08627451 ... 0.06666667 0.06666667\n",
      "    0.08627451]\n",
      "   [0.02352941 0.01960784 0.03529412 ... 0.02352941 0.01960784\n",
      "    0.03529412]\n",
      "   ...\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.05882353 0.05098039 0.09411765 ... 0.05882353 0.05098039\n",
      "    0.09411765]\n",
      "   [0.03921569 0.03921569 0.0627451  ... 0.03921569 0.03921569\n",
      "    0.0627451 ]]\n",
      "\n",
      "  [[0.3372549  0.35686275 0.38431373 ... 0.3372549  0.35686275\n",
      "    0.38431373]\n",
      "   [0.05490196 0.05098039 0.06666667 ... 0.05490196 0.05098039\n",
      "    0.06666667]\n",
      "   [0.01960784 0.01568628 0.02745098 ... 0.01960784 0.01568628\n",
      "    0.02745098]\n",
      "   ...\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.04705882 0.04313726 0.07058824 ... 0.04705882 0.04313726\n",
      "    0.07058824]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6156863  0.63529414 0.68235296 ... 0.6156863  0.63529414\n",
      "    0.68235296]\n",
      "   [0.62352943 0.64705884 0.6901961  ... 0.62352943 0.64705884\n",
      "    0.6901961 ]\n",
      "   [0.60784316 0.61960787 0.62352943 ... 0.60784316 0.61960787\n",
      "    0.62352943]\n",
      "   ...\n",
      "   [0.49411765 0.53333336 0.65882355 ... 0.49411765 0.53333336\n",
      "    0.65882355]\n",
      "   [0.5686275  0.6        0.69803923 ... 0.5686275  0.6\n",
      "    0.69803923]\n",
      "   [0.5882353  0.6156863  0.7058824  ... 0.5882353  0.6156863\n",
      "    0.7058824 ]]\n",
      "\n",
      "  [[0.61960787 0.63529414 0.68235296 ... 0.61960787 0.63529414\n",
      "    0.68235296]\n",
      "   [0.61960787 0.6431373  0.69411767 ... 0.61960787 0.6431373\n",
      "    0.69411767]\n",
      "   [0.627451   0.654902   0.7019608  ... 0.627451   0.654902\n",
      "    0.7019608 ]\n",
      "   ...\n",
      "   [0.5254902  0.5647059  0.6745098  ... 0.5254902  0.5647059\n",
      "    0.6745098 ]\n",
      "   [0.56078434 0.5921569  0.69411767 ... 0.56078434 0.5921569\n",
      "    0.69411767]\n",
      "   [0.5921569  0.61960787 0.7058824  ... 0.5921569  0.61960787\n",
      "    0.7058824 ]]\n",
      "\n",
      "  [[0.627451   0.6431373  0.6901961  ... 0.627451   0.6431373\n",
      "    0.6901961 ]\n",
      "   [0.61960787 0.6431373  0.69411767 ... 0.61960787 0.6431373\n",
      "    0.69411767]\n",
      "   [0.6392157  0.6627451  0.70980394 ... 0.6392157  0.6627451\n",
      "    0.70980394]\n",
      "   ...\n",
      "   [0.5529412  0.5882353  0.68235296 ... 0.5529412  0.5882353\n",
      "    0.68235296]\n",
      "   [0.54901963 0.58431375 0.6901961  ... 0.54901963 0.58431375\n",
      "    0.6901961 ]\n",
      "   [0.5882353  0.61960787 0.7058824  ... 0.5882353  0.61960787\n",
      "    0.7058824 ]]]\n",
      "\n",
      "\n",
      " [[[0.11764706 0.37254903 0.4117647  ... 0.11764706 0.37254903\n",
      "    0.4117647 ]\n",
      "   [0.12156863 0.38039216 0.41960785 ... 0.12156863 0.38039216\n",
      "    0.41960785]\n",
      "   [0.11764706 0.3764706  0.41568628 ... 0.11764706 0.3764706\n",
      "    0.41568628]\n",
      "   ...\n",
      "   [0.10196079 0.35686275 0.40392157 ... 0.10196079 0.35686275\n",
      "    0.40392157]\n",
      "   [0.09803922 0.36078432 0.4        ... 0.09803922 0.36078432\n",
      "    0.4       ]\n",
      "   [0.09411765 0.35686275 0.39607844 ... 0.09411765 0.35686275\n",
      "    0.39607844]]\n",
      "\n",
      "  [[0.10588235 0.43529412 0.47843137 ... 0.10588235 0.43529412\n",
      "    0.47843137]\n",
      "   [0.10588235 0.43529412 0.48235294 ... 0.10588235 0.43529412\n",
      "    0.48235294]\n",
      "   [0.10980392 0.43529412 0.48235294 ... 0.10980392 0.43529412\n",
      "    0.48235294]\n",
      "   ...\n",
      "   [0.08627451 0.41960785 0.46666667 ... 0.08627451 0.41960785\n",
      "    0.46666667]\n",
      "   [0.08627451 0.42352942 0.4627451  ... 0.08627451 0.42352942\n",
      "    0.4627451 ]\n",
      "   [0.09019608 0.42352942 0.46666667 ... 0.09019608 0.42352942\n",
      "    0.46666667]]\n",
      "\n",
      "  [[0.09411765 0.4117647  0.45490196 ... 0.09411765 0.4117647\n",
      "    0.45490196]\n",
      "   [0.09803922 0.41960785 0.4627451  ... 0.09803922 0.41960785\n",
      "    0.4627451 ]\n",
      "   [0.10980392 0.42352942 0.46666667 ... 0.10980392 0.42352942\n",
      "    0.46666667]\n",
      "   ...\n",
      "   [0.10196079 0.41960785 0.45882353 ... 0.10196079 0.41960785\n",
      "    0.45882353]\n",
      "   [0.09019608 0.4117647  0.4509804  ... 0.09019608 0.4117647\n",
      "    0.4509804 ]\n",
      "   [0.08627451 0.40784314 0.45490196 ... 0.08627451 0.40784314\n",
      "    0.45490196]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.02745098 0.11372549 0.13333334 ... 0.02745098 0.11372549\n",
      "    0.13333334]\n",
      "   [0.34117648 0.2627451  0.21960784 ... 0.34117648 0.2627451\n",
      "    0.21960784]\n",
      "   [0.7019608  0.5019608  0.46666667 ... 0.7019608  0.5019608\n",
      "    0.46666667]\n",
      "   ...\n",
      "   [0.7647059  0.5176471  0.52156866 ... 0.7647059  0.5176471\n",
      "    0.52156866]\n",
      "   [0.46666667 0.3019608  0.28235295 ... 0.46666667 0.3019608\n",
      "    0.28235295]\n",
      "   [0.19607843 0.13725491 0.13725491 ... 0.19607843 0.13725491\n",
      "    0.13725491]]\n",
      "\n",
      "  [[0.00392157 0.16078432 0.19215687 ... 0.00392157 0.16078432\n",
      "    0.19215687]\n",
      "   [0.28235295 0.23921569 0.20392157 ... 0.28235295 0.23921569\n",
      "    0.20392157]\n",
      "   [0.6627451  0.4627451  0.41960785 ... 0.6627451  0.4627451\n",
      "    0.41960785]\n",
      "   ...\n",
      "   [0.7294118  0.5137255  0.49411765 ... 0.7294118  0.5137255\n",
      "    0.49411765]\n",
      "   [0.38039216 0.25490198 0.22745098 ... 0.38039216 0.25490198\n",
      "    0.22745098]\n",
      "   [0.11764706 0.10588235 0.12156863 ... 0.11764706 0.10588235\n",
      "    0.12156863]]\n",
      "\n",
      "  [[0.01960784 0.21960784 0.25490198 ... 0.01960784 0.21960784\n",
      "    0.25490198]\n",
      "   [0.24705882 0.23529412 0.20784314 ... 0.24705882 0.23529412\n",
      "    0.20784314]\n",
      "   [0.6313726  0.44313726 0.40392157 ... 0.6313726  0.44313726\n",
      "    0.40392157]\n",
      "   ...\n",
      "   [0.6901961  0.48235294 0.45882353 ... 0.6901961  0.48235294\n",
      "    0.45882353]\n",
      "   [0.29803923 0.22745098 0.21176471 ... 0.29803923 0.22745098\n",
      "    0.21176471]\n",
      "   [0.07450981 0.13725491 0.16470589 ... 0.07450981 0.13725491\n",
      "    0.16470589]]]\n",
      "\n",
      "\n",
      " [[[0.03529412 0.03529412 0.00784314 ... 0.03529412 0.03529412\n",
      "    0.00784314]\n",
      "   [0.02352941 0.02352941 0.00784314 ... 0.02352941 0.02352941\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01176471 0.00392157 ... 0.01568628 0.01176471\n",
      "    0.00392157]\n",
      "   ...\n",
      "   [0.         0.00392157 0.00392157 ... 0.         0.00392157\n",
      "    0.00392157]\n",
      "   [0.00392157 0.00784314 0.00784314 ... 0.00392157 0.00784314\n",
      "    0.00784314]\n",
      "   [0.03529412 0.03921569 0.02745098 ... 0.03529412 0.03921569\n",
      "    0.02745098]]\n",
      "\n",
      "  [[0.03529412 0.03137255 0.01568628 ... 0.03529412 0.03137255\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03529412 0.01568628 ... 0.03529412 0.03529412\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03137255 0.01176471 ... 0.03529412 0.03137255\n",
      "    0.01176471]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157 ... 0.00392157 0.00392157\n",
      "    0.00392157]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]\n",
      "   [0.01568628 0.01960784 0.01176471 ... 0.01568628 0.01960784\n",
      "    0.01176471]]\n",
      "\n",
      "  [[0.10196079 0.09019608 0.05490196 ... 0.10196079 0.09019608\n",
      "    0.05490196]\n",
      "   [0.12941177 0.10588235 0.05882353 ... 0.12941177 0.10588235\n",
      "    0.05882353]\n",
      "   [0.1764706  0.14509805 0.09411765 ... 0.1764706  0.14509805\n",
      "    0.09411765]\n",
      "   ...\n",
      "   [0.00784314 0.01176471 0.00784314 ... 0.00784314 0.01176471\n",
      "    0.00784314]\n",
      "   [0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.7372549  0.69803923 0.6313726  ... 0.7372549  0.69803923\n",
      "    0.6313726 ]\n",
      "   [0.74509805 0.7058824  0.63529414 ... 0.74509805 0.7058824\n",
      "    0.63529414]\n",
      "   [0.73333335 0.69411767 0.61960787 ... 0.73333335 0.69411767\n",
      "    0.61960787]\n",
      "   ...\n",
      "   [0.5882353  0.5568628  0.4862745  ... 0.5882353  0.5568628\n",
      "    0.4862745 ]\n",
      "   [0.5686275  0.5294118  0.4627451  ... 0.5686275  0.5294118\n",
      "    0.4627451 ]\n",
      "   [0.5686275  0.5294118  0.45882353 ... 0.5686275  0.5294118\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.77254903 0.7294118  0.6666667  ... 0.77254903 0.7294118\n",
      "    0.6666667 ]\n",
      "   [0.7607843  0.72156864 0.65882355 ... 0.7607843  0.72156864\n",
      "    0.65882355]\n",
      "   [0.7529412  0.70980394 0.6431373  ... 0.7529412  0.70980394\n",
      "    0.6431373 ]\n",
      "   ...\n",
      "   [0.5803922  0.54509807 0.47058824 ... 0.5803922  0.54509807\n",
      "    0.47058824]\n",
      "   [0.58431375 0.5411765  0.46666667 ... 0.58431375 0.5411765\n",
      "    0.46666667]\n",
      "   [0.5764706  0.53333336 0.45882353 ... 0.5764706  0.53333336\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.7882353  0.7490196  0.68235296 ... 0.7882353  0.7490196\n",
      "    0.68235296]\n",
      "   [0.7764706  0.7372549  0.6745098  ... 0.7764706  0.7372549\n",
      "    0.6745098 ]\n",
      "   [0.7607843  0.7176471  0.654902   ... 0.7607843  0.7176471\n",
      "    0.654902  ]\n",
      "   ...\n",
      "   [0.5882353  0.54509807 0.47058824 ... 0.5882353  0.54509807\n",
      "    0.47058824]\n",
      "   [0.59607846 0.5529412  0.4745098  ... 0.59607846 0.5529412\n",
      "    0.4745098 ]\n",
      "   [0.59607846 0.5568628  0.4862745  ... 0.59607846 0.5568628\n",
      "    0.4862745 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157 ... 0.00392157 0.00392157\n",
      "    0.00392157]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.61960787 0.62352943 0.6784314  ... 0.61960787 0.62352943\n",
      "    0.6784314 ]\n",
      "   [0.6784314  0.6862745  0.73333335 ... 0.6784314  0.6862745\n",
      "    0.73333335]\n",
      "   [0.5019608  0.5058824  0.5568628  ... 0.5019608  0.5058824\n",
      "    0.5568628 ]\n",
      "   ...\n",
      "   [0.25490198 0.26666668 0.30588236 ... 0.25490198 0.26666668\n",
      "    0.30588236]\n",
      "   [0.16078432 0.16078432 0.21176471 ... 0.16078432 0.16078432\n",
      "    0.21176471]\n",
      "   [0.15686275 0.15294118 0.2        ... 0.15686275 0.15294118\n",
      "    0.2       ]]\n",
      "\n",
      "  [[0.59607846 0.6039216  0.65882355 ... 0.59607846 0.6039216\n",
      "    0.65882355]\n",
      "   [0.6        0.60784316 0.6627451  ... 0.6        0.60784316\n",
      "    0.6627451 ]\n",
      "   [0.4627451  0.46666667 0.52156866 ... 0.4627451  0.46666667\n",
      "    0.52156866]\n",
      "   ...\n",
      "   [0.16078432 0.16862746 0.19607843 ... 0.16078432 0.16862746\n",
      "    0.19607843]\n",
      "   [0.1254902  0.12156863 0.16470589 ... 0.1254902  0.12156863\n",
      "    0.16470589]\n",
      "   [0.12156863 0.12156863 0.16078432 ... 0.12156863 0.12156863\n",
      "    0.16078432]]\n",
      "\n",
      "  [[0.49411765 0.49019608 0.5568628  ... 0.49411765 0.49019608\n",
      "    0.5568628 ]\n",
      "   [0.5019608  0.49803922 0.5647059  ... 0.5019608  0.49803922\n",
      "    0.5647059 ]\n",
      "   [0.4745098  0.47058824 0.5294118  ... 0.4745098  0.47058824\n",
      "    0.5294118 ]\n",
      "   ...\n",
      "   [0.02745098 0.02352941 0.03137255 ... 0.02745098 0.02352941\n",
      "    0.03137255]\n",
      "   [0.09803922 0.09019608 0.1254902  ... 0.09803922 0.09019608\n",
      "    0.1254902 ]\n",
      "   [0.09411765 0.08627451 0.12156863 ... 0.09411765 0.08627451\n",
      "    0.12156863]]]\n",
      "\n",
      "\n",
      " [[[0.04705882 0.04705882 0.04705882 ... 0.04705882 0.04705882\n",
      "    0.04705882]\n",
      "   [0.01176471 0.01176471 0.01176471 ... 0.01176471 0.01176471\n",
      "    0.01176471]\n",
      "   [0.03529412 0.03529412 0.03529412 ... 0.03529412 0.03529412\n",
      "    0.03529412]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.01176471 0.01176471 0.01176471 ... 0.01176471 0.01176471\n",
      "    0.01176471]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.99215686 0.99215686 0.99215686 ... 0.99215686 0.99215686\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6509804  0.6117647  0.60784316 ... 0.6509804  0.6117647\n",
      "    0.60784316]\n",
      "   [0.6431373  0.6        0.5921569  ... 0.6431373  0.6\n",
      "    0.5921569 ]\n",
      "   [0.63529414 0.5921569  0.5803922  ... 0.63529414 0.5921569\n",
      "    0.5803922 ]\n",
      "   ...\n",
      "   [0.80784315 0.69803923 0.67058825 ... 0.80784315 0.69803923\n",
      "    0.67058825]\n",
      "   [0.80784315 0.7176471  0.6901961  ... 0.80784315 0.7176471\n",
      "    0.6901961 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]]\n",
      "\n",
      "  [[0.64705884 0.59607846 0.59607846 ... 0.64705884 0.59607846\n",
      "    0.59607846]\n",
      "   [0.63529414 0.5921569  0.58431375 ... 0.63529414 0.5921569\n",
      "    0.58431375]\n",
      "   [0.62352943 0.5803922  0.57254905 ... 0.62352943 0.5803922\n",
      "    0.57254905]\n",
      "   ...\n",
      "   [0.8        0.69803923 0.6745098  ... 0.8        0.69803923\n",
      "    0.6745098 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]\n",
      "   [0.7921569  0.7019608  0.68235296 ... 0.7921569  0.7019608\n",
      "    0.68235296]]\n",
      "\n",
      "  [[0.6431373  0.59607846 0.5921569  ... 0.6431373  0.59607846\n",
      "    0.5921569 ]\n",
      "   [0.6313726  0.5882353  0.5803922  ... 0.6313726  0.5882353\n",
      "    0.5803922 ]\n",
      "   [0.627451   0.57254905 0.5686275  ... 0.627451   0.57254905\n",
      "    0.5686275 ]\n",
      "   ...\n",
      "   [0.7921569  0.6901961  0.6784314  ... 0.7921569  0.6901961\n",
      "    0.6784314 ]\n",
      "   [0.80784315 0.7058824  0.6901961  ... 0.80784315 0.7058824\n",
      "    0.6901961 ]\n",
      "   [0.80784315 0.7176471  0.7019608  ... 0.80784315 0.7176471\n",
      "    0.7019608 ]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (78, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (78, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Learning rate:  0.0\n",
      "2/2 [==============================] - 6s 1s/step - loss: 3.1644 - accuracy: 0.0128 - val_loss: 3.0009 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/100\n",
      "Learning rate:  1e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 3.1722 - accuracy: 0.0128 - val_loss: 3.0581 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/100\n",
      "Learning rate:  2e-05\n",
      "2/2 [==============================] - 2s 735ms/step - loss: 3.1318 - accuracy: 0.0128 - val_loss: 3.0893 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/100\n",
      "Learning rate:  3e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 3.1096 - accuracy: 0.0128 - val_loss: 3.1141 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/100\n",
      "Learning rate:  4e-05\n",
      "2/2 [==============================] - 2s 630ms/step - loss: 3.0956 - accuracy: 0.0256 - val_loss: 3.1291 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/100\n",
      "Learning rate:  5e-05\n",
      "2/2 [==============================] - 2s 654ms/step - loss: 3.0592 - accuracy: 0.0256 - val_loss: 3.1374 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/100\n",
      "Learning rate:  6e-05\n",
      "2/2 [==============================] - 3s 2s/step - loss: 2.9969 - accuracy: 0.0513 - val_loss: 3.1393 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/100\n",
      "Learning rate:  7.000000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.9592 - accuracy: 0.1154 - val_loss: 3.1335 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/100\n",
      "Learning rate:  8e-05\n",
      "2/2 [==============================] - 3s 693ms/step - loss: 2.9072 - accuracy: 0.1026 - val_loss: 3.1206 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/100\n",
      "Learning rate:  8.999999999999999e-05\n",
      "2/2 [==============================] - 3s 2s/step - loss: 2.8118 - accuracy: 0.1667 - val_loss: 3.1001 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/100\n",
      "Learning rate:  0.0001\n",
      "2/2 [==============================] - 3s 2s/step - loss: 2.7711 - accuracy: 0.1923 - val_loss: 3.0747 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/100\n",
      "Learning rate:  0.00011\n",
      "2/2 [==============================] - 3s 692ms/step - loss: 2.6889 - accuracy: 0.2308 - val_loss: 3.0435 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/100\n",
      "Learning rate:  0.00012\n",
      "2/2 [==============================] - 3s 2s/step - loss: 2.5728 - accuracy: 0.3205 - val_loss: 3.0060 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/100\n",
      "Learning rate:  0.00013000000000000002\n",
      "2/2 [==============================] - 3s 2s/step - loss: 2.4993 - accuracy: 0.3333 - val_loss: 2.9646 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/100\n",
      "Learning rate:  0.00014000000000000001\n",
      "2/2 [==============================] - 3s 748ms/step - loss: 2.4243 - accuracy: 0.4231 - val_loss: 2.9203 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/100\n",
      "Learning rate:  0.00015\n",
      "2/2 [==============================] - 3s 2s/step - loss: 2.3032 - accuracy: 0.5641 - val_loss: 2.8691 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/100\n",
      "Learning rate:  0.00016\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.2014 - accuracy: 0.6282 - val_loss: 2.8181 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/100\n",
      "Learning rate:  0.00017\n",
      "2/2 [==============================] - 2s 632ms/step - loss: 2.1397 - accuracy: 0.5641 - val_loss: 2.7600 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/100\n",
      "Learning rate:  0.00017999999999999998\n",
      "2/2 [==============================] - 3s 2s/step - loss: 2.0181 - accuracy: 0.6410 - val_loss: 2.7153 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/100\n",
      "Learning rate:  0.00019\n",
      "2/2 [==============================] - 3s 674ms/step - loss: 1.9457 - accuracy: 0.6667 - val_loss: 2.6696 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 21/100\n",
      "Learning rate:  0.0002\n",
      "2/2 [==============================] - 3s 2s/step - loss: 1.8117 - accuracy: 0.7436 - val_loss: 2.6197 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 22/100\n",
      "Learning rate:  0.00021\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.7465 - accuracy: 0.7564 - val_loss: 2.5900 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 23/100\n",
      "Learning rate:  0.00022\n",
      "2/2 [==============================] - 3s 760ms/step - loss: 1.7404 - accuracy: 0.7179 - val_loss: 2.5825 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 24/100\n",
      "Learning rate:  0.00023\n",
      "2/2 [==============================] - 3s 738ms/step - loss: 1.6047 - accuracy: 0.7692 - val_loss: 2.6199 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 25/100\n",
      "Learning rate:  0.00024\n",
      "2/2 [==============================] - 3s 658ms/step - loss: 1.5164 - accuracy: 0.7949 - val_loss: 2.6669 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 26/100\n",
      "Learning rate:  0.00025\n",
      "2/2 [==============================] - 3s 2s/step - loss: 1.4597 - accuracy: 0.8205 - val_loss: 2.7064 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 27/100\n",
      "Learning rate:  0.00026000000000000003\n",
      "2/2 [==============================] - 3s 2s/step - loss: 1.3307 - accuracy: 0.8590 - val_loss: 2.7991 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 28/100\n",
      "Learning rate:  0.00027\n",
      "2/2 [==============================] - 3s 648ms/step - loss: 1.3182 - accuracy: 0.8333 - val_loss: 2.9080 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 29/100\n",
      "Learning rate:  0.00028000000000000003\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.2511 - accuracy: 0.8205 - val_loss: 3.0281 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 30/100\n",
      "Learning rate:  0.00029\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.1995 - accuracy: 0.8462 - val_loss: 3.1768 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 31/100\n",
      "Learning rate:  0.0003\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.2375 - accuracy: 0.8462 - val_loss: 3.2983 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 32/100\n",
      "Learning rate:  0.00031\n",
      "2/2 [==============================] - 2s 592ms/step - loss: 1.0980 - accuracy: 0.8718 - val_loss: 3.3384 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 33/100\n",
      "Learning rate:  0.00032\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0991 - accuracy: 0.8333 - val_loss: 3.3481 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "Learning rate:  0.00033\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0799 - accuracy: 0.8462 - val_loss: 3.2677 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 35/100\n",
      "Learning rate:  0.00034\n",
      "2/2 [==============================] - 2s 632ms/step - loss: 1.0579 - accuracy: 0.8205 - val_loss: 2.8662 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 36/100\n",
      "Learning rate:  0.00035\n",
      "2/2 [==============================] - 2s 628ms/step - loss: 0.9991 - accuracy: 0.8846 - val_loss: 2.4524 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 37/100\n",
      "Learning rate:  0.00035999999999999997\n",
      "2/2 [==============================] - 2s 618ms/step - loss: 0.9828 - accuracy: 0.9103 - val_loss: 1.9946 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 38/100\n",
      "Learning rate:  0.00037\n",
      "2/2 [==============================] - 3s 920ms/step - loss: 0.9635 - accuracy: 0.8974 - val_loss: 1.5785 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 39/100\n",
      "Learning rate:  0.00038\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.9005 - accuracy: 0.9359 - val_loss: 1.3877 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 40/100\n",
      "Learning rate:  0.00039000000000000005\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9285 - accuracy: 0.9103 - val_loss: 1.1125 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 41/100\n",
      "Learning rate:  0.0004\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9166 - accuracy: 0.9231 - val_loss: 0.9162 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 42/100\n",
      "Learning rate:  0.00041\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8748 - accuracy: 0.8974 - val_loss: 0.8396 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 43/100\n",
      "Learning rate:  0.00042\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9023 - accuracy: 0.8718 - val_loss: 0.7898 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 44/100\n",
      "Learning rate:  0.00043\n",
      "2/2 [==============================] - 2s 602ms/step - loss: 0.8463 - accuracy: 0.9231 - val_loss: 0.7205 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 45/100\n",
      "Learning rate:  0.00044\n",
      "2/2 [==============================] - 2s 612ms/step - loss: 0.8432 - accuracy: 0.9359 - val_loss: 0.6558 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 46/100\n",
      "Learning rate:  0.00045000000000000004\n",
      "2/2 [==============================] - 2s 598ms/step - loss: 0.8092 - accuracy: 0.9359 - val_loss: 0.6270 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 47/100\n",
      "Learning rate:  0.00046\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7908 - accuracy: 0.9487 - val_loss: 0.6194 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 48/100\n",
      "Learning rate:  0.00047\n",
      "2/2 [==============================] - 3s 738ms/step - loss: 0.7664 - accuracy: 0.9487 - val_loss: 0.6154 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 49/100\n",
      "Learning rate:  0.00048\n",
      "2/2 [==============================] - 3s 693ms/step - loss: 0.8682 - accuracy: 0.8974 - val_loss: 0.6187 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 50/100\n",
      "Learning rate:  0.00049\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7589 - accuracy: 0.9872 - val_loss: 0.6217 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 51/100\n",
      "Learning rate:  0.0005\n",
      "2/2 [==============================] - 3s 759ms/step - loss: 0.7454 - accuracy: 0.9872 - val_loss: 0.6142 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 52/100\n",
      "Learning rate:  0.00051\n",
      "2/2 [==============================] - 3s 770ms/step - loss: 0.7753 - accuracy: 0.9487 - val_loss: 0.6069 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 53/100\n",
      "Learning rate:  0.0005200000000000001\n",
      "2/2 [==============================] - 3s 835ms/step - loss: 0.7517 - accuracy: 0.9744 - val_loss: 0.6040 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 54/100\n",
      "Learning rate:  0.0005300000000000001\n",
      "2/2 [==============================] - 3s 753ms/step - loss: 0.7293 - accuracy: 0.9615 - val_loss: 0.6027 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 55/100\n",
      "Learning rate:  0.00054\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.7799 - accuracy: 0.9359 - val_loss: 0.6016 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 56/100\n",
      "Learning rate:  0.00055\n",
      "2/2 [==============================] - 3s 912ms/step - loss: 0.7114 - accuracy: 0.9744 - val_loss: 0.6009 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 57/100\n",
      "Learning rate:  0.0005600000000000001\n",
      "2/2 [==============================] - 3s 733ms/step - loss: 0.7365 - accuracy: 0.9487 - val_loss: 0.6004 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 58/100\n",
      "Learning rate:  0.00057\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7296 - accuracy: 0.9487 - val_loss: 0.6049 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 59/100\n",
      "Learning rate:  0.00058\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7477 - accuracy: 0.9487 - val_loss: 1.4086 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 60/100\n",
      "Learning rate:  0.00059\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8105 - accuracy: 0.9615 - val_loss: 0.6403 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 61/100\n",
      "Learning rate:  0.0006\n",
      "2/2 [==============================] - 3s 736ms/step - loss: 0.7424 - accuracy: 0.9487 - val_loss: 0.6060 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 62/100\n",
      "Learning rate:  0.00061\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7250 - accuracy: 0.9615 - val_loss: 0.5990 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 63/100\n",
      "Learning rate:  0.00062\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7590 - accuracy: 0.9615 - val_loss: 0.5976 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 64/100\n",
      "Learning rate:  0.00063\n",
      "2/2 [==============================] - 2s 608ms/step - loss: 0.6984 - accuracy: 0.9744 - val_loss: 0.5974 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 65/100\n",
      "Learning rate:  0.00064\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6846 - accuracy: 0.9872 - val_loss: 0.6003 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 66/100\n",
      "Learning rate:  0.0006500000000000001\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.7510 - accuracy: 0.9615 - val_loss: 0.6092 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 67/100\n",
      "Learning rate:  0.00066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 750ms/step - loss: 0.7948 - accuracy: 0.9359 - val_loss: 0.6198 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 68/100\n",
      "Learning rate:  0.00067\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7421 - accuracy: 0.9615 - val_loss: 0.6149 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 69/100\n",
      "Learning rate:  0.00068\n",
      "2/2 [==============================] - 2s 644ms/step - loss: 0.6888 - accuracy: 0.9872 - val_loss: 0.6065 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 70/100\n",
      "Learning rate:  0.00069\n",
      "2/2 [==============================] - 2s 629ms/step - loss: 0.6736 - accuracy: 0.9872 - val_loss: 0.6021 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 71/100\n",
      "Learning rate:  0.0007\n",
      "2/2 [==============================] - 2s 657ms/step - loss: 0.6758 - accuracy: 0.9744 - val_loss: 0.6008 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 72/100\n",
      "Learning rate:  0.00071\n",
      "2/2 [==============================] - 2s 612ms/step - loss: 0.6724 - accuracy: 0.9872 - val_loss: 0.6020 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 73/100\n",
      "Learning rate:  0.0007199999999999999\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.6725 - accuracy: 0.9744 - val_loss: 0.6057 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 74/100\n",
      "Learning rate:  0.00073\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.6668 - accuracy: 0.9744 - val_loss: 0.6307 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 75/100\n",
      "Learning rate:  0.00074\n",
      "2/2 [==============================] - 2s 625ms/step - loss: 0.6909 - accuracy: 0.9487 - val_loss: 0.7367 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 76/100\n",
      "Learning rate:  0.00075\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6993 - accuracy: 0.9615 - val_loss: 0.9846 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 77/100\n",
      "Learning rate:  0.00076\n",
      "2/2 [==============================] - 2s 633ms/step - loss: 0.7306 - accuracy: 0.9615 - val_loss: 0.6283 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 78/100\n",
      "Learning rate:  0.0007700000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6506 - accuracy: 0.9872 - val_loss: 0.5936 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 79/100\n",
      "Learning rate:  0.0007800000000000001\n",
      "2/2 [==============================] - 2s 614ms/step - loss: 0.7171 - accuracy: 0.9359 - val_loss: 0.5879 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 80/100\n",
      "Learning rate:  0.00079\n",
      "2/2 [==============================] - 2s 600ms/step - loss: 0.6209 - accuracy: 1.0000 - val_loss: 0.5868 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 81/100\n",
      "Learning rate:  0.0008\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6575 - accuracy: 0.9872 - val_loss: 0.5854 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 82/100\n",
      "Learning rate:  8.1e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6471 - accuracy: 0.9744 - val_loss: 0.5854 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 83/100\n",
      "Learning rate:  8.2e-05\n",
      "2/2 [==============================] - 2s 633ms/step - loss: 0.6516 - accuracy: 0.9744 - val_loss: 0.5854 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 84/100\n",
      "Learning rate:  8.3e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6346 - accuracy: 0.9872 - val_loss: 0.5856 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 85/100\n",
      "Learning rate:  8.4e-05\n",
      "2/2 [==============================] - 2s 701ms/step - loss: 0.6662 - accuracy: 0.9615 - val_loss: 0.5858 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 86/100\n",
      "Learning rate:  8.5e-05\n",
      "2/2 [==============================] - 3s 624ms/step - loss: 0.6445 - accuracy: 0.9744 - val_loss: 0.5859 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 87/100\n",
      "Learning rate:  8.6e-05\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.6387 - accuracy: 0.9872 - val_loss: 0.5860 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 88/100\n",
      "Learning rate:  8.7e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6176 - accuracy: 1.0000 - val_loss: 0.5863 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 89/100\n",
      "Learning rate:  8.800000000000001e-05\n",
      "2/2 [==============================] - 2s 631ms/step - loss: 0.6306 - accuracy: 1.0000 - val_loss: 0.5866 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 90/100\n",
      "Learning rate:  8.900000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6313 - accuracy: 0.9872 - val_loss: 0.5869 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 91/100\n",
      "Learning rate:  9e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6348 - accuracy: 0.9872 - val_loss: 0.5872 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 92/100\n",
      "Learning rate:  9.1e-05\n",
      "2/2 [==============================] - 3s 581ms/step - loss: 0.6065 - accuracy: 1.0000 - val_loss: 0.5874 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 93/100\n",
      "Learning rate:  9.200000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6180 - accuracy: 0.9872 - val_loss: 0.5877 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 94/100\n",
      "Learning rate:  9.300000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6137 - accuracy: 1.0000 - val_loss: 0.5879 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 95/100\n",
      "Learning rate:  9.4e-05\n",
      "2/2 [==============================] - 2s 587ms/step - loss: 0.6074 - accuracy: 1.0000 - val_loss: 0.5880 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 96/100\n",
      "Learning rate:  9.5e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6499 - accuracy: 0.9872 - val_loss: 0.5875 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 97/100\n",
      "Learning rate:  9.6e-05\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.6040 - accuracy: 1.0000 - val_loss: 0.5871 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 98/100\n",
      "Learning rate:  9.7e-05\n",
      "2/2 [==============================] - 2s 599ms/step - loss: 0.6065 - accuracy: 1.0000 - val_loss: 0.5868 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 99/100\n",
      "Learning rate:  9.8e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.6765 - accuracy: 0.9872 - val_loss: 0.5863 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 100/100\n",
      "Learning rate:  9.900000000000001e-05\n",
      "2/2 [==============================] - 3s 753ms/step - loss: 0.6510 - accuracy: 0.9872 - val_loss: 0.5862 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.8102 - accuracy: 0.9231\n",
      "Test loss: 0.8102356195449829\n",
      "Test accuracy: 0.9230769276618958\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision:  0.9311740890688258\n",
      "Recall:  0.9230769230769231\n",
      "F1 Score:  0.9204059829059829\n",
      "31\n",
      "31\n",
      "[[[[ 76  84  95 ...  76  84  95]\n",
      "   [ 15  15  20 ...  15  15  20]\n",
      "   [  8   7  11 ...   8   7  11]\n",
      "   ...\n",
      "   [ 16  14  25 ...  16  14  25]\n",
      "   [ 13  13  22 ...  13  13  22]\n",
      "   [  8   9  13 ...   8   9  13]]\n",
      "\n",
      "  [[ 88  96 107 ...  88  96 107]\n",
      "   [ 17  17  22 ...  17  17  22]\n",
      "   [  6   5   9 ...   6   5   9]\n",
      "   ...\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 15  13  24 ...  15  13  24]\n",
      "   [ 10  10  16 ...  10  10  16]]\n",
      "\n",
      "  [[ 86  91  98 ...  86  91  98]\n",
      "   [ 14  13  17 ...  14  13  17]\n",
      "   [  5   4   7 ...   5   4   7]\n",
      "   ...\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 12  11  18 ...  12  11  18]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[157 162 174 ... 157 162 174]\n",
      "   [159 165 176 ... 159 165 176]\n",
      "   [155 158 159 ... 155 158 159]\n",
      "   ...\n",
      "   [126 136 168 ... 126 136 168]\n",
      "   [145 153 178 ... 145 153 178]\n",
      "   [150 157 180 ... 150 157 180]]\n",
      "\n",
      "  [[158 162 174 ... 158 162 174]\n",
      "   [158 164 177 ... 158 164 177]\n",
      "   [160 167 179 ... 160 167 179]\n",
      "   ...\n",
      "   [134 144 172 ... 134 144 172]\n",
      "   [143 151 177 ... 143 151 177]\n",
      "   [151 158 180 ... 151 158 180]]\n",
      "\n",
      "  [[160 164 176 ... 160 164 176]\n",
      "   [158 164 177 ... 158 164 177]\n",
      "   [163 169 181 ... 163 169 181]\n",
      "   ...\n",
      "   [141 150 174 ... 141 150 174]\n",
      "   [140 149 176 ... 140 149 176]\n",
      "   [150 158 180 ... 150 158 180]]]\n",
      "\n",
      "\n",
      " [[[ 30  95 105 ...  30  95 105]\n",
      "   [ 31  97 107 ...  31  97 107]\n",
      "   [ 30  96 106 ...  30  96 106]\n",
      "   ...\n",
      "   [ 26  91 103 ...  26  91 103]\n",
      "   [ 25  92 102 ...  25  92 102]\n",
      "   [ 24  91 101 ...  24  91 101]]\n",
      "\n",
      "  [[ 27 111 122 ...  27 111 122]\n",
      "   [ 27 111 123 ...  27 111 123]\n",
      "   [ 28 111 123 ...  28 111 123]\n",
      "   ...\n",
      "   [ 22 107 119 ...  22 107 119]\n",
      "   [ 22 108 118 ...  22 108 118]\n",
      "   [ 23 108 119 ...  23 108 119]]\n",
      "\n",
      "  [[ 24 105 116 ...  24 105 116]\n",
      "   [ 25 107 118 ...  25 107 118]\n",
      "   [ 28 108 119 ...  28 108 119]\n",
      "   ...\n",
      "   [ 26 107 117 ...  26 107 117]\n",
      "   [ 23 105 115 ...  23 105 115]\n",
      "   [ 22 104 116 ...  22 104 116]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  7  29  34 ...   7  29  34]\n",
      "   [ 87  67  56 ...  87  67  56]\n",
      "   [179 128 119 ... 179 128 119]\n",
      "   ...\n",
      "   [195 132 133 ... 195 132 133]\n",
      "   [119  77  72 ... 119  77  72]\n",
      "   [ 50  35  35 ...  50  35  35]]\n",
      "\n",
      "  [[  1  41  49 ...   1  41  49]\n",
      "   [ 72  61  52 ...  72  61  52]\n",
      "   [169 118 107 ... 169 118 107]\n",
      "   ...\n",
      "   [186 131 126 ... 186 131 126]\n",
      "   [ 97  65  58 ...  97  65  58]\n",
      "   [ 30  27  31 ...  30  27  31]]\n",
      "\n",
      "  [[  5  56  65 ...   5  56  65]\n",
      "   [ 63  60  53 ...  63  60  53]\n",
      "   [161 113 103 ... 161 113 103]\n",
      "   ...\n",
      "   [176 123 117 ... 176 123 117]\n",
      "   [ 76  58  54 ...  76  58  54]\n",
      "   [ 19  35  42 ...  19  35  42]]]\n",
      "\n",
      "\n",
      " [[[  9   9   2 ...   9   9   2]\n",
      "   [  6   6   2 ...   6   6   2]\n",
      "   [  4   3   1 ...   4   3   1]\n",
      "   ...\n",
      "   [  0   1   1 ...   0   1   1]\n",
      "   [  1   2   2 ...   1   2   2]\n",
      "   [  9  10   7 ...   9  10   7]]\n",
      "\n",
      "  [[  9   8   4 ...   9   8   4]\n",
      "   [  9   9   4 ...   9   9   4]\n",
      "   [  9   8   3 ...   9   8   3]\n",
      "   ...\n",
      "   [  1   1   1 ...   1   1   1]\n",
      "   [  4   4   3 ...   4   4   3]\n",
      "   [  4   5   3 ...   4   5   3]]\n",
      "\n",
      "  [[ 26  23  14 ...  26  23  14]\n",
      "   [ 33  27  15 ...  33  27  15]\n",
      "   [ 45  37  24 ...  45  37  24]\n",
      "   ...\n",
      "   [  2   3   2 ...   2   3   2]\n",
      "   [  2   2   2 ...   2   2   2]\n",
      "   [  4   4   3 ...   4   4   3]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[188 178 161 ... 188 178 161]\n",
      "   [190 180 162 ... 190 180 162]\n",
      "   [187 177 158 ... 187 177 158]\n",
      "   ...\n",
      "   [150 142 124 ... 150 142 124]\n",
      "   [145 135 118 ... 145 135 118]\n",
      "   [145 135 117 ... 145 135 117]]\n",
      "\n",
      "  [[197 186 170 ... 197 186 170]\n",
      "   [194 184 168 ... 194 184 168]\n",
      "   [192 181 164 ... 192 181 164]\n",
      "   ...\n",
      "   [148 139 120 ... 148 139 120]\n",
      "   [149 138 119 ... 149 138 119]\n",
      "   [147 136 117 ... 147 136 117]]\n",
      "\n",
      "  [[201 191 174 ... 201 191 174]\n",
      "   [198 188 172 ... 198 188 172]\n",
      "   [194 183 167 ... 194 183 167]\n",
      "   ...\n",
      "   [150 139 120 ... 150 139 120]\n",
      "   [152 141 121 ... 152 141 121]\n",
      "   [152 142 124 ... 152 142 124]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (68, 40, 30, 9)\n",
      "68 train samples\n",
      "31 test samples\n",
      "y_train shape: (68, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_32 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_961 (Conv2D)             (None, 40, 30, 16)   1312        input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_868 (BatchN (None, 40, 30, 16)   64          conv2d_961[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_868 (Activation)     (None, 40, 30, 16)   0           batch_normalization_868[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_962 (Conv2D)             (None, 40, 30, 16)   272         activation_868[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_869 (BatchN (None, 40, 30, 16)   64          conv2d_962[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_869 (Activation)     (None, 40, 30, 16)   0           batch_normalization_869[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_963 (Conv2D)             (None, 40, 30, 16)   2320        activation_869[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_870 (BatchN (None, 40, 30, 16)   64          conv2d_963[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_870 (Activation)     (None, 40, 30, 16)   0           batch_normalization_870[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_965 (Conv2D)             (None, 40, 30, 64)   1088        activation_868[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_964 (Conv2D)             (None, 40, 30, 64)   1088        activation_870[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_279 (Add)                   (None, 40, 30, 64)   0           conv2d_965[0][0]                 \n",
      "                                                                 conv2d_964[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_871 (BatchN (None, 40, 30, 64)   256         add_279[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_871 (Activation)     (None, 40, 30, 64)   0           batch_normalization_871[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_966 (Conv2D)             (None, 40, 30, 16)   1040        activation_871[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_872 (BatchN (None, 40, 30, 16)   64          conv2d_966[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_872 (Activation)     (None, 40, 30, 16)   0           batch_normalization_872[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_967 (Conv2D)             (None, 40, 30, 16)   2320        activation_872[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_873 (BatchN (None, 40, 30, 16)   64          conv2d_967[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_873 (Activation)     (None, 40, 30, 16)   0           batch_normalization_873[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_968 (Conv2D)             (None, 40, 30, 64)   1088        activation_873[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_280 (Add)                   (None, 40, 30, 64)   0           add_279[0][0]                    \n",
      "                                                                 conv2d_968[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_874 (BatchN (None, 40, 30, 64)   256         add_280[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_874 (Activation)     (None, 40, 30, 64)   0           batch_normalization_874[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_969 (Conv2D)             (None, 40, 30, 16)   1040        activation_874[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_875 (BatchN (None, 40, 30, 16)   64          conv2d_969[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_875 (Activation)     (None, 40, 30, 16)   0           batch_normalization_875[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_970 (Conv2D)             (None, 40, 30, 16)   2320        activation_875[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_876 (BatchN (None, 40, 30, 16)   64          conv2d_970[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_876 (Activation)     (None, 40, 30, 16)   0           batch_normalization_876[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_971 (Conv2D)             (None, 40, 30, 64)   1088        activation_876[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_281 (Add)                   (None, 40, 30, 64)   0           add_280[0][0]                    \n",
      "                                                                 conv2d_971[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_877 (BatchN (None, 40, 30, 64)   256         add_281[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_877 (Activation)     (None, 40, 30, 64)   0           batch_normalization_877[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_972 (Conv2D)             (None, 20, 15, 64)   4160        activation_877[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_878 (BatchN (None, 20, 15, 64)   256         conv2d_972[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_878 (Activation)     (None, 20, 15, 64)   0           batch_normalization_878[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_973 (Conv2D)             (None, 20, 15, 64)   36928       activation_878[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_879 (BatchN (None, 20, 15, 64)   256         conv2d_973[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_879 (Activation)     (None, 20, 15, 64)   0           batch_normalization_879[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_975 (Conv2D)             (None, 20, 15, 128)  8320        add_281[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_974 (Conv2D)             (None, 20, 15, 128)  8320        activation_879[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_282 (Add)                   (None, 20, 15, 128)  0           conv2d_975[0][0]                 \n",
      "                                                                 conv2d_974[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_880 (BatchN (None, 20, 15, 128)  512         add_282[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_880 (Activation)     (None, 20, 15, 128)  0           batch_normalization_880[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_976 (Conv2D)             (None, 20, 15, 64)   8256        activation_880[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_881 (BatchN (None, 20, 15, 64)   256         conv2d_976[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_881 (Activation)     (None, 20, 15, 64)   0           batch_normalization_881[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_977 (Conv2D)             (None, 20, 15, 64)   36928       activation_881[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_882 (BatchN (None, 20, 15, 64)   256         conv2d_977[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_882 (Activation)     (None, 20, 15, 64)   0           batch_normalization_882[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_978 (Conv2D)             (None, 20, 15, 128)  8320        activation_882[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_283 (Add)                   (None, 20, 15, 128)  0           add_282[0][0]                    \n",
      "                                                                 conv2d_978[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_883 (BatchN (None, 20, 15, 128)  512         add_283[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_883 (Activation)     (None, 20, 15, 128)  0           batch_normalization_883[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_979 (Conv2D)             (None, 20, 15, 64)   8256        activation_883[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_884 (BatchN (None, 20, 15, 64)   256         conv2d_979[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_884 (Activation)     (None, 20, 15, 64)   0           batch_normalization_884[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_980 (Conv2D)             (None, 20, 15, 64)   36928       activation_884[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_885 (BatchN (None, 20, 15, 64)   256         conv2d_980[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_885 (Activation)     (None, 20, 15, 64)   0           batch_normalization_885[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_981 (Conv2D)             (None, 20, 15, 128)  8320        activation_885[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_284 (Add)                   (None, 20, 15, 128)  0           add_283[0][0]                    \n",
      "                                                                 conv2d_981[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_886 (BatchN (None, 20, 15, 128)  512         add_284[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_886 (Activation)     (None, 20, 15, 128)  0           batch_normalization_886[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_982 (Conv2D)             (None, 10, 8, 128)   16512       activation_886[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_887 (BatchN (None, 10, 8, 128)   512         conv2d_982[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_887 (Activation)     (None, 10, 8, 128)   0           batch_normalization_887[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_983 (Conv2D)             (None, 10, 8, 128)   147584      activation_887[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_888 (BatchN (None, 10, 8, 128)   512         conv2d_983[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_888 (Activation)     (None, 10, 8, 128)   0           batch_normalization_888[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_985 (Conv2D)             (None, 10, 8, 256)   33024       add_284[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_984 (Conv2D)             (None, 10, 8, 256)   33024       activation_888[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_285 (Add)                   (None, 10, 8, 256)   0           conv2d_985[0][0]                 \n",
      "                                                                 conv2d_984[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_889 (BatchN (None, 10, 8, 256)   1024        add_285[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_889 (Activation)     (None, 10, 8, 256)   0           batch_normalization_889[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_986 (Conv2D)             (None, 10, 8, 128)   32896       activation_889[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_890 (BatchN (None, 10, 8, 128)   512         conv2d_986[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_890 (Activation)     (None, 10, 8, 128)   0           batch_normalization_890[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_987 (Conv2D)             (None, 10, 8, 128)   147584      activation_890[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_891 (BatchN (None, 10, 8, 128)   512         conv2d_987[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_891 (Activation)     (None, 10, 8, 128)   0           batch_normalization_891[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_988 (Conv2D)             (None, 10, 8, 256)   33024       activation_891[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_286 (Add)                   (None, 10, 8, 256)   0           add_285[0][0]                    \n",
      "                                                                 conv2d_988[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_892 (BatchN (None, 10, 8, 256)   1024        add_286[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_892 (Activation)     (None, 10, 8, 256)   0           batch_normalization_892[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_989 (Conv2D)             (None, 10, 8, 128)   32896       activation_892[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_893 (BatchN (None, 10, 8, 128)   512         conv2d_989[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_893 (Activation)     (None, 10, 8, 128)   0           batch_normalization_893[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_990 (Conv2D)             (None, 10, 8, 128)   147584      activation_893[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_894 (BatchN (None, 10, 8, 128)   512         conv2d_990[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_894 (Activation)     (None, 10, 8, 128)   0           batch_normalization_894[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_991 (Conv2D)             (None, 10, 8, 256)   33024       activation_894[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_287 (Add)                   (None, 10, 8, 256)   0           add_286[0][0]                    \n",
      "                                                                 conv2d_991[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_895 (BatchN (None, 10, 8, 256)   1024        add_287[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_895 (Activation)     (None, 10, 8, 256)   0           batch_normalization_895[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_31 (AveragePo (None, 1, 1, 256)    0           activation_895[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 256)          0           average_pooling2d_31[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 10)           2570        flatten_31[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[0.29803923 0.32941177 0.37254903 ... 0.29803923 0.32941177\n",
      "    0.37254903]\n",
      "   [0.05882353 0.05882353 0.07843138 ... 0.05882353 0.05882353\n",
      "    0.07843138]\n",
      "   [0.03137255 0.02745098 0.04313726 ... 0.03137255 0.02745098\n",
      "    0.04313726]\n",
      "   ...\n",
      "   [0.0627451  0.05490196 0.09803922 ... 0.0627451  0.05490196\n",
      "    0.09803922]\n",
      "   [0.05098039 0.05098039 0.08627451 ... 0.05098039 0.05098039\n",
      "    0.08627451]\n",
      "   [0.03137255 0.03529412 0.05098039 ... 0.03137255 0.03529412\n",
      "    0.05098039]]\n",
      "\n",
      "  [[0.34509805 0.3764706  0.41960785 ... 0.34509805 0.3764706\n",
      "    0.41960785]\n",
      "   [0.06666667 0.06666667 0.08627451 ... 0.06666667 0.06666667\n",
      "    0.08627451]\n",
      "   [0.02352941 0.01960784 0.03529412 ... 0.02352941 0.01960784\n",
      "    0.03529412]\n",
      "   ...\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.05882353 0.05098039 0.09411765 ... 0.05882353 0.05098039\n",
      "    0.09411765]\n",
      "   [0.03921569 0.03921569 0.0627451  ... 0.03921569 0.03921569\n",
      "    0.0627451 ]]\n",
      "\n",
      "  [[0.3372549  0.35686275 0.38431373 ... 0.3372549  0.35686275\n",
      "    0.38431373]\n",
      "   [0.05490196 0.05098039 0.06666667 ... 0.05490196 0.05098039\n",
      "    0.06666667]\n",
      "   [0.01960784 0.01568628 0.02745098 ... 0.01960784 0.01568628\n",
      "    0.02745098]\n",
      "   ...\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.04705882 0.04313726 0.07058824 ... 0.04705882 0.04313726\n",
      "    0.07058824]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6156863  0.63529414 0.68235296 ... 0.6156863  0.63529414\n",
      "    0.68235296]\n",
      "   [0.62352943 0.64705884 0.6901961  ... 0.62352943 0.64705884\n",
      "    0.6901961 ]\n",
      "   [0.60784316 0.61960787 0.62352943 ... 0.60784316 0.61960787\n",
      "    0.62352943]\n",
      "   ...\n",
      "   [0.49411765 0.53333336 0.65882355 ... 0.49411765 0.53333336\n",
      "    0.65882355]\n",
      "   [0.5686275  0.6        0.69803923 ... 0.5686275  0.6\n",
      "    0.69803923]\n",
      "   [0.5882353  0.6156863  0.7058824  ... 0.5882353  0.6156863\n",
      "    0.7058824 ]]\n",
      "\n",
      "  [[0.61960787 0.63529414 0.68235296 ... 0.61960787 0.63529414\n",
      "    0.68235296]\n",
      "   [0.61960787 0.6431373  0.69411767 ... 0.61960787 0.6431373\n",
      "    0.69411767]\n",
      "   [0.627451   0.654902   0.7019608  ... 0.627451   0.654902\n",
      "    0.7019608 ]\n",
      "   ...\n",
      "   [0.5254902  0.5647059  0.6745098  ... 0.5254902  0.5647059\n",
      "    0.6745098 ]\n",
      "   [0.56078434 0.5921569  0.69411767 ... 0.56078434 0.5921569\n",
      "    0.69411767]\n",
      "   [0.5921569  0.61960787 0.7058824  ... 0.5921569  0.61960787\n",
      "    0.7058824 ]]\n",
      "\n",
      "  [[0.627451   0.6431373  0.6901961  ... 0.627451   0.6431373\n",
      "    0.6901961 ]\n",
      "   [0.61960787 0.6431373  0.69411767 ... 0.61960787 0.6431373\n",
      "    0.69411767]\n",
      "   [0.6392157  0.6627451  0.70980394 ... 0.6392157  0.6627451\n",
      "    0.70980394]\n",
      "   ...\n",
      "   [0.5529412  0.5882353  0.68235296 ... 0.5529412  0.5882353\n",
      "    0.68235296]\n",
      "   [0.54901963 0.58431375 0.6901961  ... 0.54901963 0.58431375\n",
      "    0.6901961 ]\n",
      "   [0.5882353  0.61960787 0.7058824  ... 0.5882353  0.61960787\n",
      "    0.7058824 ]]]\n",
      "\n",
      "\n",
      " [[[0.11764706 0.37254903 0.4117647  ... 0.11764706 0.37254903\n",
      "    0.4117647 ]\n",
      "   [0.12156863 0.38039216 0.41960785 ... 0.12156863 0.38039216\n",
      "    0.41960785]\n",
      "   [0.11764706 0.3764706  0.41568628 ... 0.11764706 0.3764706\n",
      "    0.41568628]\n",
      "   ...\n",
      "   [0.10196079 0.35686275 0.40392157 ... 0.10196079 0.35686275\n",
      "    0.40392157]\n",
      "   [0.09803922 0.36078432 0.4        ... 0.09803922 0.36078432\n",
      "    0.4       ]\n",
      "   [0.09411765 0.35686275 0.39607844 ... 0.09411765 0.35686275\n",
      "    0.39607844]]\n",
      "\n",
      "  [[0.10588235 0.43529412 0.47843137 ... 0.10588235 0.43529412\n",
      "    0.47843137]\n",
      "   [0.10588235 0.43529412 0.48235294 ... 0.10588235 0.43529412\n",
      "    0.48235294]\n",
      "   [0.10980392 0.43529412 0.48235294 ... 0.10980392 0.43529412\n",
      "    0.48235294]\n",
      "   ...\n",
      "   [0.08627451 0.41960785 0.46666667 ... 0.08627451 0.41960785\n",
      "    0.46666667]\n",
      "   [0.08627451 0.42352942 0.4627451  ... 0.08627451 0.42352942\n",
      "    0.4627451 ]\n",
      "   [0.09019608 0.42352942 0.46666667 ... 0.09019608 0.42352942\n",
      "    0.46666667]]\n",
      "\n",
      "  [[0.09411765 0.4117647  0.45490196 ... 0.09411765 0.4117647\n",
      "    0.45490196]\n",
      "   [0.09803922 0.41960785 0.4627451  ... 0.09803922 0.41960785\n",
      "    0.4627451 ]\n",
      "   [0.10980392 0.42352942 0.46666667 ... 0.10980392 0.42352942\n",
      "    0.46666667]\n",
      "   ...\n",
      "   [0.10196079 0.41960785 0.45882353 ... 0.10196079 0.41960785\n",
      "    0.45882353]\n",
      "   [0.09019608 0.4117647  0.4509804  ... 0.09019608 0.4117647\n",
      "    0.4509804 ]\n",
      "   [0.08627451 0.40784314 0.45490196 ... 0.08627451 0.40784314\n",
      "    0.45490196]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.02745098 0.11372549 0.13333334 ... 0.02745098 0.11372549\n",
      "    0.13333334]\n",
      "   [0.34117648 0.2627451  0.21960784 ... 0.34117648 0.2627451\n",
      "    0.21960784]\n",
      "   [0.7019608  0.5019608  0.46666667 ... 0.7019608  0.5019608\n",
      "    0.46666667]\n",
      "   ...\n",
      "   [0.7647059  0.5176471  0.52156866 ... 0.7647059  0.5176471\n",
      "    0.52156866]\n",
      "   [0.46666667 0.3019608  0.28235295 ... 0.46666667 0.3019608\n",
      "    0.28235295]\n",
      "   [0.19607843 0.13725491 0.13725491 ... 0.19607843 0.13725491\n",
      "    0.13725491]]\n",
      "\n",
      "  [[0.00392157 0.16078432 0.19215687 ... 0.00392157 0.16078432\n",
      "    0.19215687]\n",
      "   [0.28235295 0.23921569 0.20392157 ... 0.28235295 0.23921569\n",
      "    0.20392157]\n",
      "   [0.6627451  0.4627451  0.41960785 ... 0.6627451  0.4627451\n",
      "    0.41960785]\n",
      "   ...\n",
      "   [0.7294118  0.5137255  0.49411765 ... 0.7294118  0.5137255\n",
      "    0.49411765]\n",
      "   [0.38039216 0.25490198 0.22745098 ... 0.38039216 0.25490198\n",
      "    0.22745098]\n",
      "   [0.11764706 0.10588235 0.12156863 ... 0.11764706 0.10588235\n",
      "    0.12156863]]\n",
      "\n",
      "  [[0.01960784 0.21960784 0.25490198 ... 0.01960784 0.21960784\n",
      "    0.25490198]\n",
      "   [0.24705882 0.23529412 0.20784314 ... 0.24705882 0.23529412\n",
      "    0.20784314]\n",
      "   [0.6313726  0.44313726 0.40392157 ... 0.6313726  0.44313726\n",
      "    0.40392157]\n",
      "   ...\n",
      "   [0.6901961  0.48235294 0.45882353 ... 0.6901961  0.48235294\n",
      "    0.45882353]\n",
      "   [0.29803923 0.22745098 0.21176471 ... 0.29803923 0.22745098\n",
      "    0.21176471]\n",
      "   [0.07450981 0.13725491 0.16470589 ... 0.07450981 0.13725491\n",
      "    0.16470589]]]\n",
      "\n",
      "\n",
      " [[[0.03529412 0.03529412 0.00784314 ... 0.03529412 0.03529412\n",
      "    0.00784314]\n",
      "   [0.02352941 0.02352941 0.00784314 ... 0.02352941 0.02352941\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01176471 0.00392157 ... 0.01568628 0.01176471\n",
      "    0.00392157]\n",
      "   ...\n",
      "   [0.         0.00392157 0.00392157 ... 0.         0.00392157\n",
      "    0.00392157]\n",
      "   [0.00392157 0.00784314 0.00784314 ... 0.00392157 0.00784314\n",
      "    0.00784314]\n",
      "   [0.03529412 0.03921569 0.02745098 ... 0.03529412 0.03921569\n",
      "    0.02745098]]\n",
      "\n",
      "  [[0.03529412 0.03137255 0.01568628 ... 0.03529412 0.03137255\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03529412 0.01568628 ... 0.03529412 0.03529412\n",
      "    0.01568628]\n",
      "   [0.03529412 0.03137255 0.01176471 ... 0.03529412 0.03137255\n",
      "    0.01176471]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157 ... 0.00392157 0.00392157\n",
      "    0.00392157]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]\n",
      "   [0.01568628 0.01960784 0.01176471 ... 0.01568628 0.01960784\n",
      "    0.01176471]]\n",
      "\n",
      "  [[0.10196079 0.09019608 0.05490196 ... 0.10196079 0.09019608\n",
      "    0.05490196]\n",
      "   [0.12941177 0.10588235 0.05882353 ... 0.12941177 0.10588235\n",
      "    0.05882353]\n",
      "   [0.1764706  0.14509805 0.09411765 ... 0.1764706  0.14509805\n",
      "    0.09411765]\n",
      "   ...\n",
      "   [0.00784314 0.01176471 0.00784314 ... 0.00784314 0.01176471\n",
      "    0.00784314]\n",
      "   [0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.01568628 0.01568628 0.01176471 ... 0.01568628 0.01568628\n",
      "    0.01176471]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.7372549  0.69803923 0.6313726  ... 0.7372549  0.69803923\n",
      "    0.6313726 ]\n",
      "   [0.74509805 0.7058824  0.63529414 ... 0.74509805 0.7058824\n",
      "    0.63529414]\n",
      "   [0.73333335 0.69411767 0.61960787 ... 0.73333335 0.69411767\n",
      "    0.61960787]\n",
      "   ...\n",
      "   [0.5882353  0.5568628  0.4862745  ... 0.5882353  0.5568628\n",
      "    0.4862745 ]\n",
      "   [0.5686275  0.5294118  0.4627451  ... 0.5686275  0.5294118\n",
      "    0.4627451 ]\n",
      "   [0.5686275  0.5294118  0.45882353 ... 0.5686275  0.5294118\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.77254903 0.7294118  0.6666667  ... 0.77254903 0.7294118\n",
      "    0.6666667 ]\n",
      "   [0.7607843  0.72156864 0.65882355 ... 0.7607843  0.72156864\n",
      "    0.65882355]\n",
      "   [0.7529412  0.70980394 0.6431373  ... 0.7529412  0.70980394\n",
      "    0.6431373 ]\n",
      "   ...\n",
      "   [0.5803922  0.54509807 0.47058824 ... 0.5803922  0.54509807\n",
      "    0.47058824]\n",
      "   [0.58431375 0.5411765  0.46666667 ... 0.58431375 0.5411765\n",
      "    0.46666667]\n",
      "   [0.5764706  0.53333336 0.45882353 ... 0.5764706  0.53333336\n",
      "    0.45882353]]\n",
      "\n",
      "  [[0.7882353  0.7490196  0.68235296 ... 0.7882353  0.7490196\n",
      "    0.68235296]\n",
      "   [0.7764706  0.7372549  0.6745098  ... 0.7764706  0.7372549\n",
      "    0.6745098 ]\n",
      "   [0.7607843  0.7176471  0.654902   ... 0.7607843  0.7176471\n",
      "    0.654902  ]\n",
      "   ...\n",
      "   [0.5882353  0.54509807 0.47058824 ... 0.5882353  0.54509807\n",
      "    0.47058824]\n",
      "   [0.59607846 0.5529412  0.4745098  ... 0.59607846 0.5529412\n",
      "    0.4745098 ]\n",
      "   [0.59607846 0.5568628  0.4862745  ... 0.59607846 0.5568628\n",
      "    0.4862745 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157 ... 0.00392157 0.00392157\n",
      "    0.00392157]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.61960787 0.62352943 0.6784314  ... 0.61960787 0.62352943\n",
      "    0.6784314 ]\n",
      "   [0.6784314  0.6862745  0.73333335 ... 0.6784314  0.6862745\n",
      "    0.73333335]\n",
      "   [0.5019608  0.5058824  0.5568628  ... 0.5019608  0.5058824\n",
      "    0.5568628 ]\n",
      "   ...\n",
      "   [0.25490198 0.26666668 0.30588236 ... 0.25490198 0.26666668\n",
      "    0.30588236]\n",
      "   [0.16078432 0.16078432 0.21176471 ... 0.16078432 0.16078432\n",
      "    0.21176471]\n",
      "   [0.15686275 0.15294118 0.2        ... 0.15686275 0.15294118\n",
      "    0.2       ]]\n",
      "\n",
      "  [[0.59607846 0.6039216  0.65882355 ... 0.59607846 0.6039216\n",
      "    0.65882355]\n",
      "   [0.6        0.60784316 0.6627451  ... 0.6        0.60784316\n",
      "    0.6627451 ]\n",
      "   [0.4627451  0.46666667 0.52156866 ... 0.4627451  0.46666667\n",
      "    0.52156866]\n",
      "   ...\n",
      "   [0.16078432 0.16862746 0.19607843 ... 0.16078432 0.16862746\n",
      "    0.19607843]\n",
      "   [0.1254902  0.12156863 0.16470589 ... 0.1254902  0.12156863\n",
      "    0.16470589]\n",
      "   [0.12156863 0.12156863 0.16078432 ... 0.12156863 0.12156863\n",
      "    0.16078432]]\n",
      "\n",
      "  [[0.49411765 0.49019608 0.5568628  ... 0.49411765 0.49019608\n",
      "    0.5568628 ]\n",
      "   [0.5019608  0.49803922 0.5647059  ... 0.5019608  0.49803922\n",
      "    0.5647059 ]\n",
      "   [0.4745098  0.47058824 0.5294118  ... 0.4745098  0.47058824\n",
      "    0.5294118 ]\n",
      "   ...\n",
      "   [0.02745098 0.02352941 0.03137255 ... 0.02745098 0.02352941\n",
      "    0.03137255]\n",
      "   [0.09803922 0.09019608 0.1254902  ... 0.09803922 0.09019608\n",
      "    0.1254902 ]\n",
      "   [0.09411765 0.08627451 0.12156863 ... 0.09411765 0.08627451\n",
      "    0.12156863]]]\n",
      "\n",
      "\n",
      " [[[0.49803922 0.49803922 0.5176471  ... 0.49803922 0.49803922\n",
      "    0.5176471 ]\n",
      "   [0.5058824  0.5019608  0.52156866 ... 0.5058824  0.5019608\n",
      "    0.52156866]\n",
      "   [0.50980395 0.5058824  0.5254902  ... 0.50980395 0.5058824\n",
      "    0.5254902 ]\n",
      "   ...\n",
      "   [0.5686275  0.57254905 0.5882353  ... 0.5686275  0.57254905\n",
      "    0.5882353 ]\n",
      "   [0.57254905 0.5764706  0.5921569  ... 0.57254905 0.5764706\n",
      "    0.5921569 ]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]]\n",
      "\n",
      "  [[0.50980395 0.50980395 0.5254902  ... 0.50980395 0.50980395\n",
      "    0.5254902 ]\n",
      "   [0.5137255  0.5137255  0.5294118  ... 0.5137255  0.5137255\n",
      "    0.5294118 ]\n",
      "   [0.5176471  0.5176471  0.53333336 ... 0.5176471  0.5176471\n",
      "    0.53333336]\n",
      "   ...\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5764706  0.5803922  0.59607846 ... 0.5764706  0.5803922\n",
      "    0.59607846]\n",
      "   [0.5803922  0.58431375 0.6        ... 0.5803922  0.58431375\n",
      "    0.6       ]]\n",
      "\n",
      "  [[0.5176471  0.5176471  0.5372549  ... 0.5176471  0.5176471\n",
      "    0.5372549 ]\n",
      "   [0.52156866 0.5254902  0.5411765  ... 0.52156866 0.5254902\n",
      "    0.5411765 ]\n",
      "   [0.5176471  0.52156866 0.5372549  ... 0.5176471  0.52156866\n",
      "    0.5372549 ]\n",
      "   ...\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.58431375 0.5882353  0.6039216  ... 0.58431375 0.5882353\n",
      "    0.6039216 ]\n",
      "   [0.5882353  0.5921569  0.60784316 ... 0.5882353  0.5921569\n",
      "    0.60784316]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8901961  0.8392157  0.8039216  ... 0.8901961  0.8392157\n",
      "    0.8039216 ]\n",
      "   [0.91764706 0.8745098  0.8392157  ... 0.91764706 0.8745098\n",
      "    0.8392157 ]\n",
      "   [0.8627451  0.79607844 0.7490196  ... 0.8627451  0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9372549  0.90588236 0.8784314  ... 0.9372549  0.90588236\n",
      "    0.8784314 ]\n",
      "   [0.90588236 0.85882354 0.8235294  ... 0.90588236 0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.87058824 0.8117647  0.7764706  ... 0.87058824 0.8117647\n",
      "    0.7764706 ]]\n",
      "\n",
      "  [[0.88235295 0.8235294  0.78431374 ... 0.88235295 0.8235294\n",
      "    0.78431374]\n",
      "   [0.89411765 0.8392157  0.8        ... 0.89411765 0.8392157\n",
      "    0.8       ]\n",
      "   [0.87058824 0.8039216  0.7607843  ... 0.87058824 0.8039216\n",
      "    0.7607843 ]\n",
      "   ...\n",
      "   [0.92156863 0.8862745  0.8509804  ... 0.92156863 0.8862745\n",
      "    0.8509804 ]\n",
      "   [0.9098039  0.87058824 0.8352941  ... 0.9098039  0.87058824\n",
      "    0.8352941 ]\n",
      "   [0.8745098  0.8156863  0.77254903 ... 0.8745098  0.8156863\n",
      "    0.77254903]]\n",
      "\n",
      "  [[0.8627451  0.7921569  0.7411765  ... 0.8627451  0.7921569\n",
      "    0.7411765 ]\n",
      "   [0.8666667  0.8039216  0.7529412  ... 0.8666667  0.8039216\n",
      "    0.7529412 ]\n",
      "   [0.85882354 0.79607844 0.7490196  ... 0.85882354 0.79607844\n",
      "    0.7490196 ]\n",
      "   ...\n",
      "   [0.9098039  0.85882354 0.8235294  ... 0.9098039  0.85882354\n",
      "    0.8235294 ]\n",
      "   [0.90588236 0.85490197 0.8156863  ... 0.90588236 0.85490197\n",
      "    0.8156863 ]\n",
      "   [0.8784314  0.8156863  0.7647059  ... 0.8784314  0.8156863\n",
      "    0.7647059 ]]]\n",
      "\n",
      "\n",
      " [[[0.65882355 0.5411765  0.34901962 ... 0.65882355 0.5411765\n",
      "    0.34901962]\n",
      "   [0.79607844 0.6862745  0.44313726 ... 0.79607844 0.6862745\n",
      "    0.44313726]\n",
      "   [0.84705883 0.7372549  0.4862745  ... 0.84705883 0.7372549\n",
      "    0.4862745 ]\n",
      "   ...\n",
      "   [0.9019608  0.78431374 0.57254905 ... 0.9019608  0.78431374\n",
      "    0.57254905]\n",
      "   [0.8784314  0.77254903 0.5803922  ... 0.8784314  0.77254903\n",
      "    0.5803922 ]\n",
      "   [0.8666667  0.7607843  0.58431375 ... 0.8666667  0.7607843\n",
      "    0.58431375]]\n",
      "\n",
      "  [[0.6313726  0.5058824  0.27450982 ... 0.6313726  0.5058824\n",
      "    0.27450982]\n",
      "   [0.7372549  0.61960787 0.3764706  ... 0.7372549  0.61960787\n",
      "    0.3764706 ]\n",
      "   [0.83137256 0.72156864 0.4627451  ... 0.83137256 0.72156864\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.8784314  0.7607843  0.5568628  ... 0.8784314  0.7607843\n",
      "    0.5568628 ]\n",
      "   [0.85882354 0.7490196  0.56078434 ... 0.85882354 0.7490196\n",
      "    0.56078434]\n",
      "   [0.85490197 0.7529412  0.5647059  ... 0.85490197 0.7529412\n",
      "    0.5647059 ]]\n",
      "\n",
      "  [[0.6        0.46666667 0.22352941 ... 0.6        0.46666667\n",
      "    0.22352941]\n",
      "   [0.6313726  0.49803922 0.27058825 ... 0.6313726  0.49803922\n",
      "    0.27058825]\n",
      "   [0.80784315 0.69803923 0.44705883 ... 0.80784315 0.69803923\n",
      "    0.44705883]\n",
      "   ...\n",
      "   [0.8352941  0.72156864 0.5372549  ... 0.8352941  0.72156864\n",
      "    0.5372549 ]\n",
      "   [0.83137256 0.7254902  0.5411765  ... 0.83137256 0.7254902\n",
      "    0.5411765 ]\n",
      "   [0.8352941  0.73333335 0.5372549  ... 0.8352941  0.73333335\n",
      "    0.5372549 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6039216  0.3764706  0.27058825 ... 0.6039216  0.3764706\n",
      "    0.27058825]\n",
      "   [0.627451   0.40784314 0.31764707 ... 0.627451   0.40784314\n",
      "    0.31764707]\n",
      "   [0.67058825 0.47058824 0.4        ... 0.67058825 0.47058824\n",
      "    0.4       ]\n",
      "   ...\n",
      "   [0.76862746 0.58431375 0.4745098  ... 0.76862746 0.58431375\n",
      "    0.4745098 ]\n",
      "   [0.7058824  0.49803922 0.38431373 ... 0.7058824  0.49803922\n",
      "    0.38431373]\n",
      "   [0.65882355 0.44313726 0.32156864 ... 0.65882355 0.44313726\n",
      "    0.32156864]]\n",
      "\n",
      "  [[0.6        0.38039216 0.26666668 ... 0.6        0.38039216\n",
      "    0.26666668]\n",
      "   [0.63529414 0.41568628 0.32156864 ... 0.63529414 0.41568628\n",
      "    0.32156864]\n",
      "   [0.7372549  0.56078434 0.4627451  ... 0.7372549  0.56078434\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.83137256 0.654902   0.54901963 ... 0.83137256 0.654902\n",
      "    0.54901963]\n",
      "   [0.77254903 0.5647059  0.44313726 ... 0.77254903 0.5647059\n",
      "    0.44313726]\n",
      "   [0.7176471  0.5019608  0.38039216 ... 0.7176471  0.5019608\n",
      "    0.38039216]]\n",
      "\n",
      "  [[0.627451   0.41960785 0.29411766 ... 0.627451   0.41960785\n",
      "    0.29411766]\n",
      "   [0.68235296 0.46666667 0.3882353  ... 0.68235296 0.46666667\n",
      "    0.3882353 ]\n",
      "   [0.8509804  0.7294118  0.49019608 ... 0.8509804  0.7294118\n",
      "    0.49019608]\n",
      "   ...\n",
      "   [0.89411765 0.72156864 0.6117647  ... 0.89411765 0.72156864\n",
      "    0.6117647 ]\n",
      "   [0.8392157  0.627451   0.50980395 ... 0.8392157  0.627451\n",
      "    0.50980395]\n",
      "   [0.76862746 0.56078434 0.43529412 ... 0.76862746 0.56078434\n",
      "    0.43529412]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (68, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (68, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 7s 1s/step - loss: 2.5570 - accuracy: 0.3529 - val_loss: 2.8542 - val_accuracy: 0.0645\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/100\n",
      "Learning rate:  1e-05\n",
      "2/2 [==============================] - 2s 382ms/step - loss: 2.6066 - accuracy: 0.3382 - val_loss: 2.8088 - val_accuracy: 0.0968\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/100\n",
      "Learning rate:  2e-05\n",
      "2/2 [==============================] - 2s 359ms/step - loss: 2.5940 - accuracy: 0.3235 - val_loss: 2.7577 - val_accuracy: 0.9032\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/100\n",
      "Learning rate:  3e-05\n",
      "2/2 [==============================] - 2s 352ms/step - loss: 2.5520 - accuracy: 0.3382 - val_loss: 2.7054 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/100\n",
      "Learning rate:  4e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.5144 - accuracy: 0.3382 - val_loss: 2.6716 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/100\n",
      "Learning rate:  5e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.4815 - accuracy: 0.3529 - val_loss: 2.6170 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/100\n",
      "Learning rate:  6e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.4484 - accuracy: 0.3382 - val_loss: 2.5658 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/100\n",
      "Learning rate:  7.000000000000001e-05\n",
      "2/2 [==============================] - 2s 358ms/step - loss: 2.4397 - accuracy: 0.3382 - val_loss: 2.4903 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/100\n",
      "Learning rate:  8e-05\n",
      "2/2 [==============================] - 2s 352ms/step - loss: 2.3802 - accuracy: 0.3382 - val_loss: 2.4066 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/100\n",
      "Learning rate:  8.999999999999999e-05\n",
      "2/2 [==============================] - 2s 348ms/step - loss: 2.3285 - accuracy: 0.3529 - val_loss: 2.3169 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/100\n",
      "Learning rate:  0.0001\n",
      "2/2 [==============================] - 2s 375ms/step - loss: 2.2618 - accuracy: 0.3676 - val_loss: 2.2234 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/100\n",
      "Learning rate:  0.00011\n",
      "2/2 [==============================] - 2s 402ms/step - loss: 2.2096 - accuracy: 0.4412 - val_loss: 2.1112 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/100\n",
      "Learning rate:  0.00012\n",
      "2/2 [==============================] - 2s 393ms/step - loss: 2.1448 - accuracy: 0.4559 - val_loss: 1.9997 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/100\n",
      "Learning rate:  0.00013000000000000002\n",
      "2/2 [==============================] - 2s 2s/step - loss: 2.0356 - accuracy: 0.4706 - val_loss: 1.8835 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/100\n",
      "Learning rate:  0.00014000000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.9705 - accuracy: 0.4853 - val_loss: 1.7615 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/100\n",
      "Learning rate:  0.00015\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.8978 - accuracy: 0.5000 - val_loss: 1.6394 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/100\n",
      "Learning rate:  0.00016\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.8319 - accuracy: 0.5294 - val_loss: 1.5159 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/100\n",
      "Learning rate:  0.00017\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.7696 - accuracy: 0.5882 - val_loss: 1.3975 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/100\n",
      "Learning rate:  0.00017999999999999998\n",
      "2/2 [==============================] - 2s 351ms/step - loss: 1.7339 - accuracy: 0.6471 - val_loss: 1.2735 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/100\n",
      "Learning rate:  0.00019\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.6811 - accuracy: 0.6176 - val_loss: 1.1663 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 21/100\n",
      "Learning rate:  0.0002\n",
      "2/2 [==============================] - 2s 348ms/step - loss: 1.6326 - accuracy: 0.6176 - val_loss: 1.0623 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 22/100\n",
      "Learning rate:  0.00021\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.5410 - accuracy: 0.7647 - val_loss: 0.9665 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 23/100\n",
      "Learning rate:  0.00022\n",
      "2/2 [==============================] - 2s 348ms/step - loss: 1.5026 - accuracy: 0.7059 - val_loss: 0.8762 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 24/100\n",
      "Learning rate:  0.00023\n",
      "2/2 [==============================] - 2s 353ms/step - loss: 1.4656 - accuracy: 0.7206 - val_loss: 0.8081 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 25/100\n",
      "Learning rate:  0.00024\n",
      "2/2 [==============================] - 2s 358ms/step - loss: 1.3927 - accuracy: 0.7500 - val_loss: 0.7556 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 26/100\n",
      "Learning rate:  0.00025\n",
      "2/2 [==============================] - 2s 346ms/step - loss: 1.3613 - accuracy: 0.7941 - val_loss: 0.7242 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 27/100\n",
      "Learning rate:  0.00026000000000000003\n",
      "2/2 [==============================] - 2s 356ms/step - loss: 1.3025 - accuracy: 0.8824 - val_loss: 0.7087 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 28/100\n",
      "Learning rate:  0.00027\n",
      "2/2 [==============================] - 2s 340ms/step - loss: 1.2870 - accuracy: 0.8382 - val_loss: 0.7051 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 29/100\n",
      "Learning rate:  0.00028000000000000003\n",
      "2/2 [==============================] - 2s 376ms/step - loss: 1.2024 - accuracy: 0.8971 - val_loss: 0.7044 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 30/100\n",
      "Learning rate:  0.00029\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.1867 - accuracy: 0.8382 - val_loss: 0.7064 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 31/100\n",
      "Learning rate:  0.0003\n",
      "2/2 [==============================] - 3s 3s/step - loss: 1.1755 - accuracy: 0.8529 - val_loss: 0.7058 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 32/100\n",
      "Learning rate:  0.00031\n",
      "2/2 [==============================] - 2s 495ms/step - loss: 1.2021 - accuracy: 0.8676 - val_loss: 0.7130 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 33/100\n",
      "Learning rate:  0.00032\n",
      "2/2 [==============================] - 2s 539ms/step - loss: 1.1250 - accuracy: 0.8676 - val_loss: 0.7301 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 34/100\n",
      "Learning rate:  0.00033\n",
      "2/2 [==============================] - 3s 3s/step - loss: 1.1049 - accuracy: 0.8529 - val_loss: 0.7336 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 35/100\n",
      "Learning rate:  0.00034\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0588 - accuracy: 0.8971 - val_loss: 0.7361 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 36/100\n",
      "Learning rate:  0.00035\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0916 - accuracy: 0.8529 - val_loss: 0.7655 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 37/100\n",
      "Learning rate:  0.00035999999999999997\n",
      "2/2 [==============================] - 2s 372ms/step - loss: 1.0632 - accuracy: 0.8824 - val_loss: 0.7870 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 38/100\n",
      "Learning rate:  0.00037\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.0559 - accuracy: 0.8824 - val_loss: 0.8195 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 39/100\n",
      "Learning rate:  0.00038\n",
      "2/2 [==============================] - 2s 403ms/step - loss: 1.0470 - accuracy: 0.8824 - val_loss: 0.8359 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 40/100\n",
      "Learning rate:  0.00039000000000000005\n",
      "2/2 [==============================] - 2s 391ms/step - loss: 0.9926 - accuracy: 0.8971 - val_loss: 0.8405 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 41/100\n",
      "Learning rate:  0.0004\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.9828 - accuracy: 0.8971 - val_loss: 0.8064 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 42/100\n",
      "Learning rate:  0.00041\n",
      "2/2 [==============================] - 1s 1s/step - loss: 0.9304 - accuracy: 0.9412 - val_loss: 0.7575 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 43/100\n",
      "Learning rate:  0.00042\n",
      "2/2 [==============================] - 2s 296ms/step - loss: 0.9635 - accuracy: 0.8971 - val_loss: 0.7192 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 44/100\n",
      "Learning rate:  0.00043\n",
      "2/2 [==============================] - 2s 312ms/step - loss: 0.9244 - accuracy: 0.9118 - val_loss: 0.6928 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 45/100\n",
      "Learning rate:  0.00044\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.9646 - accuracy: 0.8971 - val_loss: 0.6619 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 46/100\n",
      "Learning rate:  0.00045000000000000004\n",
      "2/2 [==============================] - 2s 300ms/step - loss: 0.9414 - accuracy: 0.8971 - val_loss: 0.6532 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 47/100\n",
      "Learning rate:  0.00046\n",
      "2/2 [==============================] - 2s 291ms/step - loss: 0.9090 - accuracy: 0.9265 - val_loss: 0.6550 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 48/100\n",
      "Learning rate:  0.00047\n",
      "2/2 [==============================] - 1s 1s/step - loss: 0.9317 - accuracy: 0.8824 - val_loss: 0.6913 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 49/100\n",
      "Learning rate:  0.00048\n",
      "2/2 [==============================] - 1s 1s/step - loss: 0.8918 - accuracy: 0.9412 - val_loss: 0.7442 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 50/100\n",
      "Learning rate:  0.00049\n",
      "2/2 [==============================] - 2s 294ms/step - loss: 0.9009 - accuracy: 0.8971 - val_loss: 0.8657 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 51/100\n",
      "Learning rate:  0.0005\n",
      "2/2 [==============================] - 2s 288ms/step - loss: 0.9280 - accuracy: 0.8824 - val_loss: 1.3715 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 52/100\n",
      "Learning rate:  0.00051\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.9884 - accuracy: 0.8676 - val_loss: 1.9318 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 53/100\n",
      "Learning rate:  0.0005200000000000001\n",
      "2/2 [==============================] - 1s 275ms/step - loss: 0.9151 - accuracy: 0.8971 - val_loss: 2.3238 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 54/100\n",
      "Learning rate:  0.0005300000000000001\n",
      "2/2 [==============================] - 2s 360ms/step - loss: 0.9266 - accuracy: 0.8824 - val_loss: 2.5681 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 55/100\n",
      "Learning rate:  0.00054\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8757 - accuracy: 0.9412 - val_loss: 2.5378 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 56/100\n",
      "Learning rate:  0.00055\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8500 - accuracy: 0.9412 - val_loss: 2.2250 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 57/100\n",
      "Learning rate:  0.0005600000000000001\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8361 - accuracy: 0.9265 - val_loss: 1.9957 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 58/100\n",
      "Learning rate:  0.00057\n",
      "2/2 [==============================] - 2s 298ms/step - loss: 0.8643 - accuracy: 0.9412 - val_loss: 2.0576 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 59/100\n",
      "Learning rate:  0.00058\n",
      "2/2 [==============================] - 2s 366ms/step - loss: 0.8539 - accuracy: 0.8971 - val_loss: 2.3223 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 60/100\n",
      "Learning rate:  0.00059\n",
      "2/2 [==============================] - 2s 390ms/step - loss: 0.7892 - accuracy: 0.9706 - val_loss: 2.3970 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 61/100\n",
      "Learning rate:  0.0006\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8084 - accuracy: 0.9412 - val_loss: 2.2318 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 62/100\n",
      "Learning rate:  0.00061\n",
      "2/2 [==============================] - 2s 443ms/step - loss: 0.8015 - accuracy: 0.9706 - val_loss: 1.8360 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 63/100\n",
      "Learning rate:  0.00062\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8372 - accuracy: 0.9559 - val_loss: 1.4449 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 64/100\n",
      "Learning rate:  0.00063\n",
      "2/2 [==============================] - 2s 410ms/step - loss: 0.7965 - accuracy: 0.9706 - val_loss: 1.0407 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 65/100\n",
      "Learning rate:  0.00064\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7339 - accuracy: 1.0000 - val_loss: 0.6909 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 66/100\n",
      "Learning rate:  0.0006500000000000001\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7952 - accuracy: 0.9559 - val_loss: 0.7908 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 67/100\n",
      "Learning rate:  0.00066\n",
      "2/2 [==============================] - 1s 1s/step - loss: 0.7671 - accuracy: 0.9265 - val_loss: 0.7616 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100\n",
      "Learning rate:  0.00067\n",
      "2/2 [==============================] - 2s 335ms/step - loss: 0.7210 - accuracy: 0.9853 - val_loss: 0.6300 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 69/100\n",
      "Learning rate:  0.00068\n",
      "2/2 [==============================] - 1s 1s/step - loss: 0.7285 - accuracy: 0.9559 - val_loss: 0.5997 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 70/100\n",
      "Learning rate:  0.00069\n",
      "2/2 [==============================] - 2s 293ms/step - loss: 0.7642 - accuracy: 0.9118 - val_loss: 0.5975 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 71/100\n",
      "Learning rate:  0.0007\n",
      "2/2 [==============================] - 2s 283ms/step - loss: 0.7361 - accuracy: 0.9559 - val_loss: 0.5969 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 72/100\n",
      "Learning rate:  0.00071\n",
      "2/2 [==============================] - 1s 1s/step - loss: 0.7662 - accuracy: 0.9118 - val_loss: 0.5965 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 73/100\n",
      "Learning rate:  0.0007199999999999999\n",
      "2/2 [==============================] - 1s 1s/step - loss: 0.7674 - accuracy: 0.9265 - val_loss: 0.5962 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 74/100\n",
      "Learning rate:  0.00073\n",
      "2/2 [==============================] - 1s 275ms/step - loss: 0.7742 - accuracy: 0.9412 - val_loss: 0.5965 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 75/100\n",
      "Learning rate:  0.00074\n",
      "2/2 [==============================] - 2s 388ms/step - loss: 0.7162 - accuracy: 0.9559 - val_loss: 0.5981 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 76/100\n",
      "Learning rate:  0.00075\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7417 - accuracy: 0.9559 - val_loss: 0.5989 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 77/100\n",
      "Learning rate:  0.00076\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8378 - accuracy: 0.8971 - val_loss: 0.5965 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 78/100\n",
      "Learning rate:  0.0007700000000000001\n",
      "2/2 [==============================] - 2s 270ms/step - loss: 0.7603 - accuracy: 0.9412 - val_loss: 0.5941 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 79/100\n",
      "Learning rate:  0.0007800000000000001\n",
      "2/2 [==============================] - 1s 1s/step - loss: 0.7179 - accuracy: 0.9706 - val_loss: 0.5932 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 80/100\n",
      "Learning rate:  0.00079\n",
      "2/2 [==============================] - 1s 287ms/step - loss: 0.7629 - accuracy: 0.9706 - val_loss: 0.5927 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 81/100\n",
      "Learning rate:  0.0008\n",
      "2/2 [==============================] - 2s 348ms/step - loss: 0.7902 - accuracy: 0.9118 - val_loss: 0.5922 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 82/100\n",
      "Learning rate:  8.1e-05\n",
      "2/2 [==============================] - 1s 1s/step - loss: 0.8316 - accuracy: 0.8971 - val_loss: 0.5922 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 83/100\n",
      "Learning rate:  8.2e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.8481 - accuracy: 0.8971 - val_loss: 0.5921 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 84/100\n",
      "Learning rate:  8.3e-05\n",
      "2/2 [==============================] - 2s 306ms/step - loss: 0.8197 - accuracy: 0.9412 - val_loss: 0.5921 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 85/100\n",
      "Learning rate:  8.4e-05\n",
      "2/2 [==============================] - 2s 385ms/step - loss: 0.7741 - accuracy: 0.9559 - val_loss: 0.5921 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 86/100\n",
      "Learning rate:  8.5e-05\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7725 - accuracy: 0.9265 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 87/100\n",
      "Learning rate:  8.6e-05\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7626 - accuracy: 0.9559 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 88/100\n",
      "Learning rate:  8.7e-05\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7708 - accuracy: 0.9265 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 89/100\n",
      "Learning rate:  8.800000000000001e-05\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7126 - accuracy: 0.9853 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 90/100\n",
      "Learning rate:  8.900000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7492 - accuracy: 0.9412 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 91/100\n",
      "Learning rate:  9e-05\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7169 - accuracy: 0.9853 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 92/100\n",
      "Learning rate:  9.1e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7344 - accuracy: 0.9559 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 93/100\n",
      "Learning rate:  9.200000000000001e-05\n",
      "2/2 [==============================] - 2s 303ms/step - loss: 0.7348 - accuracy: 0.9706 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 94/100\n",
      "Learning rate:  9.300000000000001e-05\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7318 - accuracy: 0.9559 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 95/100\n",
      "Learning rate:  9.4e-05\n",
      "2/2 [==============================] - 1s 280ms/step - loss: 0.7138 - accuracy: 0.9853 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 96/100\n",
      "Learning rate:  9.5e-05\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7681 - accuracy: 0.9706 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 97/100\n",
      "Learning rate:  9.6e-05\n",
      "2/2 [==============================] - 1s 1s/step - loss: 0.7347 - accuracy: 0.9559 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 98/100\n",
      "Learning rate:  9.7e-05\n",
      "2/2 [==============================] - 2s 308ms/step - loss: 0.7025 - accuracy: 0.9853 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 99/100\n",
      "Learning rate:  9.8e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7842 - accuracy: 0.9412 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 100/100\n",
      "Learning rate:  9.900000000000001e-05\n",
      "2/2 [==============================] - 2s 381ms/step - loss: 0.7264 - accuracy: 0.9559 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 1.3561 - accuracy: 0.7097\n",
      "Test loss: 1.3560811281204224\n",
      "Test accuracy: 0.7096773982048035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision:  0.815994623655914\n",
      "Recall:  0.7096774193548387\n",
      "F1 Score:  0.7278023519257741\n",
      "30\n",
      "30\n",
      "[[[[ 76  84  95 ...  76  84  95]\n",
      "   [ 15  15  20 ...  15  15  20]\n",
      "   [  8   7  11 ...   8   7  11]\n",
      "   ...\n",
      "   [ 16  14  25 ...  16  14  25]\n",
      "   [ 13  13  22 ...  13  13  22]\n",
      "   [  8   9  13 ...   8   9  13]]\n",
      "\n",
      "  [[ 88  96 107 ...  88  96 107]\n",
      "   [ 17  17  22 ...  17  17  22]\n",
      "   [  6   5   9 ...   6   5   9]\n",
      "   ...\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 15  13  24 ...  15  13  24]\n",
      "   [ 10  10  16 ...  10  10  16]]\n",
      "\n",
      "  [[ 86  91  98 ...  86  91  98]\n",
      "   [ 14  13  17 ...  14  13  17]\n",
      "   [  5   4   7 ...   5   4   7]\n",
      "   ...\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 15  13  23 ...  15  13  23]\n",
      "   [ 12  11  18 ...  12  11  18]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[157 162 174 ... 157 162 174]\n",
      "   [159 165 176 ... 159 165 176]\n",
      "   [155 158 159 ... 155 158 159]\n",
      "   ...\n",
      "   [126 136 168 ... 126 136 168]\n",
      "   [145 153 178 ... 145 153 178]\n",
      "   [150 157 180 ... 150 157 180]]\n",
      "\n",
      "  [[158 162 174 ... 158 162 174]\n",
      "   [158 164 177 ... 158 164 177]\n",
      "   [160 167 179 ... 160 167 179]\n",
      "   ...\n",
      "   [134 144 172 ... 134 144 172]\n",
      "   [143 151 177 ... 143 151 177]\n",
      "   [151 158 180 ... 151 158 180]]\n",
      "\n",
      "  [[160 164 176 ... 160 164 176]\n",
      "   [158 164 177 ... 158 164 177]\n",
      "   [163 169 181 ... 163 169 181]\n",
      "   ...\n",
      "   [141 150 174 ... 141 150 174]\n",
      "   [140 149 176 ... 140 149 176]\n",
      "   [150 158 180 ... 150 158 180]]]\n",
      "\n",
      "\n",
      " [[[198 203 206 ... 198 203 206]\n",
      "   [192 196 197 ... 192 196 197]\n",
      "   [154 154 149 ... 154 154 149]\n",
      "   ...\n",
      "   [201 208 206 ... 201 208 206]\n",
      "   [199 207 205 ... 199 207 205]\n",
      "   [192 202 201 ... 192 202 201]]\n",
      "\n",
      "  [[200 204 207 ... 200 204 207]\n",
      "   [184 187 188 ... 184 187 188]\n",
      "   [109 107 101 ... 109 107 101]\n",
      "   ...\n",
      "   [199 206 206 ... 199 206 206]\n",
      "   [201 211 209 ... 201 211 209]\n",
      "   [196 207 205 ... 196 207 205]]\n",
      "\n",
      "  [[207 211 215 ... 207 211 215]\n",
      "   [168 171 170 ... 168 171 170]\n",
      "   [ 62  58  50 ...  62  58  50]\n",
      "   ...\n",
      "   [193 200 200 ... 193 200 200]\n",
      "   [198 207 206 ... 198 207 206]\n",
      "   [198 209 207 ... 198 209 207]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[217 216 217 ... 217 216 217]\n",
      "   [217 214 212 ... 217 214 212]\n",
      "   [223 219 217 ... 223 219 217]\n",
      "   ...\n",
      "   [165 114 111 ... 165 114 111]\n",
      "   [174 125 121 ... 174 125 121]\n",
      "   [175 127 123 ... 175 127 123]]\n",
      "\n",
      "  [[218 217 217 ... 218 217 217]\n",
      "   [218 216 217 ... 218 216 217]\n",
      "   [222 220 218 ... 222 220 218]\n",
      "   ...\n",
      "   [163 113 109 ... 163 113 109]\n",
      "   [171 123 119 ... 171 123 119]\n",
      "   [171 124 119 ... 171 124 119]]\n",
      "\n",
      "  [[216 216 216 ... 216 216 216]\n",
      "   [219 218 217 ... 219 218 217]\n",
      "   [222 221 220 ... 222 221 220]\n",
      "   ...\n",
      "   [162 113 109 ... 162 113 109]\n",
      "   [167 119 115 ... 167 119 115]\n",
      "   [166 120 115 ... 166 120 115]]]\n",
      "\n",
      "\n",
      " [[[166 181 195 ... 166 181 195]\n",
      "   [170 183 194 ... 170 183 194]\n",
      "   [175 188 197 ... 175 188 197]\n",
      "   ...\n",
      "   [178 188 187 ... 178 188 187]\n",
      "   [177 186 185 ... 177 186 185]\n",
      "   [175 185 183 ... 175 185 183]]\n",
      "\n",
      "  [[163 181 196 ... 163 181 196]\n",
      "   [167 183 195 ... 167 183 195]\n",
      "   [170 184 193 ... 170 184 193]\n",
      "   ...\n",
      "   [173 181 176 ... 173 181 176]\n",
      "   [170 180 176 ... 170 180 176]\n",
      "   [172 179 176 ... 172 179 176]]\n",
      "\n",
      "  [[159 178 193 ... 159 178 193]\n",
      "   [165 182 196 ... 165 182 196]\n",
      "   [164 178 188 ... 164 178 188]\n",
      "   ...\n",
      "   [170 177 167 ... 170 177 167]\n",
      "   [160 170 164 ... 160 170 164]\n",
      "   [159 169 165 ... 159 169 165]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[148 100 114 ... 148 100 114]\n",
      "   [144  89  88 ... 144  89  88]\n",
      "   [112  62  51 ... 112  62  51]\n",
      "   ...\n",
      "   [159 103 102 ... 159 103 102]\n",
      "   [156 102  99 ... 156 102  99]\n",
      "   [150 102  97 ... 150 102  97]]\n",
      "\n",
      "  [[141  96 115 ... 141  96 115]\n",
      "   [145  90  93 ... 145  90  93]\n",
      "   [110  56  45 ... 110  56  45]\n",
      "   ...\n",
      "   [156 101 100 ... 156 101 100]\n",
      "   [153  99  96 ... 153  99  96]\n",
      "   [148 101  96 ... 148 101  96]]\n",
      "\n",
      "  [[117  71  89 ... 117  71  89]\n",
      "   [142  85  91 ... 142  85  91]\n",
      "   [108  52  42 ... 108  52  42]\n",
      "   ...\n",
      "   [155 102 102 ... 155 102 102]\n",
      "   [151  98  96 ... 151  98  96]\n",
      "   [146  98  95 ... 146  98  95]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (70, 40, 30, 9)\n",
      "70 train samples\n",
      "30 test samples\n",
      "y_train shape: (70, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_32\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_33 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_992 (Conv2D)             (None, 40, 30, 16)   1312        input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_896 (BatchN (None, 40, 30, 16)   64          conv2d_992[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_896 (Activation)     (None, 40, 30, 16)   0           batch_normalization_896[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_993 (Conv2D)             (None, 40, 30, 16)   272         activation_896[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_897 (BatchN (None, 40, 30, 16)   64          conv2d_993[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_897 (Activation)     (None, 40, 30, 16)   0           batch_normalization_897[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_994 (Conv2D)             (None, 40, 30, 16)   2320        activation_897[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_898 (BatchN (None, 40, 30, 16)   64          conv2d_994[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_898 (Activation)     (None, 40, 30, 16)   0           batch_normalization_898[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_996 (Conv2D)             (None, 40, 30, 64)   1088        activation_896[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_995 (Conv2D)             (None, 40, 30, 64)   1088        activation_898[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_288 (Add)                   (None, 40, 30, 64)   0           conv2d_996[0][0]                 \n",
      "                                                                 conv2d_995[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_899 (BatchN (None, 40, 30, 64)   256         add_288[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_899 (Activation)     (None, 40, 30, 64)   0           batch_normalization_899[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_997 (Conv2D)             (None, 40, 30, 16)   1040        activation_899[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_900 (BatchN (None, 40, 30, 16)   64          conv2d_997[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_900 (Activation)     (None, 40, 30, 16)   0           batch_normalization_900[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_998 (Conv2D)             (None, 40, 30, 16)   2320        activation_900[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_901 (BatchN (None, 40, 30, 16)   64          conv2d_998[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_901 (Activation)     (None, 40, 30, 16)   0           batch_normalization_901[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_999 (Conv2D)             (None, 40, 30, 64)   1088        activation_901[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_289 (Add)                   (None, 40, 30, 64)   0           add_288[0][0]                    \n",
      "                                                                 conv2d_999[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_902 (BatchN (None, 40, 30, 64)   256         add_289[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_902 (Activation)     (None, 40, 30, 64)   0           batch_normalization_902[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1000 (Conv2D)            (None, 40, 30, 16)   1040        activation_902[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_903 (BatchN (None, 40, 30, 16)   64          conv2d_1000[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_903 (Activation)     (None, 40, 30, 16)   0           batch_normalization_903[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1001 (Conv2D)            (None, 40, 30, 16)   2320        activation_903[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_904 (BatchN (None, 40, 30, 16)   64          conv2d_1001[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_904 (Activation)     (None, 40, 30, 16)   0           batch_normalization_904[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1002 (Conv2D)            (None, 40, 30, 64)   1088        activation_904[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_290 (Add)                   (None, 40, 30, 64)   0           add_289[0][0]                    \n",
      "                                                                 conv2d_1002[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_905 (BatchN (None, 40, 30, 64)   256         add_290[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_905 (Activation)     (None, 40, 30, 64)   0           batch_normalization_905[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1003 (Conv2D)            (None, 20, 15, 64)   4160        activation_905[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_906 (BatchN (None, 20, 15, 64)   256         conv2d_1003[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_906 (Activation)     (None, 20, 15, 64)   0           batch_normalization_906[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1004 (Conv2D)            (None, 20, 15, 64)   36928       activation_906[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_907 (BatchN (None, 20, 15, 64)   256         conv2d_1004[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_907 (Activation)     (None, 20, 15, 64)   0           batch_normalization_907[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1006 (Conv2D)            (None, 20, 15, 128)  8320        add_290[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1005 (Conv2D)            (None, 20, 15, 128)  8320        activation_907[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_291 (Add)                   (None, 20, 15, 128)  0           conv2d_1006[0][0]                \n",
      "                                                                 conv2d_1005[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_908 (BatchN (None, 20, 15, 128)  512         add_291[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_908 (Activation)     (None, 20, 15, 128)  0           batch_normalization_908[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1007 (Conv2D)            (None, 20, 15, 64)   8256        activation_908[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_909 (BatchN (None, 20, 15, 64)   256         conv2d_1007[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_909 (Activation)     (None, 20, 15, 64)   0           batch_normalization_909[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1008 (Conv2D)            (None, 20, 15, 64)   36928       activation_909[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_910 (BatchN (None, 20, 15, 64)   256         conv2d_1008[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_910 (Activation)     (None, 20, 15, 64)   0           batch_normalization_910[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1009 (Conv2D)            (None, 20, 15, 128)  8320        activation_910[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_292 (Add)                   (None, 20, 15, 128)  0           add_291[0][0]                    \n",
      "                                                                 conv2d_1009[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_911 (BatchN (None, 20, 15, 128)  512         add_292[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_911 (Activation)     (None, 20, 15, 128)  0           batch_normalization_911[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1010 (Conv2D)            (None, 20, 15, 64)   8256        activation_911[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_912 (BatchN (None, 20, 15, 64)   256         conv2d_1010[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_912 (Activation)     (None, 20, 15, 64)   0           batch_normalization_912[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1011 (Conv2D)            (None, 20, 15, 64)   36928       activation_912[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_913 (BatchN (None, 20, 15, 64)   256         conv2d_1011[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_913 (Activation)     (None, 20, 15, 64)   0           batch_normalization_913[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1012 (Conv2D)            (None, 20, 15, 128)  8320        activation_913[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_293 (Add)                   (None, 20, 15, 128)  0           add_292[0][0]                    \n",
      "                                                                 conv2d_1012[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_914 (BatchN (None, 20, 15, 128)  512         add_293[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_914 (Activation)     (None, 20, 15, 128)  0           batch_normalization_914[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1013 (Conv2D)            (None, 10, 8, 128)   16512       activation_914[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_915 (BatchN (None, 10, 8, 128)   512         conv2d_1013[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_915 (Activation)     (None, 10, 8, 128)   0           batch_normalization_915[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1014 (Conv2D)            (None, 10, 8, 128)   147584      activation_915[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_916 (BatchN (None, 10, 8, 128)   512         conv2d_1014[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_916 (Activation)     (None, 10, 8, 128)   0           batch_normalization_916[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1016 (Conv2D)            (None, 10, 8, 256)   33024       add_293[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1015 (Conv2D)            (None, 10, 8, 256)   33024       activation_916[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_294 (Add)                   (None, 10, 8, 256)   0           conv2d_1016[0][0]                \n",
      "                                                                 conv2d_1015[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_917 (BatchN (None, 10, 8, 256)   1024        add_294[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_917 (Activation)     (None, 10, 8, 256)   0           batch_normalization_917[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1017 (Conv2D)            (None, 10, 8, 128)   32896       activation_917[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_918 (BatchN (None, 10, 8, 128)   512         conv2d_1017[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_918 (Activation)     (None, 10, 8, 128)   0           batch_normalization_918[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1018 (Conv2D)            (None, 10, 8, 128)   147584      activation_918[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_919 (BatchN (None, 10, 8, 128)   512         conv2d_1018[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_919 (Activation)     (None, 10, 8, 128)   0           batch_normalization_919[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1019 (Conv2D)            (None, 10, 8, 256)   33024       activation_919[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_295 (Add)                   (None, 10, 8, 256)   0           add_294[0][0]                    \n",
      "                                                                 conv2d_1019[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_920 (BatchN (None, 10, 8, 256)   1024        add_295[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_920 (Activation)     (None, 10, 8, 256)   0           batch_normalization_920[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1020 (Conv2D)            (None, 10, 8, 128)   32896       activation_920[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_921 (BatchN (None, 10, 8, 128)   512         conv2d_1020[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_921 (Activation)     (None, 10, 8, 128)   0           batch_normalization_921[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1021 (Conv2D)            (None, 10, 8, 128)   147584      activation_921[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_922 (BatchN (None, 10, 8, 128)   512         conv2d_1021[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_922 (Activation)     (None, 10, 8, 128)   0           batch_normalization_922[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1022 (Conv2D)            (None, 10, 8, 256)   33024       activation_922[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_296 (Add)                   (None, 10, 8, 256)   0           add_295[0][0]                    \n",
      "                                                                 conv2d_1022[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_923 (BatchN (None, 10, 8, 256)   1024        add_296[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_923 (Activation)     (None, 10, 8, 256)   0           batch_normalization_923[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_32 (AveragePo (None, 1, 1, 256)    0           activation_923[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 256)          0           average_pooling2d_32[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 10)           2570        flatten_32[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[0.29803923 0.32941177 0.37254903 ... 0.29803923 0.32941177\n",
      "    0.37254903]\n",
      "   [0.05882353 0.05882353 0.07843138 ... 0.05882353 0.05882353\n",
      "    0.07843138]\n",
      "   [0.03137255 0.02745098 0.04313726 ... 0.03137255 0.02745098\n",
      "    0.04313726]\n",
      "   ...\n",
      "   [0.0627451  0.05490196 0.09803922 ... 0.0627451  0.05490196\n",
      "    0.09803922]\n",
      "   [0.05098039 0.05098039 0.08627451 ... 0.05098039 0.05098039\n",
      "    0.08627451]\n",
      "   [0.03137255 0.03529412 0.05098039 ... 0.03137255 0.03529412\n",
      "    0.05098039]]\n",
      "\n",
      "  [[0.34509805 0.3764706  0.41960785 ... 0.34509805 0.3764706\n",
      "    0.41960785]\n",
      "   [0.06666667 0.06666667 0.08627451 ... 0.06666667 0.06666667\n",
      "    0.08627451]\n",
      "   [0.02352941 0.01960784 0.03529412 ... 0.02352941 0.01960784\n",
      "    0.03529412]\n",
      "   ...\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.05882353 0.05098039 0.09411765 ... 0.05882353 0.05098039\n",
      "    0.09411765]\n",
      "   [0.03921569 0.03921569 0.0627451  ... 0.03921569 0.03921569\n",
      "    0.0627451 ]]\n",
      "\n",
      "  [[0.3372549  0.35686275 0.38431373 ... 0.3372549  0.35686275\n",
      "    0.38431373]\n",
      "   [0.05490196 0.05098039 0.06666667 ... 0.05490196 0.05098039\n",
      "    0.06666667]\n",
      "   [0.01960784 0.01568628 0.02745098 ... 0.01960784 0.01568628\n",
      "    0.02745098]\n",
      "   ...\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.05882353 0.05098039 0.09019608 ... 0.05882353 0.05098039\n",
      "    0.09019608]\n",
      "   [0.04705882 0.04313726 0.07058824 ... 0.04705882 0.04313726\n",
      "    0.07058824]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6156863  0.63529414 0.68235296 ... 0.6156863  0.63529414\n",
      "    0.68235296]\n",
      "   [0.62352943 0.64705884 0.6901961  ... 0.62352943 0.64705884\n",
      "    0.6901961 ]\n",
      "   [0.60784316 0.61960787 0.62352943 ... 0.60784316 0.61960787\n",
      "    0.62352943]\n",
      "   ...\n",
      "   [0.49411765 0.53333336 0.65882355 ... 0.49411765 0.53333336\n",
      "    0.65882355]\n",
      "   [0.5686275  0.6        0.69803923 ... 0.5686275  0.6\n",
      "    0.69803923]\n",
      "   [0.5882353  0.6156863  0.7058824  ... 0.5882353  0.6156863\n",
      "    0.7058824 ]]\n",
      "\n",
      "  [[0.61960787 0.63529414 0.68235296 ... 0.61960787 0.63529414\n",
      "    0.68235296]\n",
      "   [0.61960787 0.6431373  0.69411767 ... 0.61960787 0.6431373\n",
      "    0.69411767]\n",
      "   [0.627451   0.654902   0.7019608  ... 0.627451   0.654902\n",
      "    0.7019608 ]\n",
      "   ...\n",
      "   [0.5254902  0.5647059  0.6745098  ... 0.5254902  0.5647059\n",
      "    0.6745098 ]\n",
      "   [0.56078434 0.5921569  0.69411767 ... 0.56078434 0.5921569\n",
      "    0.69411767]\n",
      "   [0.5921569  0.61960787 0.7058824  ... 0.5921569  0.61960787\n",
      "    0.7058824 ]]\n",
      "\n",
      "  [[0.627451   0.6431373  0.6901961  ... 0.627451   0.6431373\n",
      "    0.6901961 ]\n",
      "   [0.61960787 0.6431373  0.69411767 ... 0.61960787 0.6431373\n",
      "    0.69411767]\n",
      "   [0.6392157  0.6627451  0.70980394 ... 0.6392157  0.6627451\n",
      "    0.70980394]\n",
      "   ...\n",
      "   [0.5529412  0.5882353  0.68235296 ... 0.5529412  0.5882353\n",
      "    0.68235296]\n",
      "   [0.54901963 0.58431375 0.6901961  ... 0.54901963 0.58431375\n",
      "    0.6901961 ]\n",
      "   [0.5882353  0.61960787 0.7058824  ... 0.5882353  0.61960787\n",
      "    0.7058824 ]]]\n",
      "\n",
      "\n",
      " [[[0.7764706  0.79607844 0.80784315 ... 0.7764706  0.79607844\n",
      "    0.80784315]\n",
      "   [0.7529412  0.76862746 0.77254903 ... 0.7529412  0.76862746\n",
      "    0.77254903]\n",
      "   [0.6039216  0.6039216  0.58431375 ... 0.6039216  0.6039216\n",
      "    0.58431375]\n",
      "   ...\n",
      "   [0.7882353  0.8156863  0.80784315 ... 0.7882353  0.8156863\n",
      "    0.80784315]\n",
      "   [0.78039217 0.8117647  0.8039216  ... 0.78039217 0.8117647\n",
      "    0.8039216 ]\n",
      "   [0.7529412  0.7921569  0.7882353  ... 0.7529412  0.7921569\n",
      "    0.7882353 ]]\n",
      "\n",
      "  [[0.78431374 0.8        0.8117647  ... 0.78431374 0.8\n",
      "    0.8117647 ]\n",
      "   [0.72156864 0.73333335 0.7372549  ... 0.72156864 0.73333335\n",
      "    0.7372549 ]\n",
      "   [0.42745098 0.41960785 0.39607844 ... 0.42745098 0.41960785\n",
      "    0.39607844]\n",
      "   ...\n",
      "   [0.78039217 0.80784315 0.80784315 ... 0.78039217 0.80784315\n",
      "    0.80784315]\n",
      "   [0.7882353  0.827451   0.81960785 ... 0.7882353  0.827451\n",
      "    0.81960785]\n",
      "   [0.76862746 0.8117647  0.8039216  ... 0.76862746 0.8117647\n",
      "    0.8039216 ]]\n",
      "\n",
      "  [[0.8117647  0.827451   0.84313726 ... 0.8117647  0.827451\n",
      "    0.84313726]\n",
      "   [0.65882355 0.67058825 0.6666667  ... 0.65882355 0.67058825\n",
      "    0.6666667 ]\n",
      "   [0.24313726 0.22745098 0.19607843 ... 0.24313726 0.22745098\n",
      "    0.19607843]\n",
      "   ...\n",
      "   [0.75686276 0.78431374 0.78431374 ... 0.75686276 0.78431374\n",
      "    0.78431374]\n",
      "   [0.7764706  0.8117647  0.80784315 ... 0.7764706  0.8117647\n",
      "    0.80784315]\n",
      "   [0.7764706  0.81960785 0.8117647  ... 0.7764706  0.81960785\n",
      "    0.8117647 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8509804  0.84705883 0.8509804  ... 0.8509804  0.84705883\n",
      "    0.8509804 ]\n",
      "   [0.8509804  0.8392157  0.83137256 ... 0.8509804  0.8392157\n",
      "    0.83137256]\n",
      "   [0.8745098  0.85882354 0.8509804  ... 0.8745098  0.85882354\n",
      "    0.8509804 ]\n",
      "   ...\n",
      "   [0.64705884 0.44705883 0.43529412 ... 0.64705884 0.44705883\n",
      "    0.43529412]\n",
      "   [0.68235296 0.49019608 0.4745098  ... 0.68235296 0.49019608\n",
      "    0.4745098 ]\n",
      "   [0.6862745  0.49803922 0.48235294 ... 0.6862745  0.49803922\n",
      "    0.48235294]]\n",
      "\n",
      "  [[0.85490197 0.8509804  0.8509804  ... 0.85490197 0.8509804\n",
      "    0.8509804 ]\n",
      "   [0.85490197 0.84705883 0.8509804  ... 0.85490197 0.84705883\n",
      "    0.8509804 ]\n",
      "   [0.87058824 0.8627451  0.85490197 ... 0.87058824 0.8627451\n",
      "    0.85490197]\n",
      "   ...\n",
      "   [0.6392157  0.44313726 0.42745098 ... 0.6392157  0.44313726\n",
      "    0.42745098]\n",
      "   [0.67058825 0.48235294 0.46666667 ... 0.67058825 0.48235294\n",
      "    0.46666667]\n",
      "   [0.67058825 0.4862745  0.46666667 ... 0.67058825 0.4862745\n",
      "    0.46666667]]\n",
      "\n",
      "  [[0.84705883 0.84705883 0.84705883 ... 0.84705883 0.84705883\n",
      "    0.84705883]\n",
      "   [0.85882354 0.85490197 0.8509804  ... 0.85882354 0.85490197\n",
      "    0.8509804 ]\n",
      "   [0.87058824 0.8666667  0.8627451  ... 0.87058824 0.8666667\n",
      "    0.8627451 ]\n",
      "   ...\n",
      "   [0.63529414 0.44313726 0.42745098 ... 0.63529414 0.44313726\n",
      "    0.42745098]\n",
      "   [0.654902   0.46666667 0.4509804  ... 0.654902   0.46666667\n",
      "    0.4509804 ]\n",
      "   [0.6509804  0.47058824 0.4509804  ... 0.6509804  0.47058824\n",
      "    0.4509804 ]]]\n",
      "\n",
      "\n",
      " [[[0.6509804  0.70980394 0.7647059  ... 0.6509804  0.70980394\n",
      "    0.7647059 ]\n",
      "   [0.6666667  0.7176471  0.7607843  ... 0.6666667  0.7176471\n",
      "    0.7607843 ]\n",
      "   [0.6862745  0.7372549  0.77254903 ... 0.6862745  0.7372549\n",
      "    0.77254903]\n",
      "   ...\n",
      "   [0.69803923 0.7372549  0.73333335 ... 0.69803923 0.7372549\n",
      "    0.73333335]\n",
      "   [0.69411767 0.7294118  0.7254902  ... 0.69411767 0.7294118\n",
      "    0.7254902 ]\n",
      "   [0.6862745  0.7254902  0.7176471  ... 0.6862745  0.7254902\n",
      "    0.7176471 ]]\n",
      "\n",
      "  [[0.6392157  0.70980394 0.76862746 ... 0.6392157  0.70980394\n",
      "    0.76862746]\n",
      "   [0.654902   0.7176471  0.7647059  ... 0.654902   0.7176471\n",
      "    0.7647059 ]\n",
      "   [0.6666667  0.72156864 0.75686276 ... 0.6666667  0.72156864\n",
      "    0.75686276]\n",
      "   ...\n",
      "   [0.6784314  0.70980394 0.6901961  ... 0.6784314  0.70980394\n",
      "    0.6901961 ]\n",
      "   [0.6666667  0.7058824  0.6901961  ... 0.6666667  0.7058824\n",
      "    0.6901961 ]\n",
      "   [0.6745098  0.7019608  0.6901961  ... 0.6745098  0.7019608\n",
      "    0.6901961 ]]\n",
      "\n",
      "  [[0.62352943 0.69803923 0.75686276 ... 0.62352943 0.69803923\n",
      "    0.75686276]\n",
      "   [0.64705884 0.7137255  0.76862746 ... 0.64705884 0.7137255\n",
      "    0.76862746]\n",
      "   [0.6431373  0.69803923 0.7372549  ... 0.6431373  0.69803923\n",
      "    0.7372549 ]\n",
      "   ...\n",
      "   [0.6666667  0.69411767 0.654902   ... 0.6666667  0.69411767\n",
      "    0.654902  ]\n",
      "   [0.627451   0.6666667  0.6431373  ... 0.627451   0.6666667\n",
      "    0.6431373 ]\n",
      "   [0.62352943 0.6627451  0.64705884 ... 0.62352943 0.6627451\n",
      "    0.64705884]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.5803922  0.39215687 0.44705883 ... 0.5803922  0.39215687\n",
      "    0.44705883]\n",
      "   [0.5647059  0.34901962 0.34509805 ... 0.5647059  0.34901962\n",
      "    0.34509805]\n",
      "   [0.4392157  0.24313726 0.2        ... 0.4392157  0.24313726\n",
      "    0.2       ]\n",
      "   ...\n",
      "   [0.62352943 0.40392157 0.4        ... 0.62352943 0.40392157\n",
      "    0.4       ]\n",
      "   [0.6117647  0.4        0.3882353  ... 0.6117647  0.4\n",
      "    0.3882353 ]\n",
      "   [0.5882353  0.4        0.38039216 ... 0.5882353  0.4\n",
      "    0.38039216]]\n",
      "\n",
      "  [[0.5529412  0.3764706  0.4509804  ... 0.5529412  0.3764706\n",
      "    0.4509804 ]\n",
      "   [0.5686275  0.3529412  0.3647059  ... 0.5686275  0.3529412\n",
      "    0.3647059 ]\n",
      "   [0.43137255 0.21960784 0.1764706  ... 0.43137255 0.21960784\n",
      "    0.1764706 ]\n",
      "   ...\n",
      "   [0.6117647  0.39607844 0.39215687 ... 0.6117647  0.39607844\n",
      "    0.39215687]\n",
      "   [0.6        0.3882353  0.3764706  ... 0.6        0.3882353\n",
      "    0.3764706 ]\n",
      "   [0.5803922  0.39607844 0.3764706  ... 0.5803922  0.39607844\n",
      "    0.3764706 ]]\n",
      "\n",
      "  [[0.45882353 0.2784314  0.34901962 ... 0.45882353 0.2784314\n",
      "    0.34901962]\n",
      "   [0.5568628  0.33333334 0.35686275 ... 0.5568628  0.33333334\n",
      "    0.35686275]\n",
      "   [0.42352942 0.20392157 0.16470589 ... 0.42352942 0.20392157\n",
      "    0.16470589]\n",
      "   ...\n",
      "   [0.60784316 0.4        0.4        ... 0.60784316 0.4\n",
      "    0.4       ]\n",
      "   [0.5921569  0.38431373 0.3764706  ... 0.5921569  0.38431373\n",
      "    0.3764706 ]\n",
      "   [0.57254905 0.38431373 0.37254903 ... 0.57254905 0.38431373\n",
      "    0.37254903]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.04705882 0.04705882 0.04705882 ... 0.04705882 0.04705882\n",
      "    0.04705882]\n",
      "   [0.01176471 0.01176471 0.01176471 ... 0.01176471 0.01176471\n",
      "    0.01176471]\n",
      "   [0.03529412 0.03529412 0.03529412 ... 0.03529412 0.03529412\n",
      "    0.03529412]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.01176471 0.01176471 0.01176471 ... 0.01176471 0.01176471\n",
      "    0.01176471]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.00784314 0.00784314 0.00784314 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.65882355 0.5411765  0.34901962 ... 0.65882355 0.5411765\n",
      "    0.34901962]\n",
      "   [0.79607844 0.6862745  0.44313726 ... 0.79607844 0.6862745\n",
      "    0.44313726]\n",
      "   [0.84705883 0.7372549  0.4862745  ... 0.84705883 0.7372549\n",
      "    0.4862745 ]\n",
      "   ...\n",
      "   [0.9019608  0.78431374 0.57254905 ... 0.9019608  0.78431374\n",
      "    0.57254905]\n",
      "   [0.8784314  0.77254903 0.5803922  ... 0.8784314  0.77254903\n",
      "    0.5803922 ]\n",
      "   [0.8666667  0.7607843  0.58431375 ... 0.8666667  0.7607843\n",
      "    0.58431375]]\n",
      "\n",
      "  [[0.6313726  0.5058824  0.27450982 ... 0.6313726  0.5058824\n",
      "    0.27450982]\n",
      "   [0.7372549  0.61960787 0.3764706  ... 0.7372549  0.61960787\n",
      "    0.3764706 ]\n",
      "   [0.83137256 0.72156864 0.4627451  ... 0.83137256 0.72156864\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.8784314  0.7607843  0.5568628  ... 0.8784314  0.7607843\n",
      "    0.5568628 ]\n",
      "   [0.85882354 0.7490196  0.56078434 ... 0.85882354 0.7490196\n",
      "    0.56078434]\n",
      "   [0.85490197 0.7529412  0.5647059  ... 0.85490197 0.7529412\n",
      "    0.5647059 ]]\n",
      "\n",
      "  [[0.6        0.46666667 0.22352941 ... 0.6        0.46666667\n",
      "    0.22352941]\n",
      "   [0.6313726  0.49803922 0.27058825 ... 0.6313726  0.49803922\n",
      "    0.27058825]\n",
      "   [0.80784315 0.69803923 0.44705883 ... 0.80784315 0.69803923\n",
      "    0.44705883]\n",
      "   ...\n",
      "   [0.8352941  0.72156864 0.5372549  ... 0.8352941  0.72156864\n",
      "    0.5372549 ]\n",
      "   [0.83137256 0.7254902  0.5411765  ... 0.83137256 0.7254902\n",
      "    0.5411765 ]\n",
      "   [0.8352941  0.73333335 0.5372549  ... 0.8352941  0.73333335\n",
      "    0.5372549 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6039216  0.3764706  0.27058825 ... 0.6039216  0.3764706\n",
      "    0.27058825]\n",
      "   [0.627451   0.40784314 0.31764707 ... 0.627451   0.40784314\n",
      "    0.31764707]\n",
      "   [0.67058825 0.47058824 0.4        ... 0.67058825 0.47058824\n",
      "    0.4       ]\n",
      "   ...\n",
      "   [0.76862746 0.58431375 0.4745098  ... 0.76862746 0.58431375\n",
      "    0.4745098 ]\n",
      "   [0.7058824  0.49803922 0.38431373 ... 0.7058824  0.49803922\n",
      "    0.38431373]\n",
      "   [0.65882355 0.44313726 0.32156864 ... 0.65882355 0.44313726\n",
      "    0.32156864]]\n",
      "\n",
      "  [[0.6        0.38039216 0.26666668 ... 0.6        0.38039216\n",
      "    0.26666668]\n",
      "   [0.63529414 0.41568628 0.32156864 ... 0.63529414 0.41568628\n",
      "    0.32156864]\n",
      "   [0.7372549  0.56078434 0.4627451  ... 0.7372549  0.56078434\n",
      "    0.4627451 ]\n",
      "   ...\n",
      "   [0.83137256 0.654902   0.54901963 ... 0.83137256 0.654902\n",
      "    0.54901963]\n",
      "   [0.77254903 0.5647059  0.44313726 ... 0.77254903 0.5647059\n",
      "    0.44313726]\n",
      "   [0.7176471  0.5019608  0.38039216 ... 0.7176471  0.5019608\n",
      "    0.38039216]]\n",
      "\n",
      "  [[0.627451   0.41960785 0.29411766 ... 0.627451   0.41960785\n",
      "    0.29411766]\n",
      "   [0.68235296 0.46666667 0.3882353  ... 0.68235296 0.46666667\n",
      "    0.3882353 ]\n",
      "   [0.8509804  0.7294118  0.49019608 ... 0.8509804  0.7294118\n",
      "    0.49019608]\n",
      "   ...\n",
      "   [0.89411765 0.72156864 0.6117647  ... 0.89411765 0.72156864\n",
      "    0.6117647 ]\n",
      "   [0.8392157  0.627451   0.50980395 ... 0.8392157  0.627451\n",
      "    0.50980395]\n",
      "   [0.76862746 0.56078434 0.43529412 ... 0.76862746 0.56078434\n",
      "    0.43529412]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.99215686 0.99215686 0.99215686 ... 0.99215686 0.99215686\n",
      "    0.99215686]\n",
      "   ...\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]\n",
      "   [1.         1.         1.         ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6509804  0.6117647  0.60784316 ... 0.6509804  0.6117647\n",
      "    0.60784316]\n",
      "   [0.6431373  0.6        0.5921569  ... 0.6431373  0.6\n",
      "    0.5921569 ]\n",
      "   [0.63529414 0.5921569  0.5803922  ... 0.63529414 0.5921569\n",
      "    0.5803922 ]\n",
      "   ...\n",
      "   [0.80784315 0.69803923 0.67058825 ... 0.80784315 0.69803923\n",
      "    0.67058825]\n",
      "   [0.80784315 0.7176471  0.6901961  ... 0.80784315 0.7176471\n",
      "    0.6901961 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]]\n",
      "\n",
      "  [[0.64705884 0.59607846 0.59607846 ... 0.64705884 0.59607846\n",
      "    0.59607846]\n",
      "   [0.63529414 0.5921569  0.58431375 ... 0.63529414 0.5921569\n",
      "    0.58431375]\n",
      "   [0.62352943 0.5803922  0.57254905 ... 0.62352943 0.5803922\n",
      "    0.57254905]\n",
      "   ...\n",
      "   [0.8        0.69803923 0.6745098  ... 0.8        0.69803923\n",
      "    0.6745098 ]\n",
      "   [0.8        0.70980394 0.6862745  ... 0.8        0.70980394\n",
      "    0.6862745 ]\n",
      "   [0.7921569  0.7019608  0.68235296 ... 0.7921569  0.7019608\n",
      "    0.68235296]]\n",
      "\n",
      "  [[0.6431373  0.59607846 0.5921569  ... 0.6431373  0.59607846\n",
      "    0.5921569 ]\n",
      "   [0.6313726  0.5882353  0.5803922  ... 0.6313726  0.5882353\n",
      "    0.5803922 ]\n",
      "   [0.627451   0.57254905 0.5686275  ... 0.627451   0.57254905\n",
      "    0.5686275 ]\n",
      "   ...\n",
      "   [0.7921569  0.6901961  0.6784314  ... 0.7921569  0.6901961\n",
      "    0.6784314 ]\n",
      "   [0.80784315 0.7058824  0.6901961  ... 0.80784315 0.7058824\n",
      "    0.6901961 ]\n",
      "   [0.80784315 0.7176471  0.7019608  ... 0.80784315 0.7176471\n",
      "    0.7019608 ]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (70, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (70, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 6s 2s/step - loss: 3.3289 - accuracy: 0.0286 - val_loss: 2.9949 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/100\n",
      "Learning rate:  1e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 3.3343 - accuracy: 0.0286 - val_loss: 3.0326 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/100\n",
      "Learning rate:  2e-05\n",
      "2/2 [==============================] - 2s 315ms/step - loss: 3.3455 - accuracy: 0.0286 - val_loss: 3.0510 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/100\n",
      "Learning rate:  3e-05\n",
      "2/2 [==============================] - 2s 307ms/step - loss: 3.3208 - accuracy: 0.0286 - val_loss: 3.0548 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/100\n",
      "Learning rate:  4e-05\n",
      "2/2 [==============================] - 2s 309ms/step - loss: 3.2920 - accuracy: 0.0286 - val_loss: 3.0508 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/100\n",
      "Learning rate:  5e-05\n",
      "2/2 [==============================] - 2s 297ms/step - loss: 3.2826 - accuracy: 0.0286 - val_loss: 3.0416 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/100\n",
      "Learning rate:  6e-05\n",
      "2/2 [==============================] - 2s 1s/step - loss: 3.2086 - accuracy: 0.0429 - val_loss: 3.0271 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/100\n",
      "Learning rate:  7.000000000000001e-05\n",
      "2/2 [==============================] - 1s 1s/step - loss: 3.1594 - accuracy: 0.0571 - val_loss: 3.0116 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/100\n",
      "Learning rate:  8e-05\n",
      "2/2 [==============================] - 2s 337ms/step - loss: 3.1262 - accuracy: 0.0571 - val_loss: 2.9922 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/100\n",
      "Learning rate:  8.999999999999999e-05\n",
      "2/2 [==============================] - 2s 338ms/step - loss: 3.1060 - accuracy: 0.0571 - val_loss: 2.9681 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/100\n",
      "Learning rate:  0.0001\n",
      "2/2 [==============================] - 2s 1s/step - loss: 3.0126 - accuracy: 0.0571 - val_loss: 2.9473 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/100\n",
      "Learning rate:  0.00011\n",
      "2/2 [==============================] - 2s 1s/step - loss: 2.9445 - accuracy: 0.1143 - val_loss: 2.9262 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/100\n",
      "Learning rate:  0.00012\n",
      "2/2 [==============================] - 1s 1s/step - loss: 2.8738 - accuracy: 0.1429 - val_loss: 2.9022 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/100\n",
      "Learning rate:  0.00013000000000000002\n",
      "2/2 [==============================] - 2s 298ms/step - loss: 2.8091 - accuracy: 0.2286 - val_loss: 2.8767 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/100\n",
      "Learning rate:  0.00014000000000000001\n",
      "2/2 [==============================] - 2s 303ms/step - loss: 2.7418 - accuracy: 0.2571 - val_loss: 2.8486 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/100\n",
      "Learning rate:  0.00015\n",
      "2/2 [==============================] - 1s 1s/step - loss: 2.6186 - accuracy: 0.3714 - val_loss: 2.8250 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/100\n",
      "Learning rate:  0.00016\n",
      "2/2 [==============================] - 2s 1s/step - loss: 2.5189 - accuracy: 0.3857 - val_loss: 2.8094 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/100\n",
      "Learning rate:  0.00017\n",
      "2/2 [==============================] - 1s 1s/step - loss: 2.4263 - accuracy: 0.4429 - val_loss: 2.7929 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/100\n",
      "Learning rate:  0.00017999999999999998\n",
      "2/2 [==============================] - 2s 328ms/step - loss: 2.3625 - accuracy: 0.4857 - val_loss: 2.7779 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/100\n",
      "Learning rate:  0.00019\n",
      "2/2 [==============================] - 2s 1s/step - loss: 2.2275 - accuracy: 0.5429 - val_loss: 2.7673 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 21/100\n",
      "Learning rate:  0.0002\n",
      "2/2 [==============================] - 2s 411ms/step - loss: 2.1738 - accuracy: 0.5571 - val_loss: 2.7596 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 22/100\n",
      "Learning rate:  0.00021\n",
      "2/2 [==============================] - 2s 367ms/step - loss: 2.0644 - accuracy: 0.5714 - val_loss: 2.7617 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 23/100\n",
      "Learning rate:  0.00022\n",
      "2/2 [==============================] - 2s 1s/step - loss: 1.9111 - accuracy: 0.6286 - val_loss: 2.7527 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 24/100\n",
      "Learning rate:  0.00023\n",
      "2/2 [==============================] - 2s 1s/step - loss: 1.8047 - accuracy: 0.6429 - val_loss: 2.7470 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 25/100\n",
      "Learning rate:  0.00024\n",
      "2/2 [==============================] - 2s 324ms/step - loss: 1.7754 - accuracy: 0.6000 - val_loss: 2.7469 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 26/100\n",
      "Learning rate:  0.00025\n",
      "2/2 [==============================] - 2s 316ms/step - loss: 1.6559 - accuracy: 0.7143 - val_loss: 2.7597 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 27/100\n",
      "Learning rate:  0.00026000000000000003\n",
      "2/2 [==============================] - 2s 2s/step - loss: 1.5410 - accuracy: 0.7429 - val_loss: 2.7782 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 28/100\n",
      "Learning rate:  0.00027\n",
      "2/2 [==============================] - 2s 1s/step - loss: 1.4544 - accuracy: 0.7714 - val_loss: 2.7881 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 29/100\n",
      "Learning rate:  0.00028000000000000003\n",
      "2/2 [==============================] - 2s 423ms/step - loss: 1.4344 - accuracy: 0.7714 - val_loss: 2.8131 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 30/100\n",
      "Learning rate:  0.00029\n",
      "2/2 [==============================] - 2s 325ms/step - loss: 1.3615 - accuracy: 0.7714 - val_loss: 2.8118 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 31/100\n",
      "Learning rate:  0.0003\n",
      "2/2 [==============================] - 2s 306ms/step - loss: 1.3287 - accuracy: 0.7571 - val_loss: 2.7937 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 32/100\n",
      "Learning rate:  0.00031\n",
      "2/2 [==============================] - 2s 1s/step - loss: 1.2641 - accuracy: 0.8000 - val_loss: 2.7819 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 33/100\n",
      "Learning rate:  0.00032\n",
      "2/2 [==============================] - 2s 1s/step - loss: 1.2646 - accuracy: 0.8286 - val_loss: 2.7678 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 34/100\n",
      "Learning rate:  0.00033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 343ms/step - loss: 1.2386 - accuracy: 0.8000 - val_loss: 2.8196 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 35/100\n",
      "Learning rate:  0.00034\n",
      "2/2 [==============================] - 2s 304ms/step - loss: 1.2413 - accuracy: 0.8000 - val_loss: 2.8841 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 36/100\n",
      "Learning rate:  0.00035\n",
      "2/2 [==============================] - 2s 1s/step - loss: 1.1737 - accuracy: 0.8143 - val_loss: 2.9492 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 37/100\n",
      "Learning rate:  0.00035999999999999997\n",
      "2/2 [==============================] - 2s 1s/step - loss: 1.1527 - accuracy: 0.8143 - val_loss: 3.0072 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 38/100\n",
      "Learning rate:  0.00037\n",
      "2/2 [==============================] - 2s 321ms/step - loss: 1.1172 - accuracy: 0.8571 - val_loss: 3.0961 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 39/100\n",
      "Learning rate:  0.00038\n",
      "2/2 [==============================] - 2s 1s/step - loss: 1.1470 - accuracy: 0.8000 - val_loss: 3.1694 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 40/100\n",
      "Learning rate:  0.00039000000000000005\n",
      "2/2 [==============================] - 2s 1s/step - loss: 1.0677 - accuracy: 0.8429 - val_loss: 3.1350 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 41/100\n",
      "Learning rate:  0.0004\n",
      "2/2 [==============================] - 2s 1s/step - loss: 1.0742 - accuracy: 0.8286 - val_loss: 3.1449 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 42/100\n",
      "Learning rate:  0.00041\n",
      "2/2 [==============================] - 2s 1s/step - loss: 1.0700 - accuracy: 0.8429 - val_loss: 3.0912 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 43/100\n",
      "Learning rate:  0.00042\n",
      "2/2 [==============================] - 2s 314ms/step - loss: 1.0495 - accuracy: 0.8571 - val_loss: 3.0814 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 44/100\n",
      "Learning rate:  0.00043\n",
      "2/2 [==============================] - 2s 1s/step - loss: 1.0123 - accuracy: 0.8714 - val_loss: 3.1354 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 45/100\n",
      "Learning rate:  0.00044\n",
      "2/2 [==============================] - 2s 299ms/step - loss: 0.9766 - accuracy: 0.8857 - val_loss: 3.2452 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 46/100\n",
      "Learning rate:  0.00045000000000000004\n",
      "2/2 [==============================] - 2s 1s/step - loss: 1.0176 - accuracy: 0.8571 - val_loss: 2.9707 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 47/100\n",
      "Learning rate:  0.00046\n",
      "2/2 [==============================] - 2s 304ms/step - loss: 1.0494 - accuracy: 0.8143 - val_loss: 2.5324 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 48/100\n",
      "Learning rate:  0.00047\n",
      "2/2 [==============================] - 2s 333ms/step - loss: 0.9711 - accuracy: 0.8857 - val_loss: 1.8825 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 49/100\n",
      "Learning rate:  0.00048\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.8946 - accuracy: 0.9286 - val_loss: 1.3742 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 50/100\n",
      "Learning rate:  0.00049\n",
      "2/2 [==============================] - 2s 324ms/step - loss: 0.9520 - accuracy: 0.8857 - val_loss: 1.0165 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 51/100\n",
      "Learning rate:  0.0005\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.9593 - accuracy: 0.8857 - val_loss: 0.7682 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 52/100\n",
      "Learning rate:  0.00051\n",
      "2/2 [==============================] - 1s 1s/step - loss: 0.8971 - accuracy: 0.9000 - val_loss: 0.6827 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 53/100\n",
      "Learning rate:  0.0005200000000000001\n",
      "2/2 [==============================] - 2s 319ms/step - loss: 0.8805 - accuracy: 0.9429 - val_loss: 0.6447 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 54/100\n",
      "Learning rate:  0.0005300000000000001\n",
      "2/2 [==============================] - 2s 313ms/step - loss: 0.8825 - accuracy: 0.9143 - val_loss: 0.6247 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 55/100\n",
      "Learning rate:  0.00054\n",
      "2/2 [==============================] - 2s 301ms/step - loss: 0.8655 - accuracy: 0.9429 - val_loss: 0.6123 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 56/100\n",
      "Learning rate:  0.00055\n",
      "2/2 [==============================] - 2s 314ms/step - loss: 0.8620 - accuracy: 0.9429 - val_loss: 0.6052 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 57/100\n",
      "Learning rate:  0.0005600000000000001\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.8974 - accuracy: 0.9143 - val_loss: 0.6018 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 58/100\n",
      "Learning rate:  0.00057\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.8561 - accuracy: 0.8714 - val_loss: 0.6006 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 59/100\n",
      "Learning rate:  0.00058\n",
      "2/2 [==============================] - 2s 316ms/step - loss: 0.8468 - accuracy: 0.9286 - val_loss: 0.5999 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 60/100\n",
      "Learning rate:  0.00059\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.8339 - accuracy: 0.9286 - val_loss: 0.5995 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 61/100\n",
      "Learning rate:  0.0006\n",
      "2/2 [==============================] - 2s 312ms/step - loss: 0.8064 - accuracy: 0.9429 - val_loss: 0.5990 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 62/100\n",
      "Learning rate:  0.00061\n",
      "2/2 [==============================] - 2s 299ms/step - loss: 0.7622 - accuracy: 0.9714 - val_loss: 0.5986 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 63/100\n",
      "Learning rate:  0.00062\n",
      "2/2 [==============================] - 2s 296ms/step - loss: 0.7780 - accuracy: 0.9714 - val_loss: 0.5982 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 64/100\n",
      "Learning rate:  0.00063\n",
      "2/2 [==============================] - 2s 323ms/step - loss: 0.7748 - accuracy: 0.9429 - val_loss: 0.5977 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 65/100\n",
      "Learning rate:  0.00064\n",
      "2/2 [==============================] - 2s 324ms/step - loss: 0.7855 - accuracy: 0.9143 - val_loss: 0.5973 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 66/100\n",
      "Learning rate:  0.0006500000000000001\n",
      "2/2 [==============================] - 2s 309ms/step - loss: 0.7683 - accuracy: 0.9286 - val_loss: 0.5969 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 67/100\n",
      "Learning rate:  0.00066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 1s/step - loss: 0.7563 - accuracy: 0.9571 - val_loss: 0.5964 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 68/100\n",
      "Learning rate:  0.00067\n",
      "2/2 [==============================] - 2s 313ms/step - loss: 0.7437 - accuracy: 0.9571 - val_loss: 0.5960 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 69/100\n",
      "Learning rate:  0.00068\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7648 - accuracy: 0.9571 - val_loss: 0.5956 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 70/100\n",
      "Learning rate:  0.00069\n",
      "2/2 [==============================] - 1s 1s/step - loss: 0.7236 - accuracy: 0.9714 - val_loss: 0.5951 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 71/100\n",
      "Learning rate:  0.0007\n",
      "2/2 [==============================] - 2s 328ms/step - loss: 0.7001 - accuracy: 1.0000 - val_loss: 0.5947 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 72/100\n",
      "Learning rate:  0.00071\n",
      "2/2 [==============================] - 2s 321ms/step - loss: 0.7297 - accuracy: 0.9714 - val_loss: 0.5942 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 73/100\n",
      "Learning rate:  0.0007199999999999999\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7596 - accuracy: 0.9429 - val_loss: 0.5937 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 74/100\n",
      "Learning rate:  0.00073\n",
      "2/2 [==============================] - 2s 317ms/step - loss: 0.6881 - accuracy: 0.9857 - val_loss: 0.5933 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 75/100\n",
      "Learning rate:  0.00074\n",
      "2/2 [==============================] - 2s 305ms/step - loss: 0.7290 - accuracy: 0.9571 - val_loss: 0.5928 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 76/100\n",
      "Learning rate:  0.00075\n",
      "2/2 [==============================] - 2s 310ms/step - loss: 0.7703 - accuracy: 0.9286 - val_loss: 0.5923 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 77/100\n",
      "Learning rate:  0.00076\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7276 - accuracy: 0.9429 - val_loss: 0.5918 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 78/100\n",
      "Learning rate:  0.0007700000000000001\n",
      "2/2 [==============================] - 1s 1s/step - loss: 0.7146 - accuracy: 0.9429 - val_loss: 0.5913 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 79/100\n",
      "Learning rate:  0.0007800000000000001\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7017 - accuracy: 0.9714 - val_loss: 0.5908 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 80/100\n",
      "Learning rate:  0.00079\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.8115 - accuracy: 0.9286 - val_loss: 0.5903 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 81/100\n",
      "Learning rate:  0.0008\n",
      "2/2 [==============================] - 2s 324ms/step - loss: 0.7202 - accuracy: 0.9571 - val_loss: 0.5900 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 82/100\n",
      "Learning rate:  8.1e-05\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7557 - accuracy: 0.9571 - val_loss: 0.5901 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 83/100\n",
      "Learning rate:  8.2e-05\n",
      "2/2 [==============================] - 2s 357ms/step - loss: 0.7092 - accuracy: 0.9857 - val_loss: 0.5902 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 84/100\n",
      "Learning rate:  8.3e-05\n",
      "2/2 [==============================] - 2s 306ms/step - loss: 0.6865 - accuracy: 0.9857 - val_loss: 0.5903 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 85/100\n",
      "Learning rate:  8.4e-05\n",
      "2/2 [==============================] - 2s 332ms/step - loss: 0.7073 - accuracy: 0.9571 - val_loss: 0.5905 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 86/100\n",
      "Learning rate:  8.5e-05\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7024 - accuracy: 0.9857 - val_loss: 0.5909 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 87/100\n",
      "Learning rate:  8.6e-05\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7043 - accuracy: 0.9571 - val_loss: 0.5914 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 88/100\n",
      "Learning rate:  8.7e-05\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.6787 - accuracy: 1.0000 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 89/100\n",
      "Learning rate:  8.800000000000001e-05\n",
      "2/2 [==============================] - 2s 313ms/step - loss: 0.7091 - accuracy: 0.9714 - val_loss: 0.5930 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 90/100\n",
      "Learning rate:  8.900000000000001e-05\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7503 - accuracy: 0.9571 - val_loss: 0.5949 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 91/100\n",
      "Learning rate:  9e-05\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7025 - accuracy: 0.9571 - val_loss: 0.5962 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 92/100\n",
      "Learning rate:  9.1e-05\n",
      "2/2 [==============================] - 1s 1s/step - loss: 0.6846 - accuracy: 0.9857 - val_loss: 0.5984 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 93/100\n",
      "Learning rate:  9.200000000000001e-05\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.6909 - accuracy: 0.9714 - val_loss: 0.5996 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 94/100\n",
      "Learning rate:  9.300000000000001e-05\n",
      "2/2 [==============================] - 2s 324ms/step - loss: 0.6927 - accuracy: 0.9857 - val_loss: 0.6014 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 95/100\n",
      "Learning rate:  9.4e-05\n",
      "2/2 [==============================] - 2s 332ms/step - loss: 0.6811 - accuracy: 0.9714 - val_loss: 0.6043 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 96/100\n",
      "Learning rate:  9.5e-05\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7106 - accuracy: 0.9571 - val_loss: 0.6063 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 97/100\n",
      "Learning rate:  9.6e-05\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.6647 - accuracy: 1.0000 - val_loss: 0.6088 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 98/100\n",
      "Learning rate:  9.7e-05\n",
      "2/2 [==============================] - 2s 321ms/step - loss: 0.6869 - accuracy: 0.9714 - val_loss: 0.6114 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 99/100\n",
      "Learning rate:  9.8e-05\n",
      "2/2 [==============================] - 2s 331ms/step - loss: 0.6854 - accuracy: 0.9571 - val_loss: 0.6116 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 100/100\n",
      "Learning rate:  9.900000000000001e-05\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.7236 - accuracy: 0.9714 - val_loss: 0.6122 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 1.6228 - accuracy: 0.6000\n",
      "Test loss: 1.6228071451187134\n",
      "Test accuracy: 0.6000000238418579\n",
      "[0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision:  0.84\n",
      "Recall:  0.6\n",
      "F1 Score:  0.6107142857142858\n",
      "[0.7241379022598267, 0.8333333134651184, 0.699999988079071, 0.8333333134651184, 0.800000011920929, 0.75, 0.931034505367279, 0.9230769276618958, 0.7096773982048035, 0.6000000238418579]\n",
      "[0.871264367816092, 0.8888888888888888, 0.7625, 0.8928571428571428, 0.8, 0.8472222222222222, 0.9373040752351096, 0.9311740890688258, 0.815994623655914, 0.84]\n",
      "[0.7241379310344828, 0.8333333333333334, 0.7, 0.8333333333333334, 0.8, 0.75, 0.9310344827586207, 0.9230769230769231, 0.7096774193548387, 0.6]\n",
      "[0.7436433298502263, 0.8380952380952381, 0.7123384253819036, 0.8401880141010576, 0.8, 0.747765006385696, 0.9283661740558292, 0.9204059829059829, 0.7278023519257741, 0.6107142857142858]\n"
     ]
    }
   ],
   "source": [
    "accs=[]\n",
    "precisions = []\n",
    "recalls = []\n",
    "fmeasures = []\n",
    "created_model = None\n",
    "cnt = 0\n",
    "while cnt < 10:\n",
    "    vf=True\n",
    "    #print(cnt)\n",
    "    try:\n",
    "        ret = main(cnt, created_model)\n",
    "        cnt += 1\n",
    "    except:\n",
    "        vf = False\n",
    "    if not vf:\n",
    "        continue\n",
    "    \n",
    "    accs+=[ret[0]]\n",
    "    precisions+=[ret[1]]\n",
    "    recalls+=[ret[2]]\n",
    "    fmeasures+=[ret[3]]\n",
    "print(accs)\n",
    "print(precisions)\n",
    "print(recalls)\n",
    "print(fmeasures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddb57c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "accs:  [0.7241379022598267, 0.8333333134651184, 0.699999988079071, 0.8333333134651184, 0.800000011920929, 0.75, 0.931034505367279, 0.9230769276618958, 0.7096773982048035, 0.6000000238418579]\n",
      "precisions:  [0.871264367816092, 0.8888888888888888, 0.7625, 0.8928571428571428, 0.8, 0.8472222222222222, 0.9373040752351096, 0.9311740890688258, 0.815994623655914, 0.84]\n",
      "recalls:  [0.7241379310344828, 0.8333333333333334, 0.7, 0.8333333333333334, 0.8, 0.75, 0.9310344827586207, 0.9230769230769231, 0.7096774193548387, 0.6]\n",
      "fmeasures:  [0.7436433298502263, 0.8380952380952381, 0.7123384253819036, 0.8401880141010576, 0.8, 0.747765006385696, 0.9283661740558292, 0.9204059829059829, 0.7278023519257741, 0.6107142857142858]\n"
     ]
    }
   ],
   "source": [
    "print(len(accs))\n",
    "print('accs: ', accs)\n",
    "print('precisions: ', precisions)\n",
    "print('recalls: ', recalls)\n",
    "print('fmeasures: ', fmeasures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68080bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7061421141092344, 0.8547765627439455)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "#define sample data\n",
    "data = accs\n",
    "\n",
    "#create 95% confidence interval for population mean weight\n",
    "st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b031931d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee4640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e591f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs=  [0.6153846383094788, 0.8571428656578064, 0.6428571343421936, 0.692307710647583, 0.6666666865348816, 0.6666666865348816, 0.5, 1.0, 0.7142857313156128, 0.6153846383094788]\n",
    "precisions=  [0.8557692307692307, 0.9142857142857144, 0.8660714285714286, 1.0, 0.875, 0.8148148148148149, 0.888888888888889, 1.0, 0.8571428571428571, 0.8901098901098901]\n",
    "recalls=  [0.6153846153846154, 0.8571428571428571, 0.6428571428571429, 0.6923076923076923, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.7142857142857143, 0.6153846153846154]\n",
    "fmeasures=  [0.6386946386946387, 0.8678571428571429, 0.6715049656226126, 0.8181818181818181, 0.6985645933014354, 0.6476190476190476, 0.5561497326203209, 1.0, 0.7261904761904762, 0.6656611362493715]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e520e6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7061421141092344, 0.8547765627439455)\n",
      "(0.818343629247924, 0.899097452700915)\n",
      "(0.7061421194747853, 0.8547765651035212)\n",
      "(0.716612808769828, 0.857250952913371)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "#define sample data\n",
    "data = accs\n",
    "\n",
    "#create 95% confidence interval for population mean weight\n",
    "print(st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)))\n",
    "\n",
    "data = precisions\n",
    "\n",
    "#create 95% confidence interval for population mean weight\n",
    "print(st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)))\n",
    "\n",
    "data = recalls\n",
    "\n",
    "#create 95% confidence interval for population mean weight\n",
    "print(st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)))\n",
    "\n",
    "data = fmeasures\n",
    "\n",
    "#create 95% confidence interval for population mean weight\n",
    "print(st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9536615e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
