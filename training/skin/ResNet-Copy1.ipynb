{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a070c61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all imported\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "# calculates f1 for 1:100 dataset with 95tp, 5fn, 55fp\n",
    "from sklearn.metrics import f1_score\n",
    "print('all imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0419f3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[142 112  87]\n",
      "   [142 112  87]\n",
      "   [142 112  87]]\n",
      "\n",
      "  [[157 121  91]\n",
      "   [157 121  91]\n",
      "   [157 121  91]]\n",
      "\n",
      "  [[168 130  98]\n",
      "   [168 130  98]\n",
      "   [168 130  98]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[174 139 113]\n",
      "   [174 139 113]\n",
      "   [174 139 113]]\n",
      "\n",
      "  [[169 135 110]\n",
      "   [169 135 110]\n",
      "   [169 135 110]]\n",
      "\n",
      "  [[159 130 108]\n",
      "   [159 130 108]\n",
      "   [159 130 108]]]\n",
      "\n",
      "\n",
      " [[[148 115  89]\n",
      "   [148 115  89]\n",
      "   [148 115  89]]\n",
      "\n",
      "  [[161 123  95]\n",
      "   [161 123  95]\n",
      "   [161 123  95]]\n",
      "\n",
      "  [[170 133 103]\n",
      "   [170 133 103]\n",
      "   [170 133 103]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[175 139 114]\n",
      "   [175 139 114]\n",
      "   [175 139 114]]\n",
      "\n",
      "  [[171 136 112]\n",
      "   [171 136 112]\n",
      "   [171 136 112]]\n",
      "\n",
      "  [[164 132 111]\n",
      "   [164 132 111]\n",
      "   [164 132 111]]]\n",
      "\n",
      "\n",
      " [[[155 120  95]\n",
      "   [155 120  95]\n",
      "   [155 120  95]]\n",
      "\n",
      "  [[165 129 102]\n",
      "   [165 129 102]\n",
      "   [165 129 102]]\n",
      "\n",
      "  [[170 133 103]\n",
      "   [170 133 103]\n",
      "   [170 133 103]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[178 144 120]\n",
      "   [178 144 120]\n",
      "   [178 144 120]]\n",
      "\n",
      "  [[173 140 116]\n",
      "   [173 140 116]\n",
      "   [173 140 116]]\n",
      "\n",
      "  [[168 136 115]\n",
      "   [168 136 115]\n",
      "   [168 136 115]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[154 129 114]\n",
      "   [154 129 114]\n",
      "   [154 129 114]]\n",
      "\n",
      "  [[161 132 118]\n",
      "   [161 132 118]\n",
      "   [161 132 118]]\n",
      "\n",
      "  [[173 146 128]\n",
      "   [173 146 128]\n",
      "   [173 146 128]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[177 153 140]\n",
      "   [177 153 140]\n",
      "   [177 153 140]]\n",
      "\n",
      "  [[172 148 134]\n",
      "   [172 148 134]\n",
      "   [172 148 134]]\n",
      "\n",
      "  [[163 138 123]\n",
      "   [163 138 123]\n",
      "   [163 138 123]]]\n",
      "\n",
      "\n",
      " [[[153 130 115]\n",
      "   [153 130 115]\n",
      "   [153 130 115]]\n",
      "\n",
      "  [[160 133 117]\n",
      "   [160 133 117]\n",
      "   [160 133 117]]\n",
      "\n",
      "  [[172 145 128]\n",
      "   [172 145 128]\n",
      "   [172 145 128]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[173 149 135]\n",
      "   [173 149 135]\n",
      "   [173 149 135]]\n",
      "\n",
      "  [[168 145 131]\n",
      "   [168 145 131]\n",
      "   [168 145 131]]\n",
      "\n",
      "  [[158 135 119]\n",
      "   [158 135 119]\n",
      "   [158 135 119]]]\n",
      "\n",
      "\n",
      " [[[153 130 114]\n",
      "   [153 130 114]\n",
      "   [153 130 114]]\n",
      "\n",
      "  [[159 132 116]\n",
      "   [159 132 116]\n",
      "   [159 132 116]]\n",
      "\n",
      "  [[170 144 127]\n",
      "   [170 144 127]\n",
      "   [170 144 127]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[169 145 131]\n",
      "   [169 145 131]\n",
      "   [169 145 131]]\n",
      "\n",
      "  [[166 142 129]\n",
      "   [166 142 129]\n",
      "   [166 142 129]]\n",
      "\n",
      "  [[154 132 117]\n",
      "   [154 132 117]\n",
      "   [154 132 117]]]]\n"
     ]
    }
   ],
   "source": [
    "def readImage(filePath):\n",
    "    img = Image.open(filePath)\n",
    "    \n",
    "    size=(30,40)\n",
    "    #resize image\n",
    "    out = img.resize(size)\n",
    "\n",
    "    # asarray() class is used to convert\n",
    "    # PIL images into NumPy arrays\n",
    "    numpydata = asarray(out)\n",
    "    numpydata = np.repeat(numpydata[:, :, np.newaxis], 3, axis=2)\n",
    "    # <class 'numpy.ndarray'>\n",
    "    #print(type(numpydata))\n",
    "\n",
    "    #  shape\n",
    "    #print(numpydata.shape)\n",
    "    return numpydata\n",
    "print(readImage('data/nervus/ISIC_0001769.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8d30f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ISIC_0012099.jpg', 'ISIC_0012151.jpg', 'ISIC_0012288.jpg', 'ISIC_0012434.jpg', 'ISIC_0013232.jpg']\n"
     ]
    }
   ],
   "source": [
    "listFiles = os.listdir('data/melanoma/')\n",
    "print(listFiles[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1538036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[[ 54,  55,  58],\n",
      "         [ 54,  55,  58],\n",
      "         [ 54,  55,  58]],\n",
      "\n",
      "        [[168, 168, 169],\n",
      "         [168, 168, 169],\n",
      "         [168, 168, 169]],\n",
      "\n",
      "        [[184, 184, 185],\n",
      "         [184, 184, 185],\n",
      "         [184, 184, 185]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[190, 180, 183],\n",
      "         [190, 180, 183],\n",
      "         [190, 180, 183]],\n",
      "\n",
      "        [[172, 164, 169],\n",
      "         [172, 164, 169],\n",
      "         [172, 164, 169]],\n",
      "\n",
      "        [[ 66,  72,  89],\n",
      "         [ 66,  72,  89],\n",
      "         [ 66,  72,  89]]],\n",
      "\n",
      "\n",
      "       [[[ 99,  99, 102],\n",
      "         [ 99,  99, 102],\n",
      "         [ 99,  99, 102]],\n",
      "\n",
      "        [[179, 178, 179],\n",
      "         [179, 178, 179],\n",
      "         [179, 178, 179]],\n",
      "\n",
      "        [[185, 183, 184],\n",
      "         [185, 183, 184],\n",
      "         [185, 183, 184]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[192, 180, 184],\n",
      "         [192, 180, 184],\n",
      "         [192, 180, 184]],\n",
      "\n",
      "        [[184, 172, 175],\n",
      "         [184, 172, 175],\n",
      "         [184, 172, 175]],\n",
      "\n",
      "        [[100,  97, 108],\n",
      "         [100,  97, 108],\n",
      "         [100,  97, 108]]],\n",
      "\n",
      "\n",
      "       [[[149, 148, 152],\n",
      "         [149, 148, 152],\n",
      "         [149, 148, 152]],\n",
      "\n",
      "        [[184, 184, 184],\n",
      "         [184, 184, 184],\n",
      "         [184, 184, 184]],\n",
      "\n",
      "        [[188, 187, 187],\n",
      "         [188, 187, 187],\n",
      "         [188, 187, 187]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[193, 182, 186],\n",
      "         [193, 182, 186],\n",
      "         [193, 182, 186]],\n",
      "\n",
      "        [[189, 180, 183],\n",
      "         [189, 180, 183],\n",
      "         [189, 180, 183]],\n",
      "\n",
      "        [[147, 141, 147],\n",
      "         [147, 141, 147],\n",
      "         [147, 141, 147]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[180, 179, 183],\n",
      "         [180, 179, 183],\n",
      "         [180, 179, 183]],\n",
      "\n",
      "        [[191, 186, 187],\n",
      "         [191, 186, 187],\n",
      "         [191, 186, 187]],\n",
      "\n",
      "        [[195, 189, 190],\n",
      "         [195, 189, 190],\n",
      "         [195, 189, 190]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[190, 178, 181],\n",
      "         [190, 178, 181],\n",
      "         [190, 178, 181]],\n",
      "\n",
      "        [[184, 171, 175],\n",
      "         [184, 171, 175],\n",
      "         [184, 171, 175]],\n",
      "\n",
      "        [[159, 148, 153],\n",
      "         [159, 148, 153],\n",
      "         [159, 148, 153]]],\n",
      "\n",
      "\n",
      "       [[[166, 172, 181],\n",
      "         [166, 172, 181],\n",
      "         [166, 172, 181]],\n",
      "\n",
      "        [[189, 185, 186],\n",
      "         [189, 185, 186],\n",
      "         [189, 185, 186]],\n",
      "\n",
      "        [[193, 189, 189],\n",
      "         [193, 189, 189],\n",
      "         [193, 189, 189]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[188, 176, 179],\n",
      "         [188, 176, 179],\n",
      "         [188, 176, 179]],\n",
      "\n",
      "        [[183, 172, 176],\n",
      "         [183, 172, 176],\n",
      "         [183, 172, 176]],\n",
      "\n",
      "        [[113, 106, 110],\n",
      "         [113, 106, 110],\n",
      "         [113, 106, 110]]],\n",
      "\n",
      "\n",
      "       [[[138, 155, 171],\n",
      "         [138, 155, 171],\n",
      "         [138, 155, 171]],\n",
      "\n",
      "        [[185, 182, 185],\n",
      "         [185, 182, 185],\n",
      "         [185, 182, 185]],\n",
      "\n",
      "        [[192, 191, 192],\n",
      "         [192, 191, 192],\n",
      "         [192, 191, 192]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[187, 176, 180],\n",
      "         [187, 176, 180],\n",
      "         [187, 176, 180]],\n",
      "\n",
      "        [[177, 167, 171],\n",
      "         [177, 167, 171],\n",
      "         [177, 167, 171]],\n",
      "\n",
      "        [[ 64,  61,  65],\n",
      "         [ 64,  61,  65],\n",
      "         [ 64,  61,  65]]]], dtype=uint8), array([[[[156, 130, 112],\n",
      "         [156, 130, 112],\n",
      "         [156, 130, 112]],\n",
      "\n",
      "        [[155, 132, 114],\n",
      "         [155, 132, 114],\n",
      "         [155, 132, 114]],\n",
      "\n",
      "        [[162, 144, 131],\n",
      "         [162, 144, 131],\n",
      "         [162, 144, 131]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[173, 152, 138],\n",
      "         [173, 152, 138],\n",
      "         [173, 152, 138]],\n",
      "\n",
      "        [[177, 161, 153],\n",
      "         [177, 161, 153],\n",
      "         [177, 161, 153]],\n",
      "\n",
      "        [[188, 176, 178],\n",
      "         [188, 176, 178],\n",
      "         [188, 176, 178]]],\n",
      "\n",
      "\n",
      "       [[[154, 127, 107],\n",
      "         [154, 127, 107],\n",
      "         [154, 127, 107]],\n",
      "\n",
      "        [[158, 136, 121],\n",
      "         [158, 136, 121],\n",
      "         [158, 136, 121]],\n",
      "\n",
      "        [[162, 142, 126],\n",
      "         [162, 142, 126],\n",
      "         [162, 142, 126]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[174, 153, 142],\n",
      "         [174, 153, 142],\n",
      "         [174, 153, 142]],\n",
      "\n",
      "        [[175, 153, 140],\n",
      "         [175, 153, 140],\n",
      "         [175, 153, 140]],\n",
      "\n",
      "        [[189, 177, 179],\n",
      "         [189, 177, 179],\n",
      "         [189, 177, 179]]],\n",
      "\n",
      "\n",
      "       [[[145, 112,  88],\n",
      "         [145, 112,  88],\n",
      "         [145, 112,  88]],\n",
      "\n",
      "        [[161, 142, 129],\n",
      "         [161, 142, 129],\n",
      "         [161, 142, 129]],\n",
      "\n",
      "        [[158, 132, 110],\n",
      "         [158, 132, 110],\n",
      "         [158, 132, 110]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[175, 155, 147],\n",
      "         [175, 155, 147],\n",
      "         [175, 155, 147]],\n",
      "\n",
      "        [[168, 139, 119],\n",
      "         [168, 139, 119],\n",
      "         [168, 139, 119]],\n",
      "\n",
      "        [[181, 162, 155],\n",
      "         [181, 162, 155],\n",
      "         [181, 162, 155]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[199, 191, 190],\n",
      "         [199, 191, 190],\n",
      "         [199, 191, 190]],\n",
      "\n",
      "        [[191, 179, 172],\n",
      "         [191, 179, 172],\n",
      "         [191, 179, 172]],\n",
      "\n",
      "        [[173, 149, 129],\n",
      "         [173, 149, 129],\n",
      "         [173, 149, 129]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[181, 160, 145],\n",
      "         [181, 160, 145],\n",
      "         [181, 160, 145]],\n",
      "\n",
      "        [[181, 164, 150],\n",
      "         [181, 164, 150],\n",
      "         [181, 164, 150]],\n",
      "\n",
      "        [[177, 158, 146],\n",
      "         [177, 158, 146],\n",
      "         [177, 158, 146]]],\n",
      "\n",
      "\n",
      "       [[[192, 179, 175],\n",
      "         [192, 179, 175],\n",
      "         [192, 179, 175]],\n",
      "\n",
      "        [[197, 189, 186],\n",
      "         [197, 189, 186],\n",
      "         [197, 189, 186]],\n",
      "\n",
      "        [[173, 149, 127],\n",
      "         [173, 149, 127],\n",
      "         [173, 149, 127]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[180, 160, 144],\n",
      "         [180, 160, 144],\n",
      "         [180, 160, 144]],\n",
      "\n",
      "        [[179, 160, 144],\n",
      "         [179, 160, 144],\n",
      "         [179, 160, 144]],\n",
      "\n",
      "        [[172, 154, 140],\n",
      "         [172, 154, 140],\n",
      "         [172, 154, 140]]],\n",
      "\n",
      "\n",
      "       [[[186, 168, 164],\n",
      "         [186, 168, 164],\n",
      "         [186, 168, 164]],\n",
      "\n",
      "        [[201, 194, 194],\n",
      "         [201, 194, 194],\n",
      "         [201, 194, 194]],\n",
      "\n",
      "        [[180, 161, 143],\n",
      "         [180, 161, 143],\n",
      "         [180, 161, 143]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[178, 155, 137],\n",
      "         [178, 155, 137],\n",
      "         [178, 155, 137]],\n",
      "\n",
      "        [[178, 159, 144],\n",
      "         [178, 159, 144],\n",
      "         [178, 159, 144]],\n",
      "\n",
      "        [[168, 149, 136],\n",
      "         [168, 149, 136],\n",
      "         [168, 149, 136]]]], dtype=uint8), array([[[[159, 141, 136],\n",
      "         [159, 141, 136],\n",
      "         [159, 141, 136]],\n",
      "\n",
      "        [[182, 148, 136],\n",
      "         [182, 148, 136],\n",
      "         [182, 148, 136]],\n",
      "\n",
      "        [[175, 143, 133],\n",
      "         [175, 143, 133],\n",
      "         [175, 143, 133]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[186, 150, 136],\n",
      "         [186, 150, 136],\n",
      "         [186, 150, 136]],\n",
      "\n",
      "        [[181, 148, 137],\n",
      "         [181, 148, 137],\n",
      "         [181, 148, 137]],\n",
      "\n",
      "        [[117,  96,  91],\n",
      "         [117,  96,  91],\n",
      "         [117,  96,  91]]],\n",
      "\n",
      "\n",
      "       [[[176, 147, 138],\n",
      "         [176, 147, 138],\n",
      "         [176, 147, 138]],\n",
      "\n",
      "        [[185, 152, 141],\n",
      "         [185, 152, 141],\n",
      "         [185, 152, 141]],\n",
      "\n",
      "        [[176, 143, 135],\n",
      "         [176, 143, 135],\n",
      "         [176, 143, 135]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[188, 152, 138],\n",
      "         [188, 152, 138],\n",
      "         [188, 152, 138]],\n",
      "\n",
      "        [[182, 148, 136],\n",
      "         [182, 148, 136],\n",
      "         [182, 148, 136]],\n",
      "\n",
      "        [[158, 129, 122],\n",
      "         [158, 129, 122],\n",
      "         [158, 129, 122]]],\n",
      "\n",
      "\n",
      "       [[[183, 155, 146],\n",
      "         [183, 155, 146],\n",
      "         [183, 155, 146]],\n",
      "\n",
      "        [[182, 153, 144],\n",
      "         [182, 153, 144],\n",
      "         [182, 153, 144]],\n",
      "\n",
      "        [[178, 145, 135],\n",
      "         [178, 145, 135],\n",
      "         [178, 145, 135]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[190, 153, 138],\n",
      "         [190, 153, 138],\n",
      "         [190, 153, 138]],\n",
      "\n",
      "        [[183, 147, 132],\n",
      "         [183, 147, 132],\n",
      "         [183, 147, 132]],\n",
      "\n",
      "        [[174, 145, 138],\n",
      "         [174, 145, 138],\n",
      "         [174, 145, 138]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[180, 151, 143],\n",
      "         [180, 151, 143],\n",
      "         [180, 151, 143]],\n",
      "\n",
      "        [[178, 143, 129],\n",
      "         [178, 143, 129],\n",
      "         [178, 143, 129]],\n",
      "\n",
      "        [[189, 153, 141],\n",
      "         [189, 153, 141],\n",
      "         [189, 153, 141]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[192, 147, 126],\n",
      "         [192, 147, 126],\n",
      "         [192, 147, 126]],\n",
      "\n",
      "        [[187, 145, 127],\n",
      "         [187, 145, 127],\n",
      "         [187, 145, 127]],\n",
      "\n",
      "        [[178, 144, 131],\n",
      "         [178, 144, 131],\n",
      "         [178, 144, 131]]],\n",
      "\n",
      "\n",
      "       [[[174, 145, 139],\n",
      "         [174, 145, 139],\n",
      "         [174, 145, 139]],\n",
      "\n",
      "        [[180, 149, 139],\n",
      "         [180, 149, 139],\n",
      "         [180, 149, 139]],\n",
      "\n",
      "        [[188, 152, 139],\n",
      "         [188, 152, 139],\n",
      "         [188, 152, 139]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[190, 145, 126],\n",
      "         [190, 145, 126],\n",
      "         [190, 145, 126]],\n",
      "\n",
      "        [[185, 148, 133],\n",
      "         [185, 148, 133],\n",
      "         [185, 148, 133]],\n",
      "\n",
      "        [[169, 133, 122],\n",
      "         [169, 133, 122],\n",
      "         [169, 133, 122]]],\n",
      "\n",
      "\n",
      "       [[[168, 140, 134],\n",
      "         [168, 140, 134],\n",
      "         [168, 140, 134]],\n",
      "\n",
      "        [[178, 147, 137],\n",
      "         [178, 147, 137],\n",
      "         [178, 147, 137]],\n",
      "\n",
      "        [[184, 145, 131],\n",
      "         [184, 145, 131],\n",
      "         [184, 145, 131]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[190, 150, 136],\n",
      "         [190, 150, 136],\n",
      "         [190, 150, 136]],\n",
      "\n",
      "        [[185, 152, 141],\n",
      "         [185, 152, 141],\n",
      "         [185, 152, 141]],\n",
      "\n",
      "        [[161, 125, 113],\n",
      "         [161, 125, 113],\n",
      "         [161, 125, 113]]]], dtype=uint8)]\n",
      "(40, 30, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "#load Benign\n",
    "listCancer = []\n",
    "listResults = []\n",
    "for elem in listFiles:\n",
    "    img = readImage('data/melanoma/'+elem)\n",
    "    listCancer += [img]\n",
    "    listResults += [np.array([1])]\n",
    "print(listCancer[:3])\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07997934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ISIC_0001769.jpg', 'ISIC_0001852.jpg', 'ISIC_0001871.jpg', 'ISIC_0003462.jpg', 'ISIC_0003539.jpg']\n"
     ]
    }
   ],
   "source": [
    "listFiles = os.listdir('data/nervus/')\n",
    "print(listFiles[:5])\n",
    "for elem in listFiles:\n",
    "    img = readImage('data/nervus/'+elem)\n",
    "    listResults += [np.array([0])]\n",
    "    listCancer += [img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea1a0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ISIC_0012143.jpg', 'ISIC_0012204.jpg', 'ISIC_0012210.jpg', 'ISIC_0012254.jpg', 'ISIC_0012380.jpg']\n"
     ]
    }
   ],
   "source": [
    "listFiles = os.listdir('data/seborrheic_keratosis/')\n",
    "print(listFiles[:5])\n",
    "for elem in listFiles:\n",
    "    img = readImage('data/seborrheic_keratosis/'+elem)\n",
    "    listResults += [np.array([2])]\n",
    "    listCancer += [img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36904b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print(len(listCancer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60e9785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting LR for different number of Epochs\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    lr *= float(epoch) / 100\n",
    "    print('Learning rate: ', epoch / 300)\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c652817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic ResNet Building Block\n",
    "def resnet_layer(inputs, conv_first = False,\n",
    "\t\t\t\tnum_filters = 16,\n",
    "\t\t\t\tkernel_size = 3,\n",
    "\t\t\t\tstrides = 1,\n",
    "\t\t\t\tactivation ='relu',\n",
    "\t\t\t\tbatch_normalization = True):\n",
    "\tconv = Conv2D(num_filters,\n",
    "\t\t\t\tkernel_size = kernel_size,\n",
    "\t\t\t\tstrides = strides,\n",
    "\t\t\t\tpadding ='same',\n",
    "\t\t\t\tkernel_initializer ='he_normal',\n",
    "\t\t\t\tkernel_regularizer = l2(1e-4))\n",
    "\n",
    "\tx = inputs\n",
    "\tif conv_first:\n",
    "\t\tx = conv(x)\n",
    "\t\tif batch_normalization:\n",
    "\t\t\tx = BatchNormalization()(x)\n",
    "\t\tif activation is not None:\n",
    "\t\t\tx = Activation(activation)(x)\n",
    "\telse:\n",
    "\t\tif batch_normalization:\n",
    "\t\t\tx = BatchNormalization()(x)\n",
    "\t\tif activation is not None:\n",
    "\t\t\tx = Activation(activation)(x)\n",
    "\t\tx = conv(x)\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9463dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet V2 architecture\n",
    "def resnet_v2(input_shape, depth, num_classes = 10):\n",
    "\tif (depth - 2) % 9 != 0:\n",
    "\t\traise ValueError('depth should be 9n + 2 (eg 56 or 110 in [b])')\n",
    "\t# Start model definition.\n",
    "\tnum_filters_in = 16\n",
    "\tnum_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "\tinputs = Input(shape = input_shape)\n",
    "\t# v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "\tx = resnet_layer(inputs = inputs,num_filters = num_filters_in,conv_first = True)\n",
    "\n",
    "\t# Instantiate the stack of residual units\n",
    "\tfor stage in range(3):\n",
    "\t\tfor res_block in range(num_res_blocks):\n",
    "\t\t\tactivation = 'relu'\n",
    "\t\t\tbatch_normalization = True\n",
    "\t\t\tstrides = 1\n",
    "\t\t\tif stage == 0:\n",
    "\t\t\t\tnum_filters_out = num_filters_in * 4\n",
    "\t\t\t\tif res_block == 0: # first layer and first stage\n",
    "\t\t\t\t\tactivation = None\n",
    "\t\t\t\t\tbatch_normalization = False\n",
    "\t\t\telse:\n",
    "\t\t\t\tnum_filters_out = num_filters_in * 2\n",
    "\t\t\t\tif res_block == 0: # first layer but not first stage\n",
    "\t\t\t\t\tstrides = 2 # downsample\n",
    "\n",
    "\t\t\t# bottleneck residual unit\n",
    "\t\t\ty = resnet_layer(inputs = x,\n",
    "\t\t\t\t\t\t\tnum_filters = num_filters_in,\n",
    "\t\t\t\t\t\t\tkernel_size = 1,\n",
    "\t\t\t\t\t\t\tstrides = strides,\n",
    "\t\t\t\t\t\t\tactivation = activation,\n",
    "\t\t\t\t\t\t\tbatch_normalization = batch_normalization)\n",
    "\t\t\ty = resnet_layer(inputs = y,\n",
    "\t\t\t\t\t\t\tnum_filters = num_filters_in,\n",
    "\t\t\t\t\t\t\tconv_first = False)\n",
    "\t\t\ty = resnet_layer(inputs = y,\n",
    "\t\t\t\t\t\t\tnum_filters = num_filters_out,\n",
    "\t\t\t\t\t\t\tkernel_size = 1,\n",
    "\t\t\t\t\t\t\tconv_first = False)\n",
    "\t\t\tif res_block == 0:\n",
    "\t\t\t\t# linear projection residual shortcut connection to match\n",
    "\t\t\t\t# changed dims\n",
    "\t\t\t\tx = resnet_layer(inputs = x,\n",
    "\t\t\t\t\t\t\t\tnum_filters = num_filters_out,\n",
    "\t\t\t\t\t\t\t\tkernel_size = 1,\n",
    "\t\t\t\t\t\t\t\tstrides = strides,\n",
    "\t\t\t\t\t\t\t\tactivation = None,\n",
    "\t\t\t\t\t\t\t\tbatch_normalization = False)\n",
    "\t\t\tx = keras.layers.add([x, y])\n",
    "\n",
    "\t\tnum_filters_in = num_filters_out\n",
    "\n",
    "\t# Add classifier on top.\n",
    "\t# v2 has BN-ReLU before Pooling\n",
    "\tx = BatchNormalization()(x)\n",
    "\tx = Activation('relu')(x)\n",
    "\tx = AveragePooling2D(pool_size = 8)(x)\n",
    "\ty = Flatten()(x)\n",
    "\toutputs = Dense(num_classes,\n",
    "\t\t\t\t\tactivation ='softmax',\n",
    "\t\t\t\t\tkernel_initializer ='he_normal')(y)\n",
    "\n",
    "\t# Instantiate model.\n",
    "\tmodel = Model(inputs = inputs, outputs = outputs)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb24867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=[]\n",
    "y_train=[]\n",
    "accs=[]\n",
    "def main(pz):\n",
    "    \n",
    "    per = np.random.permutation(len(listCancer))\n",
    "    ln = int(len(listCancer) * 0.6)\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    positions = []\n",
    "    #for i in range(ln):\n",
    "    #    positions += [per[i]]\n",
    "    val = 1\n",
    "    while ln > 0:\n",
    "        val = 1+val\n",
    "        if val == 3:\n",
    "            val=0\n",
    "        ln-=1\n",
    "        for i in range(len(per)):\n",
    "            if per[i] in positions:\n",
    "                continue\n",
    "            if listResults[per[i]] == np.array([val]):\n",
    "                positions += [per[i]]\n",
    "                break\n",
    "    for i in range(len(listCancer)):\n",
    "        if i in positions:\n",
    "            x_train += [listCancer[i]]\n",
    "            y_train += [listResults[i]]\n",
    "        else:\n",
    "            x_test += [listCancer[i]]\n",
    "            y_test += [listResults[i]]\n",
    "            \n",
    "    ln = len(x_test) // 2\n",
    "    \n",
    "    x_valid = x_test[ln:]\n",
    "    y_valid = y_test[ln:]\n",
    "    \n",
    "    x_test = x_test[:ln]\n",
    "    y_test = y_test[:ln]\n",
    "    \n",
    "    \n",
    "    x_train = np.array(x_train)\n",
    "    x_train = x_train.reshape(len(x_train), 40, 30, 9)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "\n",
    "    x_test = np.array(x_test)\n",
    "    x_test = x_test.reshape(len(x_test), 40, 30, 9)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    x_valid = np.array(x_valid)\n",
    "    x_valid = x_valid.reshape(len(x_valid), 40, 30, 9)\n",
    "    y_valid = np.array(y_valid)\n",
    "    \n",
    "    \n",
    "    batch_size = 32 # original ResNet paper uses batch_size = 128 for training\n",
    "    epochs = 10\n",
    "    data_augmentation = True\n",
    "    num_classes = 10\n",
    "\n",
    "    # Data Preprocessing\n",
    "    subtract_pixel_mean = True\n",
    "    n = 3\n",
    "\n",
    "    # Select ResNet Version\n",
    "    version = 2\n",
    "\n",
    "    # Computed depth of\n",
    "    if version == 1:\n",
    "        depth = n * 6 + 2\n",
    "    elif version == 2:\n",
    "        depth = n * 9 + 2\n",
    "\n",
    "    # Model name, depth and version\n",
    "    model_type = 'ResNet % dv % d' % (depth, version)\n",
    "\n",
    "    # use the data\n",
    "    print(x_train[:3])\n",
    "    print(y_train[:3])\n",
    "    print(type(x_train[0]))\n",
    "    print(type(x_train))\n",
    "    print(type(y_train[0]))\n",
    "    print(type(y_train))\n",
    "\n",
    "    # Input image dimensions.\n",
    "    input_shape = x_train.shape[1:]\n",
    "    print(input_shape)\n",
    "\n",
    "    # Normalize data.\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "    x_valid = x_test.astype('float32') / 255\n",
    "\n",
    "    # If subtract pixel mean is enabled\n",
    "    if subtract_pixel_mean:\n",
    "        x_train_mean = np.mean(x_train, axis = 0)\n",
    "        x_train -= x_train_mean\n",
    "        x_test -= x_train_mean\n",
    "\n",
    "    # Print Training and Test Samples\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    print('y_train shape:', y_train.shape)\n",
    "\n",
    "    # Convert class vectors to binary class matrices.\n",
    "    y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
    "    y_valid = keras.utils.np_utils.to_categorical(y_valid, num_classes)\n",
    "    \n",
    "    \n",
    "    model = resnet_v2(input_shape = input_shape, depth = depth)\n",
    "\n",
    "    model.compile(loss ='categorical_crossentropy',\n",
    "                optimizer = Adam(learning_rate = lr_schedule(0)),\n",
    "                metrics =['accuracy'])\n",
    "    model.summary()\n",
    "    print(model_type)\n",
    "\n",
    "    # Prepare model model saving directory.\n",
    "    save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "    model_name = 'cifar10_% s_model.{epoch:03d}.h5' % model_type\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "    # Prepare callbacks for model saving and for learning rate adjustment.\n",
    "    checkpoint = ModelCheckpoint(filepath = filepath,\n",
    "                                monitor ='val_acc',\n",
    "                                verbose = 1,\n",
    "                                save_best_only = True)\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "    lr_reducer = ReduceLROnPlateau(factor = np.sqrt(0.1),\n",
    "                                cooldown = 0,\n",
    "                                patience = 5,\n",
    "                                min_lr = 0.5e-6)\n",
    "\n",
    "    callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "\n",
    "    # Run training, with or without data augmentation.\n",
    "    if not data_augmentation:\n",
    "        print('Not using data augmentation.')\n",
    "        model.fit(x_train, y_train,\n",
    "                batch_size = batch_size,\n",
    "                epochs = epochs,\n",
    "                validation_data =(x_test, y_test),\n",
    "                shuffle = True,\n",
    "                callbacks = callbacks)\n",
    "    else:\n",
    "        print('Using real-time data augmentation.')\n",
    "        # This will do preprocessing and realtime data augmentation:\n",
    "        datagen = ImageDataGenerator(\n",
    "            # set input mean to 0 over the dataset\n",
    "            featurewise_center = False,\n",
    "            # set each sample mean to 0\n",
    "            samplewise_center = False,\n",
    "            # divide inputs by std of dataset\n",
    "            featurewise_std_normalization = False,\n",
    "            # divide each input by its std\n",
    "            samplewise_std_normalization = False,\n",
    "            # apply ZCA whitening\n",
    "            zca_whitening = False,\n",
    "            # epsilon for ZCA whitening\n",
    "            zca_epsilon = 1e-06,\n",
    "            # randomly rotate images in the range (deg 0 to 180)\n",
    "            rotation_range = 0,\n",
    "            # randomly shift images horizontally\n",
    "            width_shift_range = 0.1,\n",
    "            # randomly shift images vertically\n",
    "            height_shift_range = 0.1,\n",
    "            # set range for random shear\n",
    "            shear_range = 0.,\n",
    "            # set range for random zoom\n",
    "            zoom_range = 0.,\n",
    "            # set range for random channel shifts\n",
    "            channel_shift_range = 0.,\n",
    "            # set mode for filling points outside the input boundaries\n",
    "            fill_mode ='nearest',\n",
    "            # value used for fill_mode = \"constant\"\n",
    "            cval = 0.,\n",
    "            # randomly flip images\n",
    "            horizontal_flip = True,\n",
    "            # randomly flip images\n",
    "            vertical_flip = False,\n",
    "            # set rescaling factor (applied before any other transformation)\n",
    "            rescale = None,\n",
    "            # set function that will be applied on each input\n",
    "            preprocessing_function = None,\n",
    "            # image data format, either \"channels_first\" or \"channels_last\"\n",
    "            data_format = None,\n",
    "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "            validation_split = 0.1)\n",
    "\n",
    "        # Compute quantities required for featurewise normalization\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "        print('------------------')\n",
    "        print(x_train)\n",
    "        datagen.fit(x_train)\n",
    "\n",
    "        # Fit the model on the batches generated by datagen.flow().\n",
    "        model.fit_generator(datagen.flow(x_train, y_train, batch_size = batch_size),\n",
    "                            validation_data =(x_valid, y_valid),\n",
    "                            epochs = 100, verbose = 1, workers = 4,\n",
    "                            callbacks = callbacks)\n",
    "\n",
    "    # Score trained model.\n",
    "    scores = model.evaluate(x_test, y_test, verbose = 1)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    ret = model.predict(x_test)\n",
    "    result = []\n",
    "    pred = []\n",
    "    for a,b in zip(ret, y_test):\n",
    "        pred+=[a.argmax(axis=-1)]\n",
    "        result+=[b.argmax(axis=-1)]\n",
    "    print(pred)\n",
    "    print(result)\n",
    "    precision = precision_score(result, pred, average='weighted')\n",
    "    recall = recall_score(result, pred, average='weighted')\n",
    "     # calculates f1 for 1:100 dataset with 95tp, 5fn, 55fp\n",
    "    fmeasure = f1_score(result, pred, average='weighted')\n",
    "    print('Precision: ', precision)\n",
    "    print('Recall: ', recall)\n",
    "    print('F1 Score: ', fmeasure)\n",
    "    #precisions += [precision]\n",
    "    #recalls += [recall]\n",
    "    if scores[1] > 0.5:\n",
    "        model.save('saved_models/skinResNetV'+str(pz)+'.h5')\n",
    "    return [scores[1], precision, recall, fmeasure]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31f8fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 54  55  58 ...  54  55  58]\n",
      "   [168 168 169 ... 168 168 169]\n",
      "   [184 184 185 ... 184 184 185]\n",
      "   ...\n",
      "   [190 180 183 ... 190 180 183]\n",
      "   [172 164 169 ... 172 164 169]\n",
      "   [ 66  72  89 ...  66  72  89]]\n",
      "\n",
      "  [[ 99  99 102 ...  99  99 102]\n",
      "   [179 178 179 ... 179 178 179]\n",
      "   [185 183 184 ... 185 183 184]\n",
      "   ...\n",
      "   [192 180 184 ... 192 180 184]\n",
      "   [184 172 175 ... 184 172 175]\n",
      "   [100  97 108 ... 100  97 108]]\n",
      "\n",
      "  [[149 148 152 ... 149 148 152]\n",
      "   [184 184 184 ... 184 184 184]\n",
      "   [188 187 187 ... 188 187 187]\n",
      "   ...\n",
      "   [193 182 186 ... 193 182 186]\n",
      "   [189 180 183 ... 189 180 183]\n",
      "   [147 141 147 ... 147 141 147]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[180 179 183 ... 180 179 183]\n",
      "   [191 186 187 ... 191 186 187]\n",
      "   [195 189 190 ... 195 189 190]\n",
      "   ...\n",
      "   [190 178 181 ... 190 178 181]\n",
      "   [184 171 175 ... 184 171 175]\n",
      "   [159 148 153 ... 159 148 153]]\n",
      "\n",
      "  [[166 172 181 ... 166 172 181]\n",
      "   [189 185 186 ... 189 185 186]\n",
      "   [193 189 189 ... 193 189 189]\n",
      "   ...\n",
      "   [188 176 179 ... 188 176 179]\n",
      "   [183 172 176 ... 183 172 176]\n",
      "   [113 106 110 ... 113 106 110]]\n",
      "\n",
      "  [[138 155 171 ... 138 155 171]\n",
      "   [185 182 185 ... 185 182 185]\n",
      "   [192 191 192 ... 192 191 192]\n",
      "   ...\n",
      "   [187 176 180 ... 187 176 180]\n",
      "   [177 167 171 ... 177 167 171]\n",
      "   [ 64  61  65 ...  64  61  65]]]\n",
      "\n",
      "\n",
      " [[[156 130 112 ... 156 130 112]\n",
      "   [155 132 114 ... 155 132 114]\n",
      "   [162 144 131 ... 162 144 131]\n",
      "   ...\n",
      "   [173 152 138 ... 173 152 138]\n",
      "   [177 161 153 ... 177 161 153]\n",
      "   [188 176 178 ... 188 176 178]]\n",
      "\n",
      "  [[154 127 107 ... 154 127 107]\n",
      "   [158 136 121 ... 158 136 121]\n",
      "   [162 142 126 ... 162 142 126]\n",
      "   ...\n",
      "   [174 153 142 ... 174 153 142]\n",
      "   [175 153 140 ... 175 153 140]\n",
      "   [189 177 179 ... 189 177 179]]\n",
      "\n",
      "  [[145 112  88 ... 145 112  88]\n",
      "   [161 142 129 ... 161 142 129]\n",
      "   [158 132 110 ... 158 132 110]\n",
      "   ...\n",
      "   [175 155 147 ... 175 155 147]\n",
      "   [168 139 119 ... 168 139 119]\n",
      "   [181 162 155 ... 181 162 155]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[199 191 190 ... 199 191 190]\n",
      "   [191 179 172 ... 191 179 172]\n",
      "   [173 149 129 ... 173 149 129]\n",
      "   ...\n",
      "   [181 160 145 ... 181 160 145]\n",
      "   [181 164 150 ... 181 164 150]\n",
      "   [177 158 146 ... 177 158 146]]\n",
      "\n",
      "  [[192 179 175 ... 192 179 175]\n",
      "   [197 189 186 ... 197 189 186]\n",
      "   [173 149 127 ... 173 149 127]\n",
      "   ...\n",
      "   [180 160 144 ... 180 160 144]\n",
      "   [179 160 144 ... 179 160 144]\n",
      "   [172 154 140 ... 172 154 140]]\n",
      "\n",
      "  [[186 168 164 ... 186 168 164]\n",
      "   [201 194 194 ... 201 194 194]\n",
      "   [180 161 143 ... 180 161 143]\n",
      "   ...\n",
      "   [178 155 137 ... 178 155 137]\n",
      "   [178 159 144 ... 178 159 144]\n",
      "   [168 149 136 ... 168 149 136]]]\n",
      "\n",
      "\n",
      " [[[159 141 136 ... 159 141 136]\n",
      "   [182 148 136 ... 182 148 136]\n",
      "   [175 143 133 ... 175 143 133]\n",
      "   ...\n",
      "   [186 150 136 ... 186 150 136]\n",
      "   [181 148 137 ... 181 148 137]\n",
      "   [117  96  91 ... 117  96  91]]\n",
      "\n",
      "  [[176 147 138 ... 176 147 138]\n",
      "   [185 152 141 ... 185 152 141]\n",
      "   [176 143 135 ... 176 143 135]\n",
      "   ...\n",
      "   [188 152 138 ... 188 152 138]\n",
      "   [182 148 136 ... 182 148 136]\n",
      "   [158 129 122 ... 158 129 122]]\n",
      "\n",
      "  [[183 155 146 ... 183 155 146]\n",
      "   [182 153 144 ... 182 153 144]\n",
      "   [178 145 135 ... 178 145 135]\n",
      "   ...\n",
      "   [190 153 138 ... 190 153 138]\n",
      "   [183 147 132 ... 183 147 132]\n",
      "   [174 145 138 ... 174 145 138]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[180 151 143 ... 180 151 143]\n",
      "   [178 143 129 ... 178 143 129]\n",
      "   [189 153 141 ... 189 153 141]\n",
      "   ...\n",
      "   [192 147 126 ... 192 147 126]\n",
      "   [187 145 127 ... 187 145 127]\n",
      "   [178 144 131 ... 178 144 131]]\n",
      "\n",
      "  [[174 145 139 ... 174 145 139]\n",
      "   [180 149 139 ... 180 149 139]\n",
      "   [188 152 139 ... 188 152 139]\n",
      "   ...\n",
      "   [190 145 126 ... 190 145 126]\n",
      "   [185 148 133 ... 185 148 133]\n",
      "   [169 133 122 ... 169 133 122]]\n",
      "\n",
      "  [[168 140 134 ... 168 140 134]\n",
      "   [178 147 137 ... 178 147 137]\n",
      "   [184 145 131 ... 184 145 131]\n",
      "   ...\n",
      "   [190 150 136 ... 190 150 136]\n",
      "   [185 152 141 ... 185 152 141]\n",
      "   [161 125 113 ... 161 125 113]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (90, 40, 30, 9)\n",
      "90 train samples\n",
      "30 test samples\n",
      "y_train shape: (90, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 40, 30, 16)   1312        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 40, 30, 16)   64          conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 40, 30, 16)   0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 40, 30, 16)   272         activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 40, 30, 16)   64          conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 40, 30, 16)   0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 40, 30, 16)   2320        activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 40, 30, 16)   64          conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 40, 30, 16)   0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 40, 30, 64)   1088        activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 40, 30, 64)   1088        activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 40, 30, 64)   0           conv2d_128[0][0]                 \n",
      "                                                                 conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 40, 30, 64)   256         add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 40, 30, 64)   0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 40, 30, 16)   1040        activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 40, 30, 16)   64          conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 40, 30, 16)   0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 40, 30, 16)   2320        activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 40, 30, 16)   64          conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 40, 30, 16)   0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 40, 30, 64)   1088        activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 40, 30, 64)   0           add_36[0][0]                     \n",
      "                                                                 conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 40, 30, 64)   256         add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 40, 30, 64)   0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 40, 30, 16)   1040        activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 40, 30, 16)   64          conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 40, 30, 16)   0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 40, 30, 16)   2320        activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 40, 30, 16)   64          conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 40, 30, 16)   0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 40, 30, 64)   1088        activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 40, 30, 64)   0           add_37[0][0]                     \n",
      "                                                                 conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 40, 30, 64)   256         add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 40, 30, 64)   0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 20, 15, 64)   4160        activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 20, 15, 64)   256         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 20, 15, 64)   0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 20, 15, 64)   36928       activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 20, 15, 64)   256         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 20, 15, 64)   0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 20, 15, 128)  8320        add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 20, 15, 128)  8320        activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 20, 15, 128)  0           conv2d_138[0][0]                 \n",
      "                                                                 conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 20, 15, 128)  512         add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 20, 15, 128)  0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 20, 15, 64)   8256        activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 20, 15, 64)   256         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 20, 15, 64)   0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 20, 15, 64)   36928       activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 20, 15, 64)   256         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 20, 15, 64)   0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 20, 15, 128)  8320        activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 20, 15, 128)  0           add_39[0][0]                     \n",
      "                                                                 conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 20, 15, 128)  512         add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 20, 15, 128)  0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 20, 15, 64)   8256        activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 20, 15, 64)   256         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 20, 15, 64)   0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 20, 15, 64)   36928       activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 20, 15, 64)   256         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 20, 15, 64)   0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 20, 15, 128)  8320        activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 20, 15, 128)  0           add_40[0][0]                     \n",
      "                                                                 conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 20, 15, 128)  512         add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 20, 15, 128)  0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 10, 8, 128)   16512       activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 10, 8, 128)   512         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 10, 8, 128)   0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 10, 8, 128)   147584      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 10, 8, 128)   512         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 10, 8, 128)   0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 10, 8, 256)   33024       add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 10, 8, 256)   33024       activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 10, 8, 256)   0           conv2d_148[0][0]                 \n",
      "                                                                 conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 10, 8, 256)   1024        add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 10, 8, 256)   0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 10, 8, 128)   32896       activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 10, 8, 128)   512         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 10, 8, 128)   0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 10, 8, 128)   147584      activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 10, 8, 128)   512         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 10, 8, 128)   0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 10, 8, 256)   33024       activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 10, 8, 256)   0           add_42[0][0]                     \n",
      "                                                                 conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 10, 8, 256)   1024        add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 10, 8, 256)   0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 10, 8, 128)   32896       activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 10, 8, 128)   512         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 10, 8, 128)   0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 10, 8, 128)   147584      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 10, 8, 128)   512         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 10, 8, 128)   0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 10, 8, 256)   33024       activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 10, 8, 256)   0           add_43[0][0]                     \n",
      "                                                                 conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 10, 8, 256)   1024        add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 10, 8, 256)   0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 1, 1, 256)    0           activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 256)          0           average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10)           2570        flatten_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[-0.25455332 -0.17346387 -0.14671017 ... -0.25455332 -0.17346387\n",
      "    -0.14671017]\n",
      "   [ 0.09276688  0.1957733   0.2245751  ...  0.09276688  0.1957733\n",
      "     0.2245751 ]\n",
      "   [ 0.10165584  0.21952075  0.25503266 ...  0.10165584  0.21952075\n",
      "     0.25503266]\n",
      "   ...\n",
      "   [ 0.11555558  0.18296301  0.23233116 ...  0.11555558  0.18296301\n",
      "     0.23233116]\n",
      "   [ 0.08967322  0.14479297  0.18862754 ...  0.08967322  0.14479297\n",
      "     0.18862754]\n",
      "   [-0.25058833 -0.16636163 -0.09363824 ... -0.25058833 -0.16636163\n",
      "    -0.09363824]]\n",
      "\n",
      "  [[-0.11655781 -0.03124177 -0.0023092  ... -0.11655781 -0.03124177\n",
      "    -0.0023092 ]\n",
      "   [ 0.10640526  0.21285412  0.24287581 ...  0.10640526  0.21285412\n",
      "     0.24287581]\n",
      "   [ 0.09241837  0.20361644  0.23908496 ...  0.09241837  0.20361644\n",
      "     0.23908496]\n",
      "   ...\n",
      "   [ 0.1079303   0.17250544  0.22557738 ...  0.1079303   0.17250544\n",
      "     0.22557738]\n",
      "   [ 0.11294109  0.16283226  0.20435718 ...  0.11294109  0.16283226\n",
      "     0.20435718]\n",
      "   [-0.1499348  -0.09233117 -0.03843132 ... -0.1499348  -0.09233117\n",
      "    -0.03843132]]\n",
      "\n",
      "  [[ 0.0383442   0.128366    0.16409591 ...  0.0383442   0.128366\n",
      "     0.16409591]\n",
      "   [ 0.10897601  0.22413966  0.25267982 ...  0.10897601  0.22413966\n",
      "     0.25267982]\n",
      "   [ 0.09442264  0.21137255  0.24400875 ...  0.09442264  0.21137255\n",
      "     0.24400875]\n",
      "   ...\n",
      "   [ 0.10008717  0.16888887  0.22270155 ...  0.10008717  0.16888887\n",
      "     0.22270155]\n",
      "   [ 0.11355114  0.18366015  0.23150319 ...  0.11355114  0.18366015\n",
      "     0.23150319]\n",
      "   [ 0.00736386  0.06374726  0.10666668 ...  0.00736386  0.06374726\n",
      "     0.10666668]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.11272335  0.18745083  0.22196081 ...  0.11272335  0.18745083\n",
      "     0.22196081]\n",
      "   [ 0.12331152  0.19490206  0.22666675 ...  0.12331152  0.19490206\n",
      "     0.22666675]\n",
      "   [ 0.11446625  0.19054478  0.22692817 ...  0.11446625  0.19054478\n",
      "     0.22692817]\n",
      "   ...\n",
      "   [ 0.09281045  0.15969497  0.21176466 ...  0.09281045  0.15969497\n",
      "     0.21176466]\n",
      "   [ 0.09023964  0.1433115   0.19385624 ...  0.09023964  0.1433115\n",
      "     0.19385624]\n",
      "   [ 0.05429202  0.09969506  0.14357314 ...  0.05429202  0.09969506\n",
      "     0.14357314]]\n",
      "\n",
      "  [[ 0.07738566  0.17311549  0.22405237 ...  0.07738566  0.17311549\n",
      "     0.22405237]\n",
      "   [ 0.12671018  0.19851851  0.23023978 ...  0.12671018  0.19851851\n",
      "     0.23023978]\n",
      "   [ 0.11381269  0.1945535   0.22631818 ...  0.11381269  0.1945535\n",
      "     0.22631818]\n",
      "   ...\n",
      "   [ 0.09538126  0.16021782  0.21185178 ...  0.09538126  0.16021782\n",
      "     0.21185178]\n",
      "   [ 0.10409582  0.16270149  0.21167767 ...  0.10409582  0.16270149\n",
      "     0.21167767]\n",
      "   [-0.09690639 -0.03938997 -0.00074068 ... -0.09690639 -0.03938997\n",
      "    -0.00074068]]\n",
      "\n",
      "  [[-0.00183004  0.1293248   0.20318094 ... -0.00183004  0.1293248\n",
      "     0.20318094]\n",
      "   [ 0.12222213  0.1934641   0.23172122 ...  0.12222213  0.1934641\n",
      "     0.23172122]\n",
      "   [ 0.12156862  0.21250552  0.24984741 ...  0.12156862  0.21250552\n",
      "     0.24984741]\n",
      "   ...\n",
      "   [ 0.10335511  0.16692811  0.22056651 ...  0.10335511  0.16692811\n",
      "     0.22056651]\n",
      "   [ 0.1044445   0.15978211  0.20601317 ...  0.1044445   0.15978211\n",
      "     0.20601317]\n",
      "   [-0.24958602 -0.1847059  -0.14827889 ... -0.24958602 -0.1847059\n",
      "    -0.14827889]]]\n",
      "\n",
      "\n",
      " [[[ 0.14544672  0.12065381  0.06505454 ...  0.14544672  0.12065381\n",
      "     0.06505454]\n",
      "   [ 0.04178649  0.05459684  0.00888881 ...  0.04178649  0.05459684\n",
      "     0.00888881]\n",
      "   [ 0.01538134  0.06265801  0.04326797 ...  0.01538134  0.06265801\n",
      "     0.04326797]\n",
      "   ...\n",
      "   [ 0.04888892  0.0731591   0.05586058 ...  0.04888892  0.0731591\n",
      "     0.05586058]\n",
      "   [ 0.10928106  0.13302827  0.12588245 ...  0.10928106  0.13302827\n",
      "     0.12588245]\n",
      "   [ 0.22784305  0.24148151  0.25538138 ...  0.22784305  0.24148151\n",
      "     0.25538138]]\n",
      "\n",
      "  [[ 0.09912848  0.07856214  0.01729864 ...  0.09912848  0.07856214\n",
      "     0.01729864]\n",
      "   [ 0.02405232  0.04814824  0.01542482 ...  0.02405232  0.04814824\n",
      "     0.01542482]\n",
      "   [ 0.0022223   0.04283214  0.01163396 ...  0.0022223   0.04283214\n",
      "     0.01163396]\n",
      "   ...\n",
      "   [ 0.03734207  0.06662309  0.06087151 ...  0.03734207  0.06662309\n",
      "     0.06087151]\n",
      "   [ 0.07764697  0.08832246  0.06710228 ...  0.07764697  0.08832246\n",
      "     0.06710228]\n",
      "   [ 0.19908482  0.22139433  0.24000007 ...  0.19908482  0.22139433\n",
      "     0.24000007]]\n",
      "\n",
      "  [[ 0.02265793 -0.0128105  -0.0868845  ...  0.02265793 -0.0128105\n",
      "    -0.0868845 ]\n",
      "   [ 0.01877993  0.05943379  0.03699356 ...  0.01877993  0.05943379\n",
      "     0.03699356]\n",
      "   [-0.02322441 -0.00431371 -0.05795205 ... -0.02322441 -0.00431371\n",
      "    -0.05795205]\n",
      "   ...\n",
      "   [ 0.02949893  0.06300652  0.06976038 ...  0.02949893  0.06300652\n",
      "     0.06976038]\n",
      "   [ 0.0311982   0.02287585 -0.01947722 ...  0.0311982   0.02287585\n",
      "    -0.01947722]\n",
      "   [ 0.14069718  0.1461002   0.13803923 ...  0.14069718  0.1461002\n",
      "     0.13803923]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.18723315  0.23450965  0.24941179 ...  0.18723315  0.23450965\n",
      "     0.24941179]\n",
      "   [ 0.12331152  0.16745108  0.16784322 ...  0.12331152  0.16745108\n",
      "     0.16784322]\n",
      "   [ 0.02819175  0.03368205 -0.0122875  ...  0.02819175  0.03368205\n",
      "    -0.0122875 ]\n",
      "   ...\n",
      "   [ 0.05751634  0.08910674  0.0705882  ...  0.05751634  0.08910674\n",
      "     0.0705882 ]\n",
      "   [ 0.07847494  0.11586052  0.09581703 ...  0.07847494  0.11586052\n",
      "     0.09581703]\n",
      "   [ 0.12488025  0.13891074  0.11612216 ...  0.12488025  0.13891074\n",
      "     0.11612216]]\n",
      "\n",
      "  [[ 0.17934644  0.20056647  0.20052296 ...  0.17934644  0.20056647\n",
      "     0.20052296]\n",
      "   [ 0.15808272  0.21420479  0.23023978 ...  0.15808272  0.21420479\n",
      "     0.23023978]\n",
      "   [ 0.03538132  0.03769076 -0.01681909 ...  0.03538132  0.03769076\n",
      "    -0.01681909]\n",
      "   ...\n",
      "   [ 0.06400871  0.09747273  0.07459688 ...  0.06400871  0.09747273\n",
      "     0.07459688]\n",
      "   [ 0.08840954  0.11564267  0.08618748 ...  0.08840954  0.11564267\n",
      "     0.08618748]\n",
      "   [ 0.13446617  0.14884534  0.1169064  ...  0.13446617  0.14884534\n",
      "     0.1169064 ]]\n",
      "\n",
      "  [[ 0.18640524  0.18030518  0.17572996 ...  0.18640524  0.18030518\n",
      "     0.17572996]\n",
      "   [ 0.18496722  0.24052292  0.26701534 ...  0.18496722  0.24052292\n",
      "     0.26701534]\n",
      "   [ 0.0745098   0.09485847  0.05769056 ...  0.0745098   0.09485847\n",
      "     0.05769056]\n",
      "   ...\n",
      "   [ 0.06806099  0.08457518  0.05193907 ...  0.06806099  0.08457518\n",
      "     0.05193907]\n",
      "   [ 0.10836607  0.12840956  0.10013083 ...  0.10836607  0.12840956\n",
      "     0.10013083]\n",
      "   [ 0.15825713  0.16039217  0.1301525  ...  0.15825713  0.16039217\n",
      "     0.1301525 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.15721142  0.16379106  0.1591722  ...  0.15721142  0.16379106\n",
      "     0.1591722 ]\n",
      "   [ 0.14766884  0.11734194  0.09516335 ...  0.14766884  0.11734194\n",
      "     0.09516335]\n",
      "   [ 0.06636173  0.05873644  0.0511111  ...  0.06636173  0.05873644\n",
      "     0.0511111 ]\n",
      "   ...\n",
      "   [ 0.09986931  0.06531596  0.04801744 ...  0.09986931  0.06531596\n",
      "     0.04801744]\n",
      "   [ 0.12496734  0.08204788  0.06313735 ...  0.12496734  0.08204788\n",
      "     0.06313735]\n",
      "   [-0.05058834 -0.07224399 -0.0857951  ... -0.05058834 -0.07224399\n",
      "    -0.0857951 ]]\n",
      "\n",
      "  [[ 0.18540299  0.15699354  0.13886729 ...  0.18540299  0.15699354\n",
      "     0.13886729]\n",
      "   [ 0.12993467  0.11089334  0.09385622 ...  0.12993467  0.11089334\n",
      "     0.09385622]\n",
      "   [ 0.05712426  0.0467537   0.04692811 ...  0.05712426  0.0467537\n",
      "     0.04692811]\n",
      "   ...\n",
      "   [ 0.09224403  0.06270152  0.04518524 ...  0.09224403  0.06270152\n",
      "     0.04518524]\n",
      "   [ 0.10509795  0.06871462  0.05141601 ...  0.10509795  0.06871462\n",
      "     0.05141601]\n",
      "   [ 0.0775162   0.03315905  0.01647064 ...  0.0775162   0.03315905\n",
      "     0.01647064]]\n",
      "\n",
      "  [[ 0.17167753  0.15581697  0.1405665  ...  0.17167753  0.15581697\n",
      "     0.1405665 ]\n",
      "   [ 0.10113287  0.10257104  0.09581709 ...  0.10113287  0.10257104\n",
      "     0.09581709]\n",
      "   [ 0.05520695  0.04666668  0.04008719 ...  0.05520695  0.04666668\n",
      "     0.04008719]\n",
      "   ...\n",
      "   [ 0.08832246  0.05516338  0.03446627 ...  0.08832246  0.05516338\n",
      "     0.03446627]\n",
      "   [ 0.09002173  0.05424839  0.0315032  ...  0.09002173  0.05424839\n",
      "     0.0315032 ]\n",
      "   [ 0.1132462   0.07943353  0.07137257 ...  0.1132462   0.07943353\n",
      "     0.07137257]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.11272335  0.07764691  0.06509808 ...  0.11272335  0.07764691\n",
      "     0.06509808]\n",
      "   [ 0.07233113  0.02627462 -0.00078422 ...  0.07233113  0.02627462\n",
      "    -0.00078422]\n",
      "   [ 0.09093684  0.04936832  0.03477132 ...  0.09093684  0.04936832\n",
      "     0.03477132]\n",
      "   ...\n",
      "   [ 0.10065359  0.03812635 -0.00392163 ...  0.10065359  0.03812635\n",
      "    -0.00392163]\n",
      "   [ 0.10200435  0.04135072  0.00562093 ...  0.10200435  0.04135072\n",
      "     0.00562093]\n",
      "   [ 0.12880182  0.08400878  0.05729863 ...  0.12880182  0.08400878\n",
      "     0.05729863]]\n",
      "\n",
      "  [[ 0.10875821  0.06723315  0.0593465  ...  0.10875821  0.06723315\n",
      "     0.0593465 ]\n",
      "   [ 0.09141606  0.05734205  0.04592606 ...  0.09141606  0.05734205\n",
      "     0.04592606]\n",
      "   [ 0.09420484  0.04945546  0.03023976 ...  0.09420484  0.04945546\n",
      "     0.03023976]\n",
      "   ...\n",
      "   [ 0.1032244   0.0386492   0.00400862 ...  0.1032244   0.0386492\n",
      "     0.00400862]\n",
      "   [ 0.11193895  0.06858385  0.04305023 ...  0.11193895  0.06858385\n",
      "     0.04305023]\n",
      "   [ 0.12270147  0.06649241  0.04631814 ...  0.12270147  0.06649241\n",
      "     0.04631814]]\n",
      "\n",
      "  [[ 0.11581701  0.07050127  0.05808291 ...  0.11581701  0.07050127\n",
      "     0.05808291]\n",
      "   [ 0.09477115  0.05620921  0.04348594 ...  0.09477115  0.05620921\n",
      "     0.04348594]\n",
      "   [ 0.09019607  0.03211337  0.01063174 ...  0.09019607  0.03211337\n",
      "     0.01063174]\n",
      "   ...\n",
      "   [ 0.11511981  0.06496733  0.0480175  ...  0.11511981  0.06496733\n",
      "     0.0480175 ]\n",
      "   [ 0.13581705  0.10095859  0.08836612 ...  0.13581705  0.10095859\n",
      "     0.08836612]\n",
      "   [ 0.13080615  0.06627449  0.03995639 ...  0.13080615  0.06627449\n",
      "     0.03995639]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-0.36435723 -0.29111093 -0.25259253 ... -0.36435723 -0.29111093\n",
      "    -0.25259253]\n",
      "   [-0.2601743  -0.21206984 -0.15581706 ... -0.2601743  -0.21206984\n",
      "    -0.15581706]\n",
      "   [-0.09442258 -0.10596946 -0.07437912 ... -0.09442258 -0.10596946\n",
      "    -0.07437912]\n",
      "   ...\n",
      "   [ 0.09202617  0.04178655  0.07938999 ...  0.09202617  0.04178655\n",
      "     0.07938999]\n",
      "   [ 0.01908499 -0.04344234 -0.02313718 ...  0.01908499 -0.04344234\n",
      "    -0.02313718]\n",
      "   [-0.1054903  -0.16636163 -0.15638334 ... -0.1054903  -0.16636163\n",
      "    -0.15638334]]\n",
      "\n",
      "  [[-0.32832253 -0.24300648 -0.18662293 ... -0.32832253 -0.24300648\n",
      "    -0.18662293]\n",
      "   [-0.17986926 -0.16361648 -0.12967321 ... -0.17986926 -0.16361648\n",
      "    -0.12967321]\n",
      "   [-0.04483652 -0.0552071  -0.02366015 ... -0.04483652 -0.0552071\n",
      "    -0.02366015]\n",
      "   ...\n",
      "   [ 0.10400873  0.05485839  0.08832249 ...  0.10400873  0.05485839\n",
      "     0.08832249]\n",
      "   [ 0.05803913  0.00204796  0.02004346 ...  0.05803913  0.00204796\n",
      "     0.02004346]\n",
      "   [-0.05189559 -0.10801744 -0.08941171 ... -0.05189559 -0.10801744\n",
      "    -0.08941171]]\n",
      "\n",
      "  [[-0.30675387 -0.23241834 -0.173159   ... -0.30675387 -0.23241834\n",
      "    -0.173159  ]\n",
      "   [-0.12631813 -0.12095839 -0.08457509 ... -0.12631813 -0.12095839\n",
      "    -0.08457509]\n",
      "   [ 0.0120697   0.007451    0.0518519  ...  0.0120697   0.007451\n",
      "     0.0518519 ]\n",
      "   ...\n",
      "   [ 0.13145971  0.0904575   0.13250548 ...  0.13145971  0.0904575\n",
      "     0.13250548]\n",
      "   [ 0.10178643  0.06209153  0.08248359 ...  0.10178643  0.06209153\n",
      "     0.08248359]\n",
      "   [ 0.02305013 -0.01468414 -0.01490197 ...  0.02305013 -0.01468414\n",
      "    -0.01490197]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.25198254 -0.29098058 -0.29568624 ... -0.25198254 -0.29098058\n",
      "    -0.29568624]\n",
      "   [-0.19041398 -0.30705875 -0.29882348 ... -0.19041398 -0.30705875\n",
      "    -0.29882348]\n",
      "   [-0.08553374 -0.13102385 -0.08287576 ... -0.08553374 -0.13102385\n",
      "    -0.08287576]\n",
      "   ...\n",
      "   [-0.32287583 -0.32657957 -0.30980396 ... -0.32287583 -0.32657957\n",
      "    -0.30980396]\n",
      "   [-0.41172117 -0.41355127 -0.39830065 ... -0.41172117 -0.41355127\n",
      "    -0.39830065]\n",
      "   [-0.41629624 -0.41403046 -0.39760336 ... -0.41629624 -0.41403046\n",
      "    -0.39760336]]\n",
      "\n",
      "  [[-0.3422222  -0.3210022  -0.309281   ... -0.3422222  -0.3210022\n",
      "    -0.309281  ]\n",
      "   [-0.23799571 -0.34265798 -0.35015237 ... -0.23799571 -0.34265798\n",
      "    -0.35015237]\n",
      "   [-0.14893243 -0.22897592 -0.19328967 ... -0.14893243 -0.22897592\n",
      "    -0.19328967]\n",
      "   ...\n",
      "   [-0.40657955 -0.40840966 -0.38814825 ... -0.40657955 -0.40840966\n",
      "    -0.38814825]\n",
      "   [-0.43708068 -0.4294554  -0.41185176 ... -0.43708068 -0.4294554\n",
      "    -0.41185176]\n",
      "   [-0.40671033 -0.38840958 -0.3732897  ... -0.40671033 -0.38840958\n",
      "    -0.3732897 ]]\n",
      "\n",
      "  [[-0.4096732  -0.35694975 -0.33799553 ... -0.4096732  -0.35694975\n",
      "    -0.33799553]\n",
      "   [-0.24640533 -0.30457515 -0.31337684 ... -0.24640533 -0.30457515\n",
      "    -0.31337684]\n",
      "   [-0.21960786 -0.32867098 -0.32270163 ... -0.21960786 -0.32867098\n",
      "    -0.32270163]\n",
      "   ...\n",
      "   [-0.43389982 -0.4330719  -0.4068845  ... -0.43389982 -0.4330719\n",
      "    -0.4068845 ]\n",
      "   [-0.44065356 -0.42453164 -0.40183    ... -0.44065356 -0.42453164\n",
      "    -0.40183   ]\n",
      "   [-0.37507623 -0.35725492 -0.34043577 ... -0.37507623 -0.35725492\n",
      "    -0.34043577]]]\n",
      "\n",
      "\n",
      " [[[-0.04671016 -0.03228739 -0.04082781 ... -0.04671016 -0.03228739\n",
      "    -0.04082781]\n",
      "   [-0.13468412 -0.11010906 -0.11267981 ... -0.13468412 -0.11010906\n",
      "    -0.11267981]\n",
      "   [-0.12971672 -0.08636162 -0.06653598 ... -0.12971672 -0.08636162\n",
      "    -0.06653598]\n",
      "   ...\n",
      "   [ 0.17437911  0.1476689   0.13821352 ...  0.17437911  0.1476689\n",
      "     0.13821352]\n",
      "   [ 0.31320262  0.25851846  0.23568636 ...  0.31320262  0.25851846\n",
      "     0.23568636]\n",
      "   [ 0.39254892  0.30814818  0.25930294 ...  0.39254892  0.30814818\n",
      "     0.25930294]]\n",
      "\n",
      "  [[-0.08126369 -0.05477118 -0.05721116 ... -0.08126369 -0.05477118\n",
      "    -0.05721116]\n",
      "   [-0.1759477  -0.14008707 -0.14143792 ... -0.1759477  -0.14008707\n",
      "    -0.14143792]\n",
      "   [-0.16640517 -0.11403063 -0.10601309 ... -0.16640517 -0.11403063\n",
      "    -0.10601309]\n",
      "   ...\n",
      "   [ 0.14322442  0.13328975  0.13930288 ...  0.14322442  0.13328975\n",
      "     0.13930288]\n",
      "   [ 0.29333323  0.2451852   0.24357286 ...  0.29333323  0.2451852\n",
      "     0.24357286]\n",
      "   [ 0.34418285  0.27237472  0.2360785  ...  0.34418285  0.27237472\n",
      "     0.2360785 ]]\n",
      "\n",
      "  [[-0.14204797 -0.11084971 -0.11433548 ... -0.14204797 -0.11084971\n",
      "    -0.11433548]\n",
      "   [-0.17729852 -0.12880152 -0.12771234 ... -0.17729852 -0.12880152\n",
      "    -0.12771234]\n",
      "   [-0.17616561 -0.12588236 -0.12069714 ... -0.17616561 -0.12588236\n",
      "    -0.12069714]\n",
      "   ...\n",
      "   [ 0.11969501  0.12575161  0.14819175 ...  0.11969501  0.12575161\n",
      "     0.14819175]\n",
      "   [ 0.28217858  0.24640524  0.25895417 ...  0.28217858  0.24640524\n",
      "     0.25895417]\n",
      "   [ 0.29755992  0.2441394   0.22039217 ...  0.29755992  0.2441394\n",
      "     0.22039217]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.02252728  0.00705868  0.03372553 ...  0.02252728  0.00705868\n",
      "     0.03372553]\n",
      "   [ 0.00566447 -0.0207842   0.03058833 ...  0.00566447 -0.0207842\n",
      "     0.03058833]\n",
      "   [ 0.07917213  0.02976048  0.10535955 ...  0.07917213  0.02976048\n",
      "     0.10535955]\n",
      "   ...\n",
      "   [ 0.2849673   0.2185185   0.23529407 ...  0.2849673   0.2185185\n",
      "     0.23529407]\n",
      "   [ 0.25102395  0.17076248  0.17816997 ...  0.25102395  0.17076248\n",
      "     0.17816997]\n",
      "   [ 0.26213515  0.182048    0.17886725 ...  0.26213515  0.182048\n",
      "     0.17886725]]\n",
      "\n",
      "  [[ 0.11267978  0.07115471  0.09464061 ...  0.11267978  0.07115471\n",
      "     0.09464061]\n",
      "   [ 0.03259254  0.00244009  0.03416136 ...  0.03259254  0.00244009\n",
      "     0.03416136]\n",
      "   [ 0.09420484  0.02984762  0.09298486 ...  0.09420484  0.02984762\n",
      "     0.09298486]\n",
      "   ...\n",
      "   [ 0.27969497  0.20727664  0.21577334 ...  0.27969497  0.20727664\n",
      "     0.21577334]\n",
      "   [ 0.31193894  0.23328972  0.24697179 ...  0.31193894  0.23328972\n",
      "     0.24697179]\n",
      "   [ 0.3658387   0.28610024  0.29729855 ...  0.3658387   0.28610024\n",
      "     0.29729855]]\n",
      "\n",
      "  [[ 0.11581701  0.0665797   0.07376918 ...  0.11581701  0.0665797\n",
      "     0.07376918]\n",
      "   [ 0.08300644  0.02875823  0.05132908 ...  0.08300644  0.02875823\n",
      "     0.05132908]\n",
      "   [ 0.09019607  0.02427024  0.0772984  ...  0.09019607  0.02427024\n",
      "     0.0772984 ]\n",
      "   ...\n",
      "   [ 0.27590412  0.2061438   0.20095867 ...  0.27590412  0.2061438\n",
      "     0.20095867]\n",
      "   [ 0.3279739   0.24213505  0.24915043 ...  0.3279739   0.24213505\n",
      "     0.24915043]\n",
      "   [ 0.40139437  0.31333333  0.3144662  ...  0.40139437  0.31333333\n",
      "     0.3144662 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.32583886  0.15202636  0.08466238 ...  0.32583886  0.15202636\n",
      "     0.08466238]\n",
      "   [ 0.18688452  0.05459684  0.00104567 ...  0.18688452  0.05459684\n",
      "     0.00104567]\n",
      "   [ 0.00361663 -0.07459691 -0.1135948  ...  0.00361663 -0.07459691\n",
      "    -0.1135948 ]\n",
      "   ...\n",
      "   [ 0.35477126  0.31237477  0.251939   ...  0.35477126  0.31237477\n",
      "     0.251939  ]\n",
      "   [ 0.41124183  0.42322433  0.33764714 ...  0.41124183  0.42322433\n",
      "     0.33764714]\n",
      "   [ 0.48666656  0.51991284  0.42008725 ...  0.48666656  0.51991284\n",
      "     0.42008725]]\n",
      "\n",
      "  [[ 0.23638338  0.09424844  0.03690648 ...  0.23638338  0.09424844\n",
      "     0.03690648]\n",
      "   [ 0.08679742 -0.01851845 -0.07084969 ...  0.08679742 -0.01851845\n",
      "    -0.07084969]\n",
      "   [-0.02915025 -0.07873651 -0.09032682 ... -0.02915025 -0.07873651\n",
      "    -0.09032682]\n",
      "   ...\n",
      "   [ 0.30008715  0.2391721   0.20596954 ...  0.30008715  0.2391721\n",
      "     0.20596954]\n",
      "   [ 0.38745087  0.41381264  0.34161207 ...  0.38745087  0.41381264\n",
      "     0.34161207]\n",
      "   [ 0.45398676  0.4723747   0.37725496 ...  0.45398676  0.4723747\n",
      "     0.37725496]]\n",
      "\n",
      "  [[ 0.14814812  0.03816989 -0.00845313 ...  0.14814812  0.03816989\n",
      "    -0.00845313]\n",
      "   [ 0.0109368  -0.07782114 -0.12379077 ...  0.0109368  -0.07782114\n",
      "    -0.12379077]\n",
      "   [-0.10165578 -0.1572549  -0.17167753 ... -0.10165578 -0.1572549\n",
      "    -0.17167753]\n",
      "   ...\n",
      "   [ 0.29224402  0.21594769  0.18348587 ...  0.29224402  0.21594769\n",
      "     0.18348587]\n",
      "   [ 0.36453152  0.37189543  0.30993456 ...  0.36453152  0.37189543\n",
      "     0.30993456]\n",
      "   [ 0.4230501   0.42453155  0.3419608  ...  0.4230501   0.42453155\n",
      "     0.3419608 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.11472765 -0.22823545 -0.23686272 ... -0.11472765 -0.22823545\n",
      "    -0.23686272]\n",
      "   [-0.10021788 -0.12666658 -0.07529405 ... -0.10021788 -0.12666658\n",
      "    -0.07529405]\n",
      "   [-0.18357298 -0.2251415  -0.18483654 ... -0.18357298 -0.2251415\n",
      "    -0.18483654]\n",
      "   ...\n",
      "   [ 0.3477124   0.37538123  0.32941172 ...  0.3477124   0.37538123\n",
      "     0.32941172]\n",
      "   [ 0.368671    0.4531154   0.40169936 ...  0.368671    0.4531154\n",
      "     0.40169936]\n",
      "   [ 0.42684102  0.45655778  0.36318097 ...  0.42684102  0.45655778\n",
      "     0.36318097]]\n",
      "\n",
      "  [[-0.07163393 -0.21119824 -0.2583006  ... -0.07163393 -0.21119824\n",
      "    -0.2583006 ]\n",
      "   [-0.12427023 -0.20932463 -0.19328964 ... -0.12427023 -0.20932463\n",
      "    -0.19328964]\n",
      "   [-0.152854   -0.17407396 -0.12270144 ... -0.152854   -0.17407396\n",
      "    -0.12270144]\n",
      "   ...\n",
      "   [ 0.35812634  0.44257075  0.40400863 ...  0.35812634  0.44257075\n",
      "     0.40400863]\n",
      "   [ 0.38252717  0.45681912  0.38814825 ...  0.38252717  0.45681912\n",
      "     0.38814825]\n",
      "   [ 0.45603478  0.45864925  0.34435737 ...  0.45603478  0.45864925\n",
      "     0.34435737]]\n",
      "\n",
      "  [[-0.00575161 -0.14518502 -0.18897593 ... -0.00575161 -0.14518502\n",
      "    -0.18897593]\n",
      "   [-0.10915044 -0.23006532 -0.24671017 ... -0.10915044 -0.23006532\n",
      "    -0.24671017]\n",
      "   [-0.19607845 -0.25416115 -0.2246624  ... -0.19607845 -0.25416115\n",
      "    -0.2246624 ]\n",
      "   ...\n",
      "   [ 0.37002176  0.46104574  0.41272336 ...  0.37002176  0.46104574\n",
      "     0.41272336]\n",
      "   [ 0.41032684  0.49311543  0.45307198 ...  0.41032684  0.49311543\n",
      "     0.45307198]\n",
      "   [ 0.49943358  0.552549    0.45956424 ...  0.49943358  0.552549\n",
      "     0.45956424]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Learning rate:  0.0\n",
      "3/3 [==============================] - 8s 1s/step - loss: 3.5097 - accuracy: 0.0222 - val_loss: 2.9641 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/100\n",
      "Learning rate:  0.0033333333333333335\n",
      "3/3 [==============================] - 2s 801ms/step - loss: 3.5170 - accuracy: 0.0333 - val_loss: 2.9870 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/100\n",
      "Learning rate:  0.006666666666666667\n",
      "3/3 [==============================] - 2s 866ms/step - loss: 3.4920 - accuracy: 0.0333 - val_loss: 2.9928 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/100\n",
      "Learning rate:  0.01\n",
      "3/3 [==============================] - 2s 851ms/step - loss: 3.4572 - accuracy: 0.0333 - val_loss: 2.9952 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/100\n",
      "Learning rate:  0.013333333333333334\n",
      "3/3 [==============================] - 3s 886ms/step - loss: 3.4025 - accuracy: 0.0333 - val_loss: 2.9884 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/100\n",
      "Learning rate:  0.016666666666666666\n",
      "3/3 [==============================] - 2s 790ms/step - loss: 3.3699 - accuracy: 0.0444 - val_loss: 2.9751 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/100\n",
      "Learning rate:  0.02\n",
      "3/3 [==============================] - 2s 844ms/step - loss: 3.2841 - accuracy: 0.0556 - val_loss: 2.9524 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/100\n",
      "Learning rate:  0.023333333333333334\n",
      "3/3 [==============================] - 2s 836ms/step - loss: 3.2105 - accuracy: 0.0667 - val_loss: 2.9295 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/100\n",
      "Learning rate:  0.02666666666666667\n",
      "3/3 [==============================] - 2s 822ms/step - loss: 3.1003 - accuracy: 0.0889 - val_loss: 2.9018 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/100\n",
      "Learning rate:  0.03\n",
      "3/3 [==============================] - 2s 808ms/step - loss: 3.0175 - accuracy: 0.1333 - val_loss: 2.8688 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/100\n",
      "Learning rate:  0.03333333333333333\n",
      "3/3 [==============================] - 2s 803ms/step - loss: 2.9314 - accuracy: 0.1556 - val_loss: 2.8339 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/100\n",
      "Learning rate:  0.03666666666666667\n",
      "3/3 [==============================] - 2s 788ms/step - loss: 2.8649 - accuracy: 0.1889 - val_loss: 2.7946 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/100\n",
      "Learning rate:  0.04\n",
      "3/3 [==============================] - 2s 801ms/step - loss: 2.7376 - accuracy: 0.2667 - val_loss: 2.7461 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/100\n",
      "Learning rate:  0.043333333333333335\n",
      "3/3 [==============================] - 2s 822ms/step - loss: 2.6728 - accuracy: 0.3111 - val_loss: 2.6939 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/100\n",
      "Learning rate:  0.04666666666666667\n",
      "3/3 [==============================] - 2s 896ms/step - loss: 2.5207 - accuracy: 0.4667 - val_loss: 2.6373 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/100\n",
      "Learning rate:  0.05\n",
      "3/3 [==============================] - 2s 836ms/step - loss: 2.4429 - accuracy: 0.4778 - val_loss: 2.5735 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/100\n",
      "Learning rate:  0.05333333333333334\n",
      "3/3 [==============================] - 2s 813ms/step - loss: 2.3460 - accuracy: 0.6000 - val_loss: 2.5084 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/100\n",
      "Learning rate:  0.056666666666666664\n",
      "3/3 [==============================] - 2s 873ms/step - loss: 2.2201 - accuracy: 0.6444 - val_loss: 2.4429 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/100\n",
      "Learning rate:  0.06\n",
      "3/3 [==============================] - 2s 810ms/step - loss: 2.1195 - accuracy: 0.6222 - val_loss: 2.3712 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/100\n",
      "Learning rate:  0.06333333333333334\n",
      "3/3 [==============================] - 2s 794ms/step - loss: 2.0880 - accuracy: 0.6000 - val_loss: 2.2928 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 21/100\n",
      "Learning rate:  0.06666666666666667\n",
      "3/3 [==============================] - 2s 813ms/step - loss: 1.9469 - accuracy: 0.6333 - val_loss: 2.2133 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 22/100\n",
      "Learning rate:  0.07\n",
      "3/3 [==============================] - 2s 803ms/step - loss: 1.8950 - accuracy: 0.6000 - val_loss: 2.1337 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 23/100\n",
      "Learning rate:  0.07333333333333333\n",
      "3/3 [==============================] - 2s 843ms/step - loss: 1.9007 - accuracy: 0.5778 - val_loss: 2.0587 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 24/100\n",
      "Learning rate:  0.07666666666666666\n",
      "3/3 [==============================] - 2s 893ms/step - loss: 1.7600 - accuracy: 0.6667 - val_loss: 1.9874 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 25/100\n",
      "Learning rate:  0.08\n",
      "3/3 [==============================] - 2s 860ms/step - loss: 1.8048 - accuracy: 0.6222 - val_loss: 1.9262 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 26/100\n",
      "Learning rate:  0.08333333333333333\n",
      "3/3 [==============================] - 2s 872ms/step - loss: 1.6438 - accuracy: 0.6333 - val_loss: 1.8644 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 27/100\n",
      "Learning rate:  0.08666666666666667\n",
      "3/3 [==============================] - 2s 812ms/step - loss: 1.6127 - accuracy: 0.6556 - val_loss: 1.8189 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 28/100\n",
      "Learning rate:  0.09\n",
      "3/3 [==============================] - 2s 821ms/step - loss: 1.5116 - accuracy: 0.7111 - val_loss: 1.7666 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 29/100\n",
      "Learning rate:  0.09333333333333334\n",
      "3/3 [==============================] - 2s 833ms/step - loss: 1.4930 - accuracy: 0.6333 - val_loss: 1.7100 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 30/100\n",
      "Learning rate:  0.09666666666666666\n",
      "3/3 [==============================] - 2s 819ms/step - loss: 1.5019 - accuracy: 0.7000 - val_loss: 1.6524 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 31/100\n",
      "Learning rate:  0.1\n",
      "3/3 [==============================] - 2s 870ms/step - loss: 1.4356 - accuracy: 0.6667 - val_loss: 1.6154 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 32/100\n",
      "Learning rate:  0.10333333333333333\n",
      "3/3 [==============================] - 2s 885ms/step - loss: 1.3812 - accuracy: 0.6556 - val_loss: 1.6404 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 33/100\n",
      "Learning rate:  0.10666666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 840ms/step - loss: 1.3307 - accuracy: 0.7556 - val_loss: 1.6709 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 34/100\n",
      "Learning rate:  0.11\n",
      "3/3 [==============================] - 2s 802ms/step - loss: 1.3025 - accuracy: 0.7333 - val_loss: 1.7143 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 35/100\n",
      "Learning rate:  0.11333333333333333\n",
      "3/3 [==============================] - 2s 862ms/step - loss: 1.2719 - accuracy: 0.7333 - val_loss: 1.8095 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 36/100\n",
      "Learning rate:  0.11666666666666667\n",
      "3/3 [==============================] - 2s 805ms/step - loss: 1.2148 - accuracy: 0.8111 - val_loss: 1.8427 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 37/100\n",
      "Learning rate:  0.12\n",
      "3/3 [==============================] - 2s 778ms/step - loss: 1.2717 - accuracy: 0.7222 - val_loss: 1.8692 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 38/100\n",
      "Learning rate:  0.12333333333333334\n",
      "3/3 [==============================] - 3s 858ms/step - loss: 1.1452 - accuracy: 0.8222 - val_loss: 1.7909 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 39/100\n",
      "Learning rate:  0.12666666666666668\n",
      "3/3 [==============================] - 2s 805ms/step - loss: 1.2504 - accuracy: 0.7778 - val_loss: 1.7676 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 40/100\n",
      "Learning rate:  0.13\n",
      "3/3 [==============================] - 2s 850ms/step - loss: 1.2283 - accuracy: 0.7556 - val_loss: 1.6993 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 41/100\n",
      "Learning rate:  0.13333333333333333\n",
      "3/3 [==============================] - 3s 864ms/step - loss: 1.1254 - accuracy: 0.7778 - val_loss: 1.5390 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 42/100\n",
      "Learning rate:  0.13666666666666666\n",
      "3/3 [==============================] - 3s 861ms/step - loss: 1.2546 - accuracy: 0.7000 - val_loss: 1.4985 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 43/100\n",
      "Learning rate:  0.14\n",
      "3/3 [==============================] - 2s 889ms/step - loss: 1.3109 - accuracy: 0.7444 - val_loss: 1.5002 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 44/100\n",
      "Learning rate:  0.14333333333333334\n",
      "3/3 [==============================] - 2s 852ms/step - loss: 1.1857 - accuracy: 0.8111 - val_loss: 1.6546 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 45/100\n",
      "Learning rate:  0.14666666666666667\n",
      "3/3 [==============================] - 2s 807ms/step - loss: 1.2250 - accuracy: 0.7444 - val_loss: 1.7751 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 46/100\n",
      "Learning rate:  0.15\n",
      "3/3 [==============================] - 2s 804ms/step - loss: 1.0550 - accuracy: 0.8222 - val_loss: 1.9957 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 47/100\n",
      "Learning rate:  0.15333333333333332\n",
      "3/3 [==============================] - 2s 813ms/step - loss: 1.0433 - accuracy: 0.8778 - val_loss: 2.1274 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 48/100\n",
      "Learning rate:  0.15666666666666668\n",
      "3/3 [==============================] - 2s 807ms/step - loss: 1.0811 - accuracy: 0.8222 - val_loss: 2.5381 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 49/100\n",
      "Learning rate:  0.16\n",
      "3/3 [==============================] - 2s 876ms/step - loss: 1.1208 - accuracy: 0.7778 - val_loss: 3.3974 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 50/100\n",
      "Learning rate:  0.16333333333333333\n",
      "3/3 [==============================] - 2s 885ms/step - loss: 1.0806 - accuracy: 0.7778 - val_loss: 4.4382 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 51/100\n",
      "Learning rate:  0.16666666666666666\n",
      "3/3 [==============================] - 2s 838ms/step - loss: 0.9635 - accuracy: 0.8444 - val_loss: 5.1599 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 52/100\n",
      "Learning rate:  0.17\n",
      "3/3 [==============================] - 2s 811ms/step - loss: 0.9636 - accuracy: 0.8889 - val_loss: 4.8015 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 53/100\n",
      "Learning rate:  0.17333333333333334\n",
      "3/3 [==============================] - 2s 843ms/step - loss: 1.0100 - accuracy: 0.8444 - val_loss: 3.8307 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 54/100\n",
      "Learning rate:  0.17666666666666667\n",
      "3/3 [==============================] - 2s 821ms/step - loss: 0.9609 - accuracy: 0.9000 - val_loss: 2.8056 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 55/100\n",
      "Learning rate:  0.18\n",
      "3/3 [==============================] - 2s 807ms/step - loss: 0.9768 - accuracy: 0.8556 - val_loss: 4.3239 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 56/100\n",
      "Learning rate:  0.18333333333333332\n",
      "3/3 [==============================] - 2s 922ms/step - loss: 1.0325 - accuracy: 0.8444 - val_loss: 5.5365 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 57/100\n",
      "Learning rate:  0.18666666666666668\n",
      "3/3 [==============================] - 2s 861ms/step - loss: 0.9312 - accuracy: 0.9000 - val_loss: 6.0702 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 58/100\n",
      "Learning rate:  0.19\n",
      "3/3 [==============================] - 3s 819ms/step - loss: 0.9143 - accuracy: 0.8889 - val_loss: 5.4192 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 59/100\n",
      "Learning rate:  0.19333333333333333\n",
      "3/3 [==============================] - 2s 902ms/step - loss: 0.8984 - accuracy: 0.8778 - val_loss: 5.2603 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 60/100\n",
      "Learning rate:  0.19666666666666666\n",
      "3/3 [==============================] - 2s 810ms/step - loss: 0.8643 - accuracy: 0.9000 - val_loss: 6.4393 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 61/100\n",
      "Learning rate:  0.2\n",
      "3/3 [==============================] - 2s 813ms/step - loss: 0.8351 - accuracy: 0.9444 - val_loss: 5.8957 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 62/100\n",
      "Learning rate:  0.20333333333333334\n",
      "3/3 [==============================] - 2s 820ms/step - loss: 0.8854 - accuracy: 0.8778 - val_loss: 4.8129 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 63/100\n",
      "Learning rate:  0.20666666666666667\n",
      "3/3 [==============================] - 2s 911ms/step - loss: 0.9403 - accuracy: 0.8889 - val_loss: 4.0860 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 64/100\n",
      "Learning rate:  0.21\n",
      "3/3 [==============================] - 3s 927ms/step - loss: 0.9985 - accuracy: 0.8444 - val_loss: 4.1257 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 65/100\n",
      "Learning rate:  0.21333333333333335\n",
      "3/3 [==============================] - 3s 871ms/step - loss: 0.7925 - accuracy: 0.9111 - val_loss: 3.2682 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100\n",
      "Learning rate:  0.21666666666666667\n",
      "3/3 [==============================] - 3s 890ms/step - loss: 0.9227 - accuracy: 0.9111 - val_loss: 5.7755 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 67/100\n",
      "Learning rate:  0.22\n",
      "3/3 [==============================] - 2s 853ms/step - loss: 1.0340 - accuracy: 0.8444 - val_loss: 5.0752 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 68/100\n",
      "Learning rate:  0.22333333333333333\n",
      "3/3 [==============================] - 2s 798ms/step - loss: 1.0999 - accuracy: 0.8222 - val_loss: 6.7095 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 69/100\n",
      "Learning rate:  0.22666666666666666\n",
      "3/3 [==============================] - 2s 810ms/step - loss: 0.8628 - accuracy: 0.9000 - val_loss: 16.9052 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 70/100\n",
      "Learning rate:  0.23\n",
      "3/3 [==============================] - 2s 873ms/step - loss: 0.9403 - accuracy: 0.8667 - val_loss: 15.7566 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 71/100\n",
      "Learning rate:  0.23333333333333334\n",
      "3/3 [==============================] - 2s 808ms/step - loss: 1.0396 - accuracy: 0.8556 - val_loss: 8.5671 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 72/100\n",
      "Learning rate:  0.23666666666666666\n",
      "3/3 [==============================] - 2s 860ms/step - loss: 0.8690 - accuracy: 0.8889 - val_loss: 6.2366 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 73/100\n",
      "Learning rate:  0.24\n",
      "3/3 [==============================] - 2s 863ms/step - loss: 0.9633 - accuracy: 0.8556 - val_loss: 7.0612 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 74/100\n",
      "Learning rate:  0.24333333333333335\n",
      "3/3 [==============================] - 2s 828ms/step - loss: 0.9835 - accuracy: 0.8667 - val_loss: 7.9893 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 75/100\n",
      "Learning rate:  0.24666666666666667\n",
      "3/3 [==============================] - 2s 794ms/step - loss: 0.8175 - accuracy: 0.9222 - val_loss: 8.1137 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 76/100\n",
      "Learning rate:  0.25\n",
      "3/3 [==============================] - 2s 839ms/step - loss: 0.8060 - accuracy: 0.9444 - val_loss: 9.1598 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 77/100\n",
      "Learning rate:  0.25333333333333335\n",
      "3/3 [==============================] - 2s 880ms/step - loss: 0.7922 - accuracy: 0.9333 - val_loss: 10.1865 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 78/100\n",
      "Learning rate:  0.25666666666666665\n",
      "3/3 [==============================] - 2s 897ms/step - loss: 1.0156 - accuracy: 0.8333 - val_loss: 7.9559 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 79/100\n",
      "Learning rate:  0.26\n",
      "3/3 [==============================] - 2s 847ms/step - loss: 0.7925 - accuracy: 0.9333 - val_loss: 6.4085 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 80/100\n",
      "Learning rate:  0.2633333333333333\n",
      "3/3 [==============================] - 2s 879ms/step - loss: 0.8841 - accuracy: 0.8889 - val_loss: 7.0579 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 81/100\n",
      "Learning rate:  0.26666666666666666\n",
      "3/3 [==============================] - 2s 882ms/step - loss: 0.8876 - accuracy: 0.8778 - val_loss: 8.1848 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 82/100\n",
      "Learning rate:  0.27\n",
      "3/3 [==============================] - 2s 831ms/step - loss: 0.7629 - accuracy: 0.9778 - val_loss: 8.1124 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 83/100\n",
      "Learning rate:  0.2733333333333333\n",
      "3/3 [==============================] - 2s 888ms/step - loss: 0.7583 - accuracy: 0.9222 - val_loss: 8.0591 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 84/100\n",
      "Learning rate:  0.27666666666666667\n",
      "3/3 [==============================] - 2s 822ms/step - loss: 0.7068 - accuracy: 0.9667 - val_loss: 8.0408 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 85/100\n",
      "Learning rate:  0.28\n",
      "3/3 [==============================] - 2s 870ms/step - loss: 0.8807 - accuracy: 0.8778 - val_loss: 8.0368 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 86/100\n",
      "Learning rate:  0.2833333333333333\n",
      "3/3 [==============================] - 2s 871ms/step - loss: 0.7274 - accuracy: 0.9778 - val_loss: 7.9517 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 87/100\n",
      "Learning rate:  0.2866666666666667\n",
      "3/3 [==============================] - 2s 826ms/step - loss: 0.7631 - accuracy: 0.9333 - val_loss: 8.0296 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 88/100\n",
      "Learning rate:  0.29\n",
      "3/3 [==============================] - 2s 825ms/step - loss: 0.7302 - accuracy: 0.9556 - val_loss: 8.2279 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 89/100\n",
      "Learning rate:  0.29333333333333333\n",
      "3/3 [==============================] - 2s 858ms/step - loss: 0.7021 - accuracy: 0.9778 - val_loss: 8.3628 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 90/100\n",
      "Learning rate:  0.2966666666666667\n",
      "3/3 [==============================] - 2s 862ms/step - loss: 0.6672 - accuracy: 1.0000 - val_loss: 8.3432 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 91/100\n",
      "Learning rate:  0.3\n",
      "3/3 [==============================] - 2s 845ms/step - loss: 0.7357 - accuracy: 0.9667 - val_loss: 8.3392 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 92/100\n",
      "Learning rate:  0.30333333333333334\n",
      "3/3 [==============================] - 2s 821ms/step - loss: 0.7029 - accuracy: 0.9667 - val_loss: 8.4080 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 93/100\n",
      "Learning rate:  0.30666666666666664\n",
      "3/3 [==============================] - 3s 920ms/step - loss: 0.7058 - accuracy: 0.9778 - val_loss: 8.5326 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 94/100\n",
      "Learning rate:  0.31\n",
      "3/3 [==============================] - 2s 859ms/step - loss: 0.8723 - accuracy: 0.9111 - val_loss: 8.6614 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 95/100\n",
      "Learning rate:  0.31333333333333335\n",
      "3/3 [==============================] - 2s 874ms/step - loss: 0.7807 - accuracy: 0.9111 - val_loss: 8.5397 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 96/100\n",
      "Learning rate:  0.31666666666666665\n",
      "3/3 [==============================] - 3s 939ms/step - loss: 0.7046 - accuracy: 0.9556 - val_loss: 8.4213 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 97/100\n",
      "Learning rate:  0.32\n",
      "3/3 [==============================] - 2s 856ms/step - loss: 0.6827 - accuracy: 0.9667 - val_loss: 8.3733 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 98/100\n",
      "Learning rate:  0.3233333333333333\n",
      "3/3 [==============================] - 3s 928ms/step - loss: 0.6783 - accuracy: 0.9778 - val_loss: 8.3543 - val_accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 99/100\n",
      "Learning rate:  0.32666666666666666\n",
      "3/3 [==============================] - 2s 844ms/step - loss: 0.6538 - accuracy: 1.0000 - val_loss: 8.3231 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 100/100\n",
      "Learning rate:  0.33\n",
      "3/3 [==============================] - 3s 981ms/step - loss: 0.9075 - accuracy: 0.8556 - val_loss: 8.1118 - val_accuracy: 0.6000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 1.9831 - accuracy: 0.5333\n",
      "Test loss: 1.9830704927444458\n",
      "Test accuracy: 0.5333333611488342\n",
      "[0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 2, 0, 0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 0, 2, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision:  1.0\n",
      "Recall:  0.5333333333333333\n",
      "F1 Score:  0.6956521739130436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 54  55  58 ...  54  55  58]\n",
      "   [168 168 169 ... 168 168 169]\n",
      "   [184 184 185 ... 184 184 185]\n",
      "   ...\n",
      "   [190 180 183 ... 190 180 183]\n",
      "   [172 164 169 ... 172 164 169]\n",
      "   [ 66  72  89 ...  66  72  89]]\n",
      "\n",
      "  [[ 99  99 102 ...  99  99 102]\n",
      "   [179 178 179 ... 179 178 179]\n",
      "   [185 183 184 ... 185 183 184]\n",
      "   ...\n",
      "   [192 180 184 ... 192 180 184]\n",
      "   [184 172 175 ... 184 172 175]\n",
      "   [100  97 108 ... 100  97 108]]\n",
      "\n",
      "  [[149 148 152 ... 149 148 152]\n",
      "   [184 184 184 ... 184 184 184]\n",
      "   [188 187 187 ... 188 187 187]\n",
      "   ...\n",
      "   [193 182 186 ... 193 182 186]\n",
      "   [189 180 183 ... 189 180 183]\n",
      "   [147 141 147 ... 147 141 147]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[180 179 183 ... 180 179 183]\n",
      "   [191 186 187 ... 191 186 187]\n",
      "   [195 189 190 ... 195 189 190]\n",
      "   ...\n",
      "   [190 178 181 ... 190 178 181]\n",
      "   [184 171 175 ... 184 171 175]\n",
      "   [159 148 153 ... 159 148 153]]\n",
      "\n",
      "  [[166 172 181 ... 166 172 181]\n",
      "   [189 185 186 ... 189 185 186]\n",
      "   [193 189 189 ... 193 189 189]\n",
      "   ...\n",
      "   [188 176 179 ... 188 176 179]\n",
      "   [183 172 176 ... 183 172 176]\n",
      "   [113 106 110 ... 113 106 110]]\n",
      "\n",
      "  [[138 155 171 ... 138 155 171]\n",
      "   [185 182 185 ... 185 182 185]\n",
      "   [192 191 192 ... 192 191 192]\n",
      "   ...\n",
      "   [187 176 180 ... 187 176 180]\n",
      "   [177 167 171 ... 177 167 171]\n",
      "   [ 64  61  65 ...  64  61  65]]]\n",
      "\n",
      "\n",
      " [[[156 130 112 ... 156 130 112]\n",
      "   [155 132 114 ... 155 132 114]\n",
      "   [162 144 131 ... 162 144 131]\n",
      "   ...\n",
      "   [173 152 138 ... 173 152 138]\n",
      "   [177 161 153 ... 177 161 153]\n",
      "   [188 176 178 ... 188 176 178]]\n",
      "\n",
      "  [[154 127 107 ... 154 127 107]\n",
      "   [158 136 121 ... 158 136 121]\n",
      "   [162 142 126 ... 162 142 126]\n",
      "   ...\n",
      "   [174 153 142 ... 174 153 142]\n",
      "   [175 153 140 ... 175 153 140]\n",
      "   [189 177 179 ... 189 177 179]]\n",
      "\n",
      "  [[145 112  88 ... 145 112  88]\n",
      "   [161 142 129 ... 161 142 129]\n",
      "   [158 132 110 ... 158 132 110]\n",
      "   ...\n",
      "   [175 155 147 ... 175 155 147]\n",
      "   [168 139 119 ... 168 139 119]\n",
      "   [181 162 155 ... 181 162 155]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[199 191 190 ... 199 191 190]\n",
      "   [191 179 172 ... 191 179 172]\n",
      "   [173 149 129 ... 173 149 129]\n",
      "   ...\n",
      "   [181 160 145 ... 181 160 145]\n",
      "   [181 164 150 ... 181 164 150]\n",
      "   [177 158 146 ... 177 158 146]]\n",
      "\n",
      "  [[192 179 175 ... 192 179 175]\n",
      "   [197 189 186 ... 197 189 186]\n",
      "   [173 149 127 ... 173 149 127]\n",
      "   ...\n",
      "   [180 160 144 ... 180 160 144]\n",
      "   [179 160 144 ... 179 160 144]\n",
      "   [172 154 140 ... 172 154 140]]\n",
      "\n",
      "  [[186 168 164 ... 186 168 164]\n",
      "   [201 194 194 ... 201 194 194]\n",
      "   [180 161 143 ... 180 161 143]\n",
      "   ...\n",
      "   [178 155 137 ... 178 155 137]\n",
      "   [178 159 144 ... 178 159 144]\n",
      "   [168 149 136 ... 168 149 136]]]\n",
      "\n",
      "\n",
      " [[[159 141 136 ... 159 141 136]\n",
      "   [182 148 136 ... 182 148 136]\n",
      "   [175 143 133 ... 175 143 133]\n",
      "   ...\n",
      "   [186 150 136 ... 186 150 136]\n",
      "   [181 148 137 ... 181 148 137]\n",
      "   [117  96  91 ... 117  96  91]]\n",
      "\n",
      "  [[176 147 138 ... 176 147 138]\n",
      "   [185 152 141 ... 185 152 141]\n",
      "   [176 143 135 ... 176 143 135]\n",
      "   ...\n",
      "   [188 152 138 ... 188 152 138]\n",
      "   [182 148 136 ... 182 148 136]\n",
      "   [158 129 122 ... 158 129 122]]\n",
      "\n",
      "  [[183 155 146 ... 183 155 146]\n",
      "   [182 153 144 ... 182 153 144]\n",
      "   [178 145 135 ... 178 145 135]\n",
      "   ...\n",
      "   [190 153 138 ... 190 153 138]\n",
      "   [183 147 132 ... 183 147 132]\n",
      "   [174 145 138 ... 174 145 138]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[180 151 143 ... 180 151 143]\n",
      "   [178 143 129 ... 178 143 129]\n",
      "   [189 153 141 ... 189 153 141]\n",
      "   ...\n",
      "   [192 147 126 ... 192 147 126]\n",
      "   [187 145 127 ... 187 145 127]\n",
      "   [178 144 131 ... 178 144 131]]\n",
      "\n",
      "  [[174 145 139 ... 174 145 139]\n",
      "   [180 149 139 ... 180 149 139]\n",
      "   [188 152 139 ... 188 152 139]\n",
      "   ...\n",
      "   [190 145 126 ... 190 145 126]\n",
      "   [185 148 133 ... 185 148 133]\n",
      "   [169 133 122 ... 169 133 122]]\n",
      "\n",
      "  [[168 140 134 ... 168 140 134]\n",
      "   [178 147 137 ... 178 147 137]\n",
      "   [184 145 131 ... 184 145 131]\n",
      "   ...\n",
      "   [190 150 136 ... 190 150 136]\n",
      "   [185 152 141 ... 185 152 141]\n",
      "   [161 125 113 ... 161 125 113]]]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (90, 40, 30, 9)\n",
      "90 train samples\n",
      "30 test samples\n",
      "y_train shape: (90, 1)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 40, 30, 16)   1312        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 40, 30, 16)   64          conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 40, 30, 16)   0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 40, 30, 16)   272         activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 40, 30, 16)   64          conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 40, 30, 16)   0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 40, 30, 16)   2320        activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 40, 30, 16)   64          conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 40, 30, 16)   0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 40, 30, 64)   1088        activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 40, 30, 64)   1088        activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 40, 30, 64)   0           conv2d_159[0][0]                 \n",
      "                                                                 conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 40, 30, 64)   256         add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 40, 30, 64)   0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 40, 30, 16)   1040        activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 40, 30, 16)   64          conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 40, 30, 16)   0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 40, 30, 16)   2320        activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 40, 30, 16)   64          conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 40, 30, 16)   0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 40, 30, 64)   1088        activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 40, 30, 64)   0           add_45[0][0]                     \n",
      "                                                                 conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 40, 30, 64)   256         add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 40, 30, 64)   0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 40, 30, 16)   1040        activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 40, 30, 16)   64          conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 40, 30, 16)   0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 40, 30, 16)   2320        activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 40, 30, 16)   64          conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 40, 30, 16)   0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 40, 30, 64)   1088        activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 40, 30, 64)   0           add_46[0][0]                     \n",
      "                                                                 conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 40, 30, 64)   256         add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 40, 30, 64)   0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 20, 15, 64)   4160        activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 20, 15, 64)   256         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 20, 15, 64)   0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 20, 15, 64)   36928       activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 20, 15, 64)   256         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 20, 15, 64)   0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 20, 15, 128)  8320        add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 20, 15, 128)  8320        activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 20, 15, 128)  0           conv2d_169[0][0]                 \n",
      "                                                                 conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 20, 15, 128)  512         add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 20, 15, 128)  0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 20, 15, 64)   8256        activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 20, 15, 64)   256         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 20, 15, 64)   0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 20, 15, 64)   36928       activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 20, 15, 64)   256         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 20, 15, 64)   0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 20, 15, 128)  8320        activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (None, 20, 15, 128)  0           add_48[0][0]                     \n",
      "                                                                 conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 20, 15, 128)  512         add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 20, 15, 128)  0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 20, 15, 64)   8256        activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 20, 15, 64)   256         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 20, 15, 64)   0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 20, 15, 64)   36928       activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 20, 15, 64)   256         conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 20, 15, 64)   0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 20, 15, 128)  8320        activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_50 (Add)                    (None, 20, 15, 128)  0           add_49[0][0]                     \n",
      "                                                                 conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 20, 15, 128)  512         add_50[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 20, 15, 128)  0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 10, 8, 128)   16512       activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 10, 8, 128)   512         conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 10, 8, 128)   0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 10, 8, 128)   147584      activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 10, 8, 128)   512         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 10, 8, 128)   0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 10, 8, 256)   33024       add_50[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 10, 8, 256)   33024       activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_51 (Add)                    (None, 10, 8, 256)   0           conv2d_179[0][0]                 \n",
      "                                                                 conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 10, 8, 256)   1024        add_51[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 10, 8, 256)   0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 10, 8, 128)   32896       activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 10, 8, 128)   512         conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 10, 8, 128)   0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 10, 8, 128)   147584      activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 10, 8, 128)   512         conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 10, 8, 128)   0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 10, 8, 256)   33024       activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_52 (Add)                    (None, 10, 8, 256)   0           add_51[0][0]                     \n",
      "                                                                 conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 10, 8, 256)   1024        add_52[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 10, 8, 256)   0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 10, 8, 128)   32896       activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 10, 8, 128)   512         conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 10, 8, 128)   0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 10, 8, 128)   147584      activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 10, 8, 128)   512         conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 10, 8, 128)   0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 10, 8, 256)   33024       activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_53 (Add)                    (None, 10, 8, 256)   0           add_52[0][0]                     \n",
      "                                                                 conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 10, 8, 256)   1024        add_53[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 10, 8, 256)   0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 1, 1, 256)    0           activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 256)          0           average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           2570        flatten_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[-2.35991195e-01 -1.60609826e-01 -1.35686293e-01 ... -2.35991195e-01\n",
      "    -1.60609826e-01 -1.35686293e-01]\n",
      "   [ 1.04880154e-01  2.02832103e-01  2.32592553e-01 ...  1.04880154e-01\n",
      "     2.02832103e-01  2.32592553e-01]\n",
      "   [ 1.05708122e-01  2.19520807e-01  2.57560045e-01 ...  1.05708122e-01\n",
      "     2.19520807e-01  2.57560045e-01]\n",
      "   ...\n",
      "   [ 1.00653470e-01  1.68235362e-01  2.17211366e-01 ...  1.00653470e-01\n",
      "     1.68235362e-01  2.17211366e-01]\n",
      "   [ 9.18954611e-02  1.48278803e-01  1.94379270e-01 ...  9.18954611e-02\n",
      "     1.48278803e-01  1.94379270e-01]\n",
      "   [-2.36034840e-01 -1.52810335e-01 -7.83876777e-02 ... -2.36034840e-01\n",
      "    -1.52810335e-01 -7.83876777e-02]]\n",
      "\n",
      "  [[-1.00653678e-01 -2.08713114e-02  7.14612007e-03 ... -1.00653678e-01\n",
      "    -2.08713114e-02  7.14612007e-03]\n",
      "   [ 1.15163386e-01  2.17385739e-01  2.49019623e-01 ...  1.15163386e-01\n",
      "     2.17385739e-01  2.49019623e-01]\n",
      "   [ 9.41613913e-02  2.02483714e-01  2.40958720e-01 ...  9.41613913e-02\n",
      "     2.02483714e-01  2.40958720e-01]\n",
      "   ...\n",
      "   [ 9.27232504e-02  1.56078339e-01  2.09150434e-01 ...  9.27232504e-02\n",
      "     1.56078339e-01  2.09150434e-01]\n",
      "   [ 1.07363641e-01  1.58605635e-01  2.01873600e-01 ...  1.07363641e-01\n",
      "     1.58605635e-01  2.01873600e-01]\n",
      "   [-1.36775702e-01 -7.89541900e-02 -2.36599445e-02 ... -1.36775702e-01\n",
      "    -7.89541900e-02 -2.36599445e-02]]\n",
      "\n",
      "  [[ 5.15468717e-02  1.37298316e-01  1.73551172e-01 ...  5.15468717e-02\n",
      "     1.37298316e-01  1.73551172e-01]\n",
      "   [ 1.13899827e-01  2.25446790e-01  2.55686462e-01 ...  1.13899827e-01\n",
      "     2.25446790e-01  2.55686462e-01]\n",
      "   [ 9.35511589e-02  2.07843184e-01  2.43050188e-01 ...  9.35511589e-02\n",
      "     2.07843184e-01  2.43050188e-01]\n",
      "   ...\n",
      "   [ 8.45751762e-02  1.53202474e-01  2.07145929e-01 ...  8.45751762e-02\n",
      "     1.53202474e-01  2.07145929e-01]\n",
      "   [ 1.02222085e-01  1.70849562e-01  2.17167795e-01 ...  1.02222085e-01\n",
      "     1.70849562e-01  2.17167795e-01]\n",
      "   [ 1.21569633e-02  6.94988668e-02  1.12984896e-01 ...  1.21569633e-02\n",
      "     6.94988668e-02  1.12984896e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.12766862e-01  1.95424736e-01  2.32592642e-01 ...  1.12766862e-01\n",
      "     1.95424736e-01  2.32592642e-01]\n",
      "   [ 1.13115430e-01  1.94727719e-01  2.29455471e-01 ...  1.13115430e-01\n",
      "     1.94727719e-01  2.29455471e-01]\n",
      "   [ 1.02483630e-01  1.89411938e-01  2.29368269e-01 ...  1.02483630e-01\n",
      "     1.89411938e-01  2.29368269e-01]\n",
      "   ...\n",
      "   [ 8.98038745e-02  1.53202653e-01  1.99215770e-01 ...  8.98038745e-02\n",
      "     1.53202653e-01  1.99215770e-01]\n",
      "   [ 8.53594542e-02  1.37211323e-01  1.81960702e-01 ...  8.53594542e-02\n",
      "     1.37211323e-01  1.81960702e-01]\n",
      "   [ 5.01524806e-02  9.88671780e-02  1.39302969e-01 ...  5.01524806e-02\n",
      "     9.88671780e-02  1.39302969e-01]]\n",
      "\n",
      "  [[ 8.32679868e-02  1.84575230e-01  2.37429261e-01 ...  8.32679868e-02\n",
      "     1.84575230e-01  2.37429261e-01]\n",
      "   [ 1.14814758e-01  1.95512056e-01  2.29455441e-01 ...  1.14814758e-01\n",
      "     1.95512056e-01  2.29455441e-01]\n",
      "   [ 1.02222204e-01  1.94596946e-01  2.30239749e-01 ...  1.02222204e-01\n",
      "     1.94596946e-01  2.30239749e-01]\n",
      "   ...\n",
      "   [ 9.18953419e-02  1.53333306e-01  1.96732044e-01 ...  9.18953419e-02\n",
      "     1.53333306e-01  1.96732044e-01]\n",
      "   [ 9.86055732e-02  1.57124162e-01  1.98780060e-01 ...  9.86055732e-02\n",
      "     1.57124162e-01  1.98780060e-01]\n",
      "   [-8.95861089e-02 -3.36819589e-02 -1.74283981e-04 ... -8.95861089e-02\n",
      "    -3.36819589e-02 -1.74283981e-04]]\n",
      "\n",
      "  [[ 6.79737329e-03  1.41612351e-01  2.16688544e-01 ...  6.79737329e-03\n",
      "     1.41612351e-01  2.16688544e-01]\n",
      "   [ 1.12461805e-01  1.91634059e-01  2.32113332e-01 ...  1.12461805e-01\n",
      "     1.91634059e-01  2.32113332e-01]\n",
      "   [ 1.07712388e-01  2.08714545e-01  2.48845279e-01 ...  1.07712388e-01\n",
      "     2.08714545e-01  2.48845279e-01]\n",
      "   ...\n",
      "   [ 9.84313488e-02  1.58779979e-01  2.04880118e-01 ...  9.84313488e-02\n",
      "     1.58779979e-01  2.04880118e-01]\n",
      "   [ 9.91721153e-02  1.55512005e-01  1.95642859e-01 ...  9.91721153e-02\n",
      "     1.55512005e-01  1.95642859e-01]\n",
      "   [-2.35991269e-01 -1.75599232e-01 -1.44618869e-01 ... -2.35991269e-01\n",
      "    -1.75599232e-01 -1.44618869e-01]]]\n",
      "\n",
      "\n",
      " [[[ 1.64008826e-01  1.33507848e-01  7.60784149e-02 ...  1.64008826e-01\n",
      "     1.33507848e-01  7.60784149e-02]\n",
      "   [ 5.38997650e-02  6.16556406e-02  1.69062614e-02 ...  5.38997650e-02\n",
      "     6.16556406e-02  1.69062614e-02]\n",
      "   [ 1.94336176e-02  6.26580715e-02  4.57953513e-02 ...  1.94336176e-02\n",
      "     6.26580715e-02  4.57953513e-02]\n",
      "   ...\n",
      "   [ 3.39868069e-02  5.84314466e-02  4.07407880e-02 ...  3.39868069e-02\n",
      "     5.84314466e-02  4.07407880e-02]\n",
      "   [ 1.11503303e-01  1.36514097e-01  1.31634176e-01 ...  1.11503303e-01\n",
      "     1.36514097e-01  1.31634176e-01]\n",
      "   [ 2.42396533e-01  2.55032808e-01  2.70631939e-01 ...  2.42396533e-01\n",
      "     2.55032808e-01  2.70631939e-01]]\n",
      "\n",
      "  [[ 1.15032613e-01  8.89326036e-02  2.67539620e-02 ...  1.15032613e-01\n",
      "     8.89326036e-02  2.67539620e-02]\n",
      "   [ 3.28104496e-02  5.26798666e-02  2.15686262e-02 ...  3.28104496e-02\n",
      "     5.26798666e-02  2.15686262e-02]\n",
      "   [ 3.96531820e-03  4.16994095e-02  1.35077238e-02 ...  3.96531820e-03\n",
      "     4.16994095e-02  1.35077238e-02]\n",
      "   ...\n",
      "   [ 2.21350193e-02  5.01959920e-02  4.44445610e-02 ...  2.21350193e-02\n",
      "     5.01959920e-02  4.44445610e-02]\n",
      "   [ 7.20695257e-02  8.40958357e-02  6.46187067e-02 ...  7.20695257e-02\n",
      "     8.40958357e-02  6.46187067e-02]\n",
      "   [ 2.12243915e-01  2.34771311e-01  2.54771441e-01 ...  2.12243915e-01\n",
      "     2.34771311e-01  2.54771441e-01]]\n",
      "\n",
      "  [[ 3.58605981e-02 -3.87817621e-03 -7.74292350e-02 ...  3.58605981e-02\n",
      "    -3.87817621e-03 -7.74292350e-02]\n",
      "   [ 2.37037539e-02  6.07409179e-02  4.00002003e-02 ...  2.37037539e-02\n",
      "     6.07409179e-02  4.00002003e-02]\n",
      "   [-2.40958929e-02 -7.84307718e-03 -5.89106083e-02 ... -2.40958929e-02\n",
      "    -7.84307718e-03 -5.89106083e-02]\n",
      "   ...\n",
      "   [ 1.39869452e-02  4.73201275e-02  5.42047620e-02 ...  1.39869452e-02\n",
      "     4.73201275e-02  5.42047620e-02]\n",
      "   [ 1.98691487e-02  1.00652575e-02 -3.38126123e-02 ...  1.98691487e-02\n",
      "     1.00652575e-02 -3.38126123e-02]\n",
      "   [ 1.45490289e-01  1.51851803e-01  1.44357443e-01 ...  1.45490289e-01\n",
      "     1.51851803e-01  1.44357443e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.87276661e-01  2.42483556e-01  2.60043621e-01 ...  1.87276661e-01\n",
      "     2.42483556e-01  2.60043621e-01]\n",
      "   [ 1.13115430e-01  1.67276740e-01  1.70631945e-01 ...  1.13115430e-01\n",
      "     1.67276740e-01  1.70631945e-01]\n",
      "   [ 1.62091255e-02  3.25492024e-02 -9.84740257e-03 ...  1.62091255e-02\n",
      "     3.25492024e-02 -9.84740257e-03]\n",
      "   ...\n",
      "   [ 5.45097589e-02  8.26144218e-02  5.80393076e-02 ...  5.45097589e-02\n",
      "     8.26144218e-02  5.80393076e-02]\n",
      "   [ 7.35947490e-02  1.09760344e-01  8.39214921e-02 ...  7.35947490e-02\n",
      "     1.09760344e-01  8.39214921e-02]\n",
      "   [ 1.20740712e-01  1.38082862e-01  1.11851990e-01 ...  1.20740712e-01\n",
      "     1.38082862e-01  1.11851990e-01]]\n",
      "\n",
      "  [[ 1.85228765e-01  2.12026209e-01  2.13899851e-01 ...  1.85228765e-01\n",
      "     2.12026209e-01  2.13899851e-01]\n",
      "   [ 1.46187305e-01  2.11198330e-01  2.29455441e-01 ...  1.46187305e-01\n",
      "     2.11198330e-01  2.29455441e-01]\n",
      "   [ 2.37908363e-02  3.77342105e-02 -1.28975213e-02 ...  2.37908363e-02\n",
      "     3.77342105e-02 -1.28975213e-02]\n",
      "   ...\n",
      "   [ 6.05227947e-02  9.05882120e-02  5.94771504e-02 ...  6.05227947e-02\n",
      "     9.05882120e-02  5.94771504e-02]\n",
      "   [ 8.29192996e-02  1.10065341e-01  7.32898712e-02 ...  8.29192996e-02\n",
      "     1.10065341e-01  7.32898712e-02]\n",
      "   [ 1.41786456e-01  1.54553354e-01  1.17472798e-01 ...  1.41786456e-01\n",
      "     1.54553354e-01  1.17472798e-01]]\n",
      "\n",
      "  [[ 1.95032656e-01  1.92592740e-01  1.89237565e-01 ...  1.95032656e-01\n",
      "     1.92592740e-01  1.89237565e-01]\n",
      "   [ 1.75206900e-01  2.38692880e-01  2.67407447e-01 ...  1.75206900e-01\n",
      "     2.38692880e-01  2.67407447e-01]\n",
      "   [ 6.06535673e-02  9.10674930e-02  5.66884279e-02 ...  6.06535673e-02\n",
      "     9.10674930e-02  5.66884279e-02]\n",
      "   ...\n",
      "   [ 6.31372333e-02  7.64270425e-02  3.62526774e-02 ...  6.31372333e-02\n",
      "     7.64270425e-02  3.62526774e-02]\n",
      "   [ 1.03093684e-01  1.24139458e-01  8.97605121e-02 ...  1.03093684e-01\n",
      "     1.24139458e-01  8.97605121e-02]\n",
      "   [ 1.71851873e-01  1.69498831e-01  1.33812517e-01 ...  1.71851873e-01\n",
      "     1.69498831e-01  1.33812517e-01]]]\n",
      "\n",
      "\n",
      " [[[ 1.75773531e-01  1.76645100e-01  1.70196086e-01 ...  1.75773531e-01\n",
      "     1.76645100e-01  1.70196086e-01]\n",
      "   [ 1.59782112e-01  1.24400735e-01  1.03180796e-01 ...  1.59782112e-01\n",
      "     1.24400735e-01  1.03180796e-01]\n",
      "   [ 7.04140067e-02  5.87365031e-02  5.36384881e-02 ...  7.04140067e-02\n",
      "     5.87365031e-02  5.36384881e-02]\n",
      "   ...\n",
      "   [ 8.49671960e-02  5.05883098e-02  3.28976512e-02 ...  8.49671960e-02\n",
      "     5.05883098e-02  3.28976512e-02]\n",
      "   [ 1.27189577e-01  8.55337083e-02  6.88890815e-02 ...  1.27189577e-01\n",
      "     8.55337083e-02  6.88890815e-02]\n",
      "   [-3.60348523e-02 -5.86926937e-02 -7.05445409e-02 ... -3.60348523e-02\n",
      "    -5.86926937e-02 -7.05445409e-02]]\n",
      "\n",
      "  [[ 2.01307118e-01  1.67364001e-01  1.48322612e-01 ...  2.01307118e-01\n",
      "     1.67364001e-01  1.48322612e-01]\n",
      "   [ 1.38692796e-01  1.15424961e-01  1.00000024e-01 ...  1.38692796e-01\n",
      "     1.15424961e-01  1.00000024e-01]\n",
      "   [ 5.88672757e-02  4.56209779e-02  4.88018692e-02 ...  5.88672757e-02\n",
      "     4.56209779e-02  4.88018692e-02]\n",
      "   ...\n",
      "   [ 7.70369768e-02  4.62744236e-02  2.87582874e-02 ...  7.70369768e-02\n",
      "     4.62744236e-02  2.87582874e-02]\n",
      "   [ 9.95205045e-02  6.44879937e-02  4.89324331e-02 ...  9.95205045e-02\n",
      "     6.44879937e-02  4.89324331e-02]\n",
      "   [ 9.06752944e-02  4.65360284e-02  3.12420130e-02 ...  9.06752944e-02\n",
      "     4.65360284e-02  3.12420130e-02]]\n",
      "\n",
      "  [[ 1.84880197e-01  1.64749295e-01  1.50021762e-01 ...  1.84880197e-01\n",
      "     1.64749295e-01  1.50021762e-01]\n",
      "   [ 1.06056690e-01  1.03878170e-01  9.88237262e-02 ...  1.06056690e-01\n",
      "     1.03878170e-01  9.88237262e-02]\n",
      "   [ 5.43354750e-02  4.31373119e-02  3.91286314e-02 ...  5.43354750e-02\n",
      "     4.31373119e-02  3.91286314e-02]\n",
      "   ...\n",
      "   [ 7.28104711e-02  3.94769907e-02  1.89106464e-02 ...  7.28104711e-02\n",
      "     3.94769907e-02  1.89106464e-02]\n",
      "   [ 7.86926746e-02  4.14378047e-02  1.71678066e-02 ...  7.86926746e-02\n",
      "     4.14378047e-02  1.71678066e-02]\n",
      "   [ 1.18039310e-01  8.51851404e-02  7.76907802e-02 ...  1.18039310e-01\n",
      "     8.51851404e-02  7.76907802e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.12766862e-01  8.56208205e-02  7.57299066e-02 ...  1.12766862e-01\n",
      "     8.56208205e-02  7.57299066e-02]\n",
      "   [ 6.21350408e-02  2.61002779e-02  2.00450420e-03 ...  6.21350408e-02\n",
      "     2.61002779e-02  2.00450420e-03]\n",
      "   [ 7.89542198e-02  4.82354760e-02  3.72114182e-02 ...  7.89542198e-02\n",
      "     4.82354760e-02  3.72114182e-02]\n",
      "   ...\n",
      "   [ 9.76470113e-02  3.16340327e-02 -1.64705217e-02 ...  9.76470113e-02\n",
      "     3.16340327e-02 -1.64705217e-02]\n",
      "   [ 9.71241593e-02  3.52505445e-02 -6.27461076e-03 ...  9.71241593e-02\n",
      "     3.52505445e-02 -6.27461076e-03]\n",
      "   [ 1.24662280e-01  8.31809044e-02  5.30284643e-02 ...  1.24662280e-01\n",
      "     8.31809044e-02  5.30284643e-02]]\n",
      "\n",
      "  [[ 1.14640534e-01  7.86928833e-02  7.27233887e-02 ...  1.14640534e-01\n",
      "     7.86928833e-02  7.27233887e-02]\n",
      "   [ 7.95206428e-02  5.43355942e-02  4.51417267e-02 ...  7.95206428e-02\n",
      "     5.43355942e-02  4.51417267e-02]\n",
      "   [ 8.26143622e-02  4.94989157e-02  3.41613293e-02 ...  8.26143622e-02\n",
      "     4.94989157e-02  3.41613293e-02]\n",
      "   ...\n",
      "   [ 9.97384787e-02  3.17646861e-02 -1.11111104e-02 ...  9.97384787e-02\n",
      "     3.17646861e-02 -1.11111104e-02]\n",
      "   [ 1.06448710e-01  6.30065203e-02  3.01526189e-02 ...  1.06448710e-01\n",
      "     6.30065203e-02  3.01526189e-02]\n",
      "   [ 1.30021751e-01  7.22004175e-02  4.68845367e-02 ...  1.30021751e-01\n",
      "     7.22004175e-02  4.68845367e-02]]\n",
      "\n",
      "  [[ 1.24444425e-01  8.27888250e-02  7.15905130e-02 ...  1.24444425e-01\n",
      "     8.27888250e-02  7.15905130e-02]\n",
      "   [ 8.50108266e-02  5.43791652e-02  4.38780487e-02 ...  8.50108266e-02\n",
      "     5.43791652e-02  4.38780487e-02]\n",
      "   [ 7.63398409e-02  2.83223987e-02  9.62960720e-03 ...  7.63398409e-02\n",
      "     2.83223987e-02  9.62960720e-03]\n",
      "   ...\n",
      "   [ 1.10196054e-01  5.68192005e-02  3.23311090e-02 ...  1.10196054e-01\n",
      "     5.68192005e-02  3.23311090e-02]\n",
      "   [ 1.30544662e-01  9.66884792e-02  7.79958069e-02 ...  1.30544662e-01\n",
      "     9.66884792e-02  7.79958069e-02]\n",
      "   [ 1.44400895e-01  7.53811598e-02  4.36164141e-02 ...  1.44400895e-01\n",
      "     7.53811598e-02  4.36164141e-02]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-3.53638262e-01 -2.86100030e-01 -2.61176497e-01 ... -3.53638262e-01\n",
      "    -2.86100030e-01 -2.61176497e-01]\n",
      "   [-3.26492429e-01 -2.75599301e-01 -2.45838836e-01 ... -3.26492429e-01\n",
      "    -2.75599301e-01 -2.45838836e-01]\n",
      "   [-2.19782084e-01 -1.72636062e-01 -1.11067414e-01 ... -2.19782084e-01\n",
      "    -1.72636062e-01 -1.11067414e-01]\n",
      "   ...\n",
      "   [-1.69934779e-01 -1.29803866e-01 -5.33768833e-02 ... -1.69934779e-01\n",
      "    -1.29803866e-01 -5.33768833e-02]\n",
      "   [-2.64967293e-01 -2.55642772e-01 -2.17385441e-01 ... -2.64967293e-01\n",
      "    -2.55642772e-01 -2.17385441e-01]\n",
      "   [-2.94858396e-01 -2.93986797e-01 -2.94073939e-01 ... -2.94858396e-01\n",
      "    -2.93986797e-01 -2.94073939e-01]]\n",
      "\n",
      "  [[-3.51634085e-01 -2.87537992e-01 -2.63442099e-01 ... -3.51634085e-01\n",
      "    -2.87537992e-01 -2.63442099e-01]\n",
      "   [-2.61307210e-01 -2.25751519e-01 -1.86274499e-01 ... -2.61307210e-01\n",
      "    -2.25751519e-01 -1.86274499e-01]\n",
      "   [-1.41132742e-01 -8.37908089e-02 -6.10011816e-03 ... -1.41132742e-01\n",
      "    -8.37908089e-02 -6.10011816e-03]\n",
      "   ...\n",
      "   [-1.73943430e-01 -1.30196184e-01 -6.14378154e-02 ... -1.73943430e-01\n",
      "    -1.30196184e-01 -6.14378154e-02]\n",
      "   [-2.14204997e-01 -1.90413982e-01 -1.35381311e-01 ... -2.14204997e-01\n",
      "    -1.90413982e-01 -1.35381311e-01]\n",
      "   [-2.77952164e-01 -2.71111071e-01 -2.58954048e-01 ... -2.77952164e-01\n",
      "    -2.71111071e-01 -2.58954048e-01]]\n",
      "\n",
      "  [[-3.21002185e-01 -2.74466395e-01 -2.53899813e-01 ... -3.21002185e-01\n",
      "    -2.74466395e-01 -2.53899813e-01]\n",
      "   [-1.99825674e-01 -1.66710079e-01 -1.16862565e-01 ... -1.99825674e-01\n",
      "    -1.66710079e-01 -1.16862565e-01]\n",
      "   [-1.14291966e-01 -5.09803593e-02  2.73639262e-02 ... -1.14291966e-01\n",
      "    -5.09803593e-02  2.73639262e-02]\n",
      "   ...\n",
      "   [-1.78169936e-01 -1.36993617e-01 -7.52070248e-02 ... -1.78169936e-01\n",
      "    -1.36993617e-01 -7.52070248e-02]\n",
      "   [-1.76209301e-01 -1.38954371e-01 -7.69498646e-02 ... -1.76209301e-01\n",
      "    -1.38954371e-01 -7.69498646e-02]\n",
      "   [-2.42745012e-01 -2.28540361e-01 -2.04662174e-01 ... -2.42745012e-01\n",
      "    -2.28540361e-01 -2.04662174e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-3.53899837e-01 -3.26143920e-01 -3.43877971e-01 ... -3.53899837e-01\n",
      "    -3.26143920e-01 -3.43877971e-01]\n",
      "   [-1.84923798e-01 -1.62135035e-01 -1.39171988e-01 ... -1.84923798e-01\n",
      "    -1.62135035e-01 -1.39171988e-01]\n",
      "   [-1.17124200e-01 -6.54900372e-02 -1.76905692e-02 ... -1.17124200e-01\n",
      "    -6.54900372e-02 -1.76905692e-02]\n",
      "   ...\n",
      "   [-9.84314084e-02 -5.85620701e-02 -7.84218311e-04 ... -9.84314084e-02\n",
      "    -5.85620701e-02 -7.84218311e-04]\n",
      "   [-1.49934679e-01 -1.05925947e-01 -5.72549999e-02 ... -1.49934679e-01\n",
      "    -1.05925947e-01 -5.72549999e-02]\n",
      "   [-2.28278905e-01 -2.07015187e-01 -1.86187238e-01 ... -2.28278905e-01\n",
      "    -2.07015187e-01 -1.86187238e-01]]\n",
      "\n",
      "  [[-4.46143806e-01 -3.91895384e-01 -3.90021741e-01 ... -4.46143806e-01\n",
      "    -3.91895384e-01 -3.90021741e-01]\n",
      "   [-2.42047995e-01 -2.24095792e-01 -2.09760249e-01 ... -2.42047995e-01\n",
      "    -2.24095792e-01 -2.09760249e-01]\n",
      "   [-1.40915036e-01 -9.16775763e-02 -4.03485000e-02 ... -1.40915036e-01\n",
      "    -9.16775763e-02 -4.03485000e-02]\n",
      "   ...\n",
      "   [-1.27712488e-01 -9.76471007e-02 -4.24836576e-02 ... -1.27712488e-01\n",
      "    -9.76471007e-02 -4.24836576e-02]\n",
      "   [-1.83747381e-01 -1.60522908e-01 -1.18867010e-01 ... -1.83747381e-01\n",
      "    -1.60522908e-01 -1.18867010e-01]\n",
      "   [-2.58213550e-01 -2.41525099e-01 -2.27625266e-01 ... -2.58213550e-01\n",
      "    -2.41525099e-01 -2.27625266e-01]]\n",
      "\n",
      "  [[-4.79477167e-01 -4.11328852e-01 -4.02919322e-01 ... -4.79477167e-01\n",
      "    -4.11328852e-01 -4.02919322e-01]\n",
      "   [-3.34597021e-01 -3.06405187e-01 -3.05141568e-01 ... -3.34597021e-01\n",
      "    -3.06405187e-01 -3.05141568e-01]\n",
      "   [-1.78562135e-01 -1.40305072e-01 -9.62527692e-02 ... -1.78562135e-01\n",
      "    -1.40305072e-01 -9.62527692e-02]\n",
      "   ...\n",
      "   [-1.56470627e-01 -1.35337681e-01 -8.53159726e-02 ... -1.56470627e-01\n",
      "    -1.35337681e-01 -8.53159726e-02]\n",
      "   [-2.14553386e-01 -1.93507612e-01 -1.65141463e-01 ... -2.14553386e-01\n",
      "    -1.93507612e-01 -1.65141463e-01]\n",
      "   [-2.75206983e-01 -2.61873722e-01 -2.58344352e-01 ... -2.75206983e-01\n",
      "    -2.61873722e-01 -2.58344352e-01]]]\n",
      "\n",
      "\n",
      " [[[-3.45795125e-01 -2.78256893e-01 -2.41568655e-01 ... -3.45795125e-01\n",
      "    -2.78256893e-01 -2.41568655e-01]\n",
      "   [-2.48061031e-01 -2.05011040e-01 -1.47799611e-01 ... -2.48061031e-01\n",
      "    -2.05011040e-01 -1.47799611e-01]\n",
      "   [-9.03702974e-02 -1.05969399e-01 -7.18517303e-02 ... -9.03702974e-02\n",
      "    -1.05969399e-01 -7.18517303e-02]\n",
      "   ...\n",
      "   [ 7.71240592e-02  2.70588994e-02  6.42701983e-02 ...  7.71240592e-02\n",
      "     2.70588994e-02  6.42701983e-02]\n",
      "   [ 2.13072300e-02 -3.99565101e-02 -1.73854530e-02 ...  2.13072300e-02\n",
      "    -3.99565101e-02 -1.73854530e-02]\n",
      "   [-9.09368098e-02 -1.52810335e-01 -1.41132772e-01 ... -9.09368098e-02\n",
      "    -1.52810335e-01 -1.41132772e-01]]\n",
      "\n",
      "  [[-3.12418401e-01 -2.32636020e-01 -1.77167609e-01 ... -3.12418401e-01\n",
      "    -2.32636020e-01 -1.77167609e-01]\n",
      "   [-1.71111137e-01 -1.59084857e-01 -1.23529404e-01 ... -1.71111137e-01\n",
      "    -1.59084857e-01 -1.23529404e-01]\n",
      "   [-4.30935025e-02 -5.63398302e-02 -2.17863917e-02 ... -4.30935025e-02\n",
      "    -5.63398302e-02 -2.17863917e-02]\n",
      "   ...\n",
      "   [ 8.88016820e-02  3.84312868e-02  7.18955398e-02 ...  8.88016820e-02\n",
      "     3.84312868e-02  7.18955398e-02]\n",
      "   [ 5.24616838e-02 -2.17866898e-03  1.75598860e-02 ...  5.24616838e-02\n",
      "    -2.17866898e-03  1.75598860e-02]\n",
      "   [-3.87364924e-02 -9.46404636e-02 -7.46403337e-02 ... -3.87364924e-02\n",
      "    -9.46404636e-02 -7.46403337e-02]]\n",
      "\n",
      "  [[-2.93551207e-01 -2.23486021e-01 -1.63703740e-01 ... -2.93551207e-01\n",
      "    -2.23486021e-01 -1.63703740e-01]\n",
      "   [-1.21394306e-01 -1.19651258e-01 -8.15684497e-02 ... -1.21394306e-01\n",
      "    -1.19651258e-01 -8.15684497e-02]\n",
      "   [ 1.11982226e-02  3.92162800e-03  5.08933365e-02 ...  1.11982226e-02\n",
      "     3.92162800e-03  5.08933365e-02]\n",
      "   ...\n",
      "   [ 1.15947723e-01  7.47711062e-02  1.16949856e-01 ...  1.15947723e-01\n",
      "     7.47711062e-02  1.16949856e-01]\n",
      "   [ 9.04573798e-02  4.92809415e-02  6.81481957e-02 ...  9.04573798e-02\n",
      "     4.92809415e-02  6.81481957e-02]\n",
      "   [ 2.78432369e-02 -8.93253088e-03 -8.58375430e-03 ...  2.78432369e-02\n",
      "    -8.93253088e-03 -8.58375430e-03]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-2.51939029e-01 -2.83006668e-01 -2.85054445e-01 ... -2.51939029e-01\n",
      "    -2.83006668e-01 -2.85054445e-01]\n",
      "   [-2.00610071e-01 -3.07233095e-01 -2.96034753e-01 ... -2.00610071e-01\n",
      "    -3.07233095e-01 -2.96034753e-01]\n",
      "   [-9.75163579e-02 -1.32156700e-01 -8.04356635e-02 ... -9.75163579e-02\n",
      "    -1.32156700e-01 -8.04356635e-02]\n",
      "   ...\n",
      "   [-3.25882405e-01 -3.33071887e-01 -3.22352886e-01 ... -3.25882405e-01\n",
      "    -3.33071887e-01 -3.22352886e-01]\n",
      "   [-4.16601360e-01 -4.19651449e-01 -4.10196185e-01 ... -4.16601360e-01\n",
      "    -4.19651449e-01 -4.10196185e-01]\n",
      "   [-4.20435786e-01 -4.14858341e-01 -4.01873529e-01 ... -4.20435786e-01\n",
      "    -4.14858341e-01 -4.01873529e-01]]\n",
      "\n",
      "  [[-3.36339891e-01 -3.09542418e-01 -2.95904100e-01 ... -3.36339891e-01\n",
      "    -3.09542418e-01 -2.95904100e-01]\n",
      "   [-2.49891132e-01 -3.45664442e-01 -3.50936711e-01 ... -2.49891132e-01\n",
      "    -3.45664442e-01 -3.50936711e-01]\n",
      "   [-1.60522908e-01 -2.28932470e-01 -1.89368099e-01 ... -1.60522908e-01\n",
      "    -2.28932470e-01 -1.89368099e-01]\n",
      "   ...\n",
      "   [-4.10065472e-01 -4.15294170e-01 -4.03267980e-01 ... -4.10065472e-01\n",
      "    -4.15294170e-01 -4.03267980e-01]\n",
      "   [-4.42570925e-01 -4.35032725e-01 -4.24749374e-01 ... -4.42570925e-01\n",
      "    -4.35032725e-01 -4.24749374e-01]\n",
      "   [-3.99390042e-01 -3.82701576e-01 -3.72723311e-01 ... -3.99390042e-01\n",
      "    -3.82701576e-01 -3.72723311e-01]]\n",
      "\n",
      "  [[-4.01045799e-01 -3.44662189e-01 -3.24487925e-01 ... -4.01045799e-01\n",
      "    -3.44662189e-01 -3.24487925e-01]\n",
      "   [-2.56165653e-01 -3.06405187e-01 -3.12984705e-01 ... -2.56165653e-01\n",
      "    -3.06405187e-01 -3.12984705e-01]\n",
      "   [-2.33464092e-01 -3.32461953e-01 -3.23703766e-01 ... -2.33464092e-01\n",
      "    -3.32461953e-01 -3.23703766e-01]\n",
      "   ...\n",
      "   [-4.38823581e-01 -4.41220045e-01 -4.22570884e-01 ... -4.38823581e-01\n",
      "    -4.41220045e-01 -4.22570884e-01]\n",
      "   [-4.45925951e-01 -4.28801745e-01 -4.12200302e-01 ... -4.45925951e-01\n",
      "    -4.28801745e-01 -4.12200302e-01]\n",
      "   [-3.61481488e-01 -3.48148257e-01 -3.36775750e-01 ... -3.61481488e-01\n",
      "    -3.48148257e-01 -3.36775750e-01]]]\n",
      "\n",
      "\n",
      " [[[ 3.44400972e-01  1.64880395e-01  9.56862569e-02 ...  3.44400972e-01\n",
      "     1.64880395e-01  9.56862569e-02]\n",
      "   [ 1.98997796e-01  6.16556406e-02  9.06312466e-03 ...  1.98997796e-01\n",
      "     6.16556406e-02  9.06312466e-03]\n",
      "   [ 7.66891241e-03 -7.45968521e-02 -1.11067414e-01 ...  7.66891241e-03\n",
      "    -7.45968521e-02 -1.11067414e-01]\n",
      "   ...\n",
      "   [ 3.39869142e-01  2.97647119e-01  2.36819208e-01 ...  3.39869142e-01\n",
      "     2.97647119e-01  2.36819208e-01]\n",
      "   [ 4.13464069e-01  4.26710159e-01  3.43398869e-01 ...  4.13464069e-01\n",
      "     4.26710159e-01  3.43398869e-01]\n",
      "   [ 5.01220047e-01  5.33464193e-01  4.35337812e-01 ...  5.01220047e-01\n",
      "     5.33464193e-01  4.35337812e-01]]\n",
      "\n",
      "  [[ 2.52287507e-01  1.04618907e-01  4.63618040e-02 ...  2.52287507e-01\n",
      "     1.04618907e-01  4.63618040e-02]\n",
      "   [ 9.55555439e-02 -1.39868259e-02 -6.47058785e-02 ...  9.55555439e-02\n",
      "    -1.39868259e-02 -6.47058785e-02]\n",
      "   [-2.74072289e-02 -7.98692405e-02 -8.84530544e-02 ... -2.74072289e-02\n",
      "    -7.98692405e-02 -8.84530544e-02]\n",
      "   ...\n",
      "   [ 2.84880102e-01  2.22745001e-01  1.89542592e-01 ...  2.84880102e-01\n",
      "     2.22745001e-01  1.89542592e-01]\n",
      "   [ 3.81873429e-01  4.09586012e-01  3.39128494e-01 ...  3.81873429e-01\n",
      "     4.09586012e-01  3.39128494e-01]\n",
      "   [ 4.67145860e-01  4.85751688e-01  3.92026335e-01 ...  4.67145860e-01\n",
      "     4.85751688e-01  3.92026335e-01]]\n",
      "\n",
      "  [[ 1.61350787e-01  4.71022129e-02  1.00213289e-03 ...  1.61350787e-01\n",
      "     4.71022129e-02  1.00213289e-03]\n",
      "   [ 1.58606172e-02 -7.65140057e-02 -1.20784134e-01 ...  1.58606172e-02\n",
      "    -7.65140057e-02 -1.20784134e-01]\n",
      "   [-1.02527261e-01 -1.60784274e-01 -1.72636092e-01 ... -1.02527261e-01\n",
      "    -1.60784274e-01 -1.72636092e-01]\n",
      "   ...\n",
      "   [ 2.76732028e-01  2.00261295e-01  1.67930245e-01 ...  2.76732028e-01\n",
      "     2.00261295e-01  1.67930245e-01]\n",
      "   [ 3.53202462e-01  3.59084845e-01  2.95599163e-01 ...  3.53202462e-01\n",
      "     3.59084845e-01  2.95599163e-01]\n",
      "   [ 4.27843213e-01  4.30283159e-01  3.48278999e-01 ...  4.27843213e-01\n",
      "     4.30283159e-01  3.48278999e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1.14684135e-01 -2.20261544e-01 -2.26230890e-01 ... -1.14684135e-01\n",
      "    -2.20261544e-01 -2.26230890e-01]\n",
      "   [-1.10413969e-01 -1.26840919e-01 -7.25053251e-02 ... -1.10413969e-01\n",
      "    -1.26840919e-01 -7.25053251e-02]\n",
      "   [-1.95555598e-01 -2.26274341e-01 -1.82396442e-01 ... -1.95555598e-01\n",
      "    -2.26274341e-01 -1.82396442e-01]\n",
      "   ...\n",
      "   [ 3.44705820e-01  3.68888915e-01  3.16862822e-01 ...  3.44705820e-01\n",
      "     3.68888915e-01  3.16862822e-01]\n",
      "   [ 3.63790810e-01  4.47015226e-01  3.89803827e-01 ...  3.63790810e-01\n",
      "     4.47015226e-01  3.89803827e-01]\n",
      "   [ 4.22701478e-01  4.55729902e-01  3.58910799e-01 ...  4.22701478e-01\n",
      "     4.55729902e-01  3.58910799e-01]]\n",
      "\n",
      "  [[-6.57516122e-02 -1.99738503e-01 -2.44923696e-01 ... -6.57516122e-02\n",
      "    -1.99738503e-01 -2.44923696e-01]\n",
      "   [-1.36165649e-01 -2.12331086e-01 -1.94073975e-01 ... -1.36165649e-01\n",
      "    -2.12331086e-01 -1.94073975e-01]\n",
      "   [-1.64444476e-01 -1.74030513e-01 -1.18779868e-01 ... -1.64444476e-01\n",
      "    -1.74030513e-01 -1.18779868e-01]\n",
      "   ...\n",
      "   [ 3.54640424e-01  4.35686231e-01  3.88888896e-01 ...  3.54640424e-01\n",
      "     4.35686231e-01  3.88888896e-01]\n",
      "   [ 3.77036929e-01  4.51241791e-01  3.75250638e-01 ...  3.77036929e-01\n",
      "     4.51241791e-01  3.75250638e-01]\n",
      "   [ 4.63355064e-01  4.64357257e-01  3.44923764e-01 ...  4.63355064e-01\n",
      "     4.64357257e-01  3.44923764e-01]]\n",
      "\n",
      "  [[ 2.87580490e-03 -1.32897466e-01 -1.75468326e-01 ...  2.87580490e-03\n",
      "    -1.32897466e-01 -1.75468326e-01]\n",
      "   [-1.18910760e-01 -2.31895357e-01 -2.46318057e-01 ... -1.18910760e-01\n",
      "    -2.31895357e-01 -2.46318057e-01]\n",
      "   [-2.09934682e-01 -2.57952124e-01 -2.25664526e-01 ... -2.09934682e-01\n",
      "    -2.57952124e-01 -2.25664526e-01]\n",
      "   ...\n",
      "   [ 3.65098000e-01  4.52897608e-01  3.97036970e-01 ...  3.65098000e-01\n",
      "     4.52897608e-01  3.97036970e-01]\n",
      "   [ 4.05054450e-01  4.88845319e-01  4.42701668e-01 ...  4.05054450e-01\n",
      "     4.88845319e-01  4.42701668e-01]\n",
      "   [ 5.13028324e-01  5.61655641e-01  4.63224262e-01 ...  5.13028324e-01\n",
      "     5.61655641e-01  4.63224262e-01]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (90, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (90, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Learning rate:  0.0\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.9104 - accuracy: 0.1889"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-625644a2075e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfmeasures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0maccs\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprecisions\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-9e72c7eb0f94>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(pz)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;31m# Fit the model on the batches generated by datagen.flow().\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m         model.fit_generator(datagen.flow(x_train, y_train, batch_size = batch_size),\n\u001b[0m\u001b[0;32m    206\u001b[0m                             \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m                             \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1973\u001b[0m                   \u001b[1;34m'will be removed in a future version. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1974\u001b[0m                   'Please use `Model.fit`, which supports generators.')\n\u001b[1;32m-> 1975\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   1976\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1977\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1213\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1215\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1216\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_test_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1494\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1495\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1496\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1497\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32mc:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    694\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    717\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 719\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32mc:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3117\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3118\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3119\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   3120\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0;32m   3121\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accs=[]\n",
    "precisions = []\n",
    "recalls = []\n",
    "fmeasures = []\n",
    "for i in range(10):\n",
    "    ret = main(i)\n",
    "    accs+=[ret[0]]\n",
    "    precisions+=[ret[1]]\n",
    "    recalls+=[ret[2]]\n",
    "    fmeasures+=[ret[3]]\n",
    "print(accs)\n",
    "print(precisions)\n",
    "print(recalls)\n",
    "print(fmeasures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5675593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(accs))\n",
    "print('accs: ', accs)\n",
    "print('precisions: ', precisions)\n",
    "print('recalls: ', recalls)\n",
    "print('fmeasures: ', fmeasures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907dffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "#define sample data\n",
    "data = accs\n",
    "\n",
    "#create 95% confidence interval for population mean weight\n",
    "st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02434ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5bf6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "990f3e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs=  [0.3333333432674408, 0.5666666626930237, 0.5666666626930237, 0.4333333373069763, 0.4000000059604645, 0.46666666865348816, 0.4333333373069763, 0.46666666865348816, 0.5666666626930237, 0.6333333253860474]\n",
    "precisions=  [0.4483333333333333, 0.736904761904762, 0.6722222222222223, 0.43333333333333335, 0.7, 0.6844444444444444, 0.7225108225108224, 0.6288888888888889, 0.6428571428571428, 0.7252723311546841]\n",
    "recalls=  [0.3333333333333333, 0.5666666666666667, 0.5666666666666667, 0.43333333333333335, 0.4, 0.4666666666666667, 0.43333333333333335, 0.4666666666666667, 0.5666666666666667, 0.6333333333333333]\n",
    "fmeasures=  [0.36973039215686276, 0.637989417989418, 0.6140056022408964, 0.43333333333333335, 0.5066666666666666, 0.5473039215686274, 0.5414814814814815, 0.5333333333333333, 0.5830429732868756, 0.6759259259259259]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "276eca8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4199757689864667, 0.5533575659363238)\n",
      "(0.5605134999069246, 0.7184399562230022)\n",
      "(0.4199757642166506, 0.5533575691166828)\n",
      "(0.47819747292875353, 0.6103651366679308)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "#define sample data\n",
    "data = accs\n",
    "\n",
    "#create 95% confidence interval for population mean weight\n",
    "print(st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)))\n",
    "\n",
    "data = precisions\n",
    "\n",
    "#create 95% confidence interval for population mean weight\n",
    "print(st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)))\n",
    "\n",
    "data = recalls\n",
    "\n",
    "#create 95% confidence interval for population mean weight\n",
    "print(st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)))\n",
    "\n",
    "data = fmeasures\n",
    "\n",
    "#create 95% confidence interval for population mean weight\n",
    "print(st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edd9f401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "150\n",
      "234\n",
      "234\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "print(len(listCancer))\n",
    "print(len(listResults))\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(np.array(listCancer).reshape(len(listCancer),-1), listResults)\n",
    "print(len(X))\n",
    "print(len(y))\n",
    "listCancer = list(X)\n",
    "listResults = list(y)\n",
    "print(listResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a66da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
