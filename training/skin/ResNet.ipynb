{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a070c61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all imported\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "# calculates f1 for 1:100 dataset with 95tp, 5fn, 55fp\n",
    "from sklearn.metrics import f1_score\n",
    "print('all imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0419f3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[142 112  87]\n",
      "   [142 112  87]\n",
      "   [142 112  87]]\n",
      "\n",
      "  [[157 121  91]\n",
      "   [157 121  91]\n",
      "   [157 121  91]]\n",
      "\n",
      "  [[168 130  98]\n",
      "   [168 130  98]\n",
      "   [168 130  98]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[174 139 113]\n",
      "   [174 139 113]\n",
      "   [174 139 113]]\n",
      "\n",
      "  [[169 135 110]\n",
      "   [169 135 110]\n",
      "   [169 135 110]]\n",
      "\n",
      "  [[159 130 108]\n",
      "   [159 130 108]\n",
      "   [159 130 108]]]\n",
      "\n",
      "\n",
      " [[[148 115  89]\n",
      "   [148 115  89]\n",
      "   [148 115  89]]\n",
      "\n",
      "  [[161 123  95]\n",
      "   [161 123  95]\n",
      "   [161 123  95]]\n",
      "\n",
      "  [[170 133 103]\n",
      "   [170 133 103]\n",
      "   [170 133 103]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[175 139 114]\n",
      "   [175 139 114]\n",
      "   [175 139 114]]\n",
      "\n",
      "  [[171 136 112]\n",
      "   [171 136 112]\n",
      "   [171 136 112]]\n",
      "\n",
      "  [[164 132 111]\n",
      "   [164 132 111]\n",
      "   [164 132 111]]]\n",
      "\n",
      "\n",
      " [[[155 120  95]\n",
      "   [155 120  95]\n",
      "   [155 120  95]]\n",
      "\n",
      "  [[165 129 102]\n",
      "   [165 129 102]\n",
      "   [165 129 102]]\n",
      "\n",
      "  [[170 133 103]\n",
      "   [170 133 103]\n",
      "   [170 133 103]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[178 144 120]\n",
      "   [178 144 120]\n",
      "   [178 144 120]]\n",
      "\n",
      "  [[173 140 116]\n",
      "   [173 140 116]\n",
      "   [173 140 116]]\n",
      "\n",
      "  [[168 136 115]\n",
      "   [168 136 115]\n",
      "   [168 136 115]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[154 129 114]\n",
      "   [154 129 114]\n",
      "   [154 129 114]]\n",
      "\n",
      "  [[161 132 118]\n",
      "   [161 132 118]\n",
      "   [161 132 118]]\n",
      "\n",
      "  [[173 146 128]\n",
      "   [173 146 128]\n",
      "   [173 146 128]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[177 153 140]\n",
      "   [177 153 140]\n",
      "   [177 153 140]]\n",
      "\n",
      "  [[172 148 134]\n",
      "   [172 148 134]\n",
      "   [172 148 134]]\n",
      "\n",
      "  [[163 138 123]\n",
      "   [163 138 123]\n",
      "   [163 138 123]]]\n",
      "\n",
      "\n",
      " [[[153 130 115]\n",
      "   [153 130 115]\n",
      "   [153 130 115]]\n",
      "\n",
      "  [[160 133 117]\n",
      "   [160 133 117]\n",
      "   [160 133 117]]\n",
      "\n",
      "  [[172 145 128]\n",
      "   [172 145 128]\n",
      "   [172 145 128]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[173 149 135]\n",
      "   [173 149 135]\n",
      "   [173 149 135]]\n",
      "\n",
      "  [[168 145 131]\n",
      "   [168 145 131]\n",
      "   [168 145 131]]\n",
      "\n",
      "  [[158 135 119]\n",
      "   [158 135 119]\n",
      "   [158 135 119]]]\n",
      "\n",
      "\n",
      " [[[153 130 114]\n",
      "   [153 130 114]\n",
      "   [153 130 114]]\n",
      "\n",
      "  [[159 132 116]\n",
      "   [159 132 116]\n",
      "   [159 132 116]]\n",
      "\n",
      "  [[170 144 127]\n",
      "   [170 144 127]\n",
      "   [170 144 127]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[169 145 131]\n",
      "   [169 145 131]\n",
      "   [169 145 131]]\n",
      "\n",
      "  [[166 142 129]\n",
      "   [166 142 129]\n",
      "   [166 142 129]]\n",
      "\n",
      "  [[154 132 117]\n",
      "   [154 132 117]\n",
      "   [154 132 117]]]]\n"
     ]
    }
   ],
   "source": [
    "def readImage(filePath):\n",
    "    img = Image.open(filePath)\n",
    "    \n",
    "    size=(30,40)\n",
    "    #resize image\n",
    "    out = img.resize(size)\n",
    "\n",
    "    # asarray() class is used to convert\n",
    "    # PIL images into NumPy arrays\n",
    "    numpydata = asarray(out)\n",
    "    numpydata = np.repeat(numpydata[:, :, np.newaxis], 3, axis=2)\n",
    "    # <class 'numpy.ndarray'>\n",
    "    #print(type(numpydata))\n",
    "\n",
    "    #  shape\n",
    "    #print(numpydata.shape)\n",
    "    return numpydata\n",
    "print(readImage('data/nervus/ISIC_0001769.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8d30f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ISIC_0012099.jpg', 'ISIC_0012151.jpg', 'ISIC_0012288.jpg', 'ISIC_0012434.jpg', 'ISIC_0013232.jpg']\n"
     ]
    }
   ],
   "source": [
    "listFiles = os.listdir('data/melanoma/')\n",
    "print(listFiles[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1538036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[[ 54,  55,  58],\n",
      "         [ 54,  55,  58],\n",
      "         [ 54,  55,  58]],\n",
      "\n",
      "        [[168, 168, 169],\n",
      "         [168, 168, 169],\n",
      "         [168, 168, 169]],\n",
      "\n",
      "        [[184, 184, 185],\n",
      "         [184, 184, 185],\n",
      "         [184, 184, 185]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[190, 180, 183],\n",
      "         [190, 180, 183],\n",
      "         [190, 180, 183]],\n",
      "\n",
      "        [[172, 164, 169],\n",
      "         [172, 164, 169],\n",
      "         [172, 164, 169]],\n",
      "\n",
      "        [[ 66,  72,  89],\n",
      "         [ 66,  72,  89],\n",
      "         [ 66,  72,  89]]],\n",
      "\n",
      "\n",
      "       [[[ 99,  99, 102],\n",
      "         [ 99,  99, 102],\n",
      "         [ 99,  99, 102]],\n",
      "\n",
      "        [[179, 178, 179],\n",
      "         [179, 178, 179],\n",
      "         [179, 178, 179]],\n",
      "\n",
      "        [[185, 183, 184],\n",
      "         [185, 183, 184],\n",
      "         [185, 183, 184]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[192, 180, 184],\n",
      "         [192, 180, 184],\n",
      "         [192, 180, 184]],\n",
      "\n",
      "        [[184, 172, 175],\n",
      "         [184, 172, 175],\n",
      "         [184, 172, 175]],\n",
      "\n",
      "        [[100,  97, 108],\n",
      "         [100,  97, 108],\n",
      "         [100,  97, 108]]],\n",
      "\n",
      "\n",
      "       [[[149, 148, 152],\n",
      "         [149, 148, 152],\n",
      "         [149, 148, 152]],\n",
      "\n",
      "        [[184, 184, 184],\n",
      "         [184, 184, 184],\n",
      "         [184, 184, 184]],\n",
      "\n",
      "        [[188, 187, 187],\n",
      "         [188, 187, 187],\n",
      "         [188, 187, 187]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[193, 182, 186],\n",
      "         [193, 182, 186],\n",
      "         [193, 182, 186]],\n",
      "\n",
      "        [[189, 180, 183],\n",
      "         [189, 180, 183],\n",
      "         [189, 180, 183]],\n",
      "\n",
      "        [[147, 141, 147],\n",
      "         [147, 141, 147],\n",
      "         [147, 141, 147]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[180, 179, 183],\n",
      "         [180, 179, 183],\n",
      "         [180, 179, 183]],\n",
      "\n",
      "        [[191, 186, 187],\n",
      "         [191, 186, 187],\n",
      "         [191, 186, 187]],\n",
      "\n",
      "        [[195, 189, 190],\n",
      "         [195, 189, 190],\n",
      "         [195, 189, 190]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[190, 178, 181],\n",
      "         [190, 178, 181],\n",
      "         [190, 178, 181]],\n",
      "\n",
      "        [[184, 171, 175],\n",
      "         [184, 171, 175],\n",
      "         [184, 171, 175]],\n",
      "\n",
      "        [[159, 148, 153],\n",
      "         [159, 148, 153],\n",
      "         [159, 148, 153]]],\n",
      "\n",
      "\n",
      "       [[[166, 172, 181],\n",
      "         [166, 172, 181],\n",
      "         [166, 172, 181]],\n",
      "\n",
      "        [[189, 185, 186],\n",
      "         [189, 185, 186],\n",
      "         [189, 185, 186]],\n",
      "\n",
      "        [[193, 189, 189],\n",
      "         [193, 189, 189],\n",
      "         [193, 189, 189]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[188, 176, 179],\n",
      "         [188, 176, 179],\n",
      "         [188, 176, 179]],\n",
      "\n",
      "        [[183, 172, 176],\n",
      "         [183, 172, 176],\n",
      "         [183, 172, 176]],\n",
      "\n",
      "        [[113, 106, 110],\n",
      "         [113, 106, 110],\n",
      "         [113, 106, 110]]],\n",
      "\n",
      "\n",
      "       [[[138, 155, 171],\n",
      "         [138, 155, 171],\n",
      "         [138, 155, 171]],\n",
      "\n",
      "        [[185, 182, 185],\n",
      "         [185, 182, 185],\n",
      "         [185, 182, 185]],\n",
      "\n",
      "        [[192, 191, 192],\n",
      "         [192, 191, 192],\n",
      "         [192, 191, 192]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[187, 176, 180],\n",
      "         [187, 176, 180],\n",
      "         [187, 176, 180]],\n",
      "\n",
      "        [[177, 167, 171],\n",
      "         [177, 167, 171],\n",
      "         [177, 167, 171]],\n",
      "\n",
      "        [[ 64,  61,  65],\n",
      "         [ 64,  61,  65],\n",
      "         [ 64,  61,  65]]]], dtype=uint8), array([[[[156, 130, 112],\n",
      "         [156, 130, 112],\n",
      "         [156, 130, 112]],\n",
      "\n",
      "        [[155, 132, 114],\n",
      "         [155, 132, 114],\n",
      "         [155, 132, 114]],\n",
      "\n",
      "        [[162, 144, 131],\n",
      "         [162, 144, 131],\n",
      "         [162, 144, 131]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[173, 152, 138],\n",
      "         [173, 152, 138],\n",
      "         [173, 152, 138]],\n",
      "\n",
      "        [[177, 161, 153],\n",
      "         [177, 161, 153],\n",
      "         [177, 161, 153]],\n",
      "\n",
      "        [[188, 176, 178],\n",
      "         [188, 176, 178],\n",
      "         [188, 176, 178]]],\n",
      "\n",
      "\n",
      "       [[[154, 127, 107],\n",
      "         [154, 127, 107],\n",
      "         [154, 127, 107]],\n",
      "\n",
      "        [[158, 136, 121],\n",
      "         [158, 136, 121],\n",
      "         [158, 136, 121]],\n",
      "\n",
      "        [[162, 142, 126],\n",
      "         [162, 142, 126],\n",
      "         [162, 142, 126]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[174, 153, 142],\n",
      "         [174, 153, 142],\n",
      "         [174, 153, 142]],\n",
      "\n",
      "        [[175, 153, 140],\n",
      "         [175, 153, 140],\n",
      "         [175, 153, 140]],\n",
      "\n",
      "        [[189, 177, 179],\n",
      "         [189, 177, 179],\n",
      "         [189, 177, 179]]],\n",
      "\n",
      "\n",
      "       [[[145, 112,  88],\n",
      "         [145, 112,  88],\n",
      "         [145, 112,  88]],\n",
      "\n",
      "        [[161, 142, 129],\n",
      "         [161, 142, 129],\n",
      "         [161, 142, 129]],\n",
      "\n",
      "        [[158, 132, 110],\n",
      "         [158, 132, 110],\n",
      "         [158, 132, 110]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[175, 155, 147],\n",
      "         [175, 155, 147],\n",
      "         [175, 155, 147]],\n",
      "\n",
      "        [[168, 139, 119],\n",
      "         [168, 139, 119],\n",
      "         [168, 139, 119]],\n",
      "\n",
      "        [[181, 162, 155],\n",
      "         [181, 162, 155],\n",
      "         [181, 162, 155]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[199, 191, 190],\n",
      "         [199, 191, 190],\n",
      "         [199, 191, 190]],\n",
      "\n",
      "        [[191, 179, 172],\n",
      "         [191, 179, 172],\n",
      "         [191, 179, 172]],\n",
      "\n",
      "        [[173, 149, 129],\n",
      "         [173, 149, 129],\n",
      "         [173, 149, 129]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[181, 160, 145],\n",
      "         [181, 160, 145],\n",
      "         [181, 160, 145]],\n",
      "\n",
      "        [[181, 164, 150],\n",
      "         [181, 164, 150],\n",
      "         [181, 164, 150]],\n",
      "\n",
      "        [[177, 158, 146],\n",
      "         [177, 158, 146],\n",
      "         [177, 158, 146]]],\n",
      "\n",
      "\n",
      "       [[[192, 179, 175],\n",
      "         [192, 179, 175],\n",
      "         [192, 179, 175]],\n",
      "\n",
      "        [[197, 189, 186],\n",
      "         [197, 189, 186],\n",
      "         [197, 189, 186]],\n",
      "\n",
      "        [[173, 149, 127],\n",
      "         [173, 149, 127],\n",
      "         [173, 149, 127]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[180, 160, 144],\n",
      "         [180, 160, 144],\n",
      "         [180, 160, 144]],\n",
      "\n",
      "        [[179, 160, 144],\n",
      "         [179, 160, 144],\n",
      "         [179, 160, 144]],\n",
      "\n",
      "        [[172, 154, 140],\n",
      "         [172, 154, 140],\n",
      "         [172, 154, 140]]],\n",
      "\n",
      "\n",
      "       [[[186, 168, 164],\n",
      "         [186, 168, 164],\n",
      "         [186, 168, 164]],\n",
      "\n",
      "        [[201, 194, 194],\n",
      "         [201, 194, 194],\n",
      "         [201, 194, 194]],\n",
      "\n",
      "        [[180, 161, 143],\n",
      "         [180, 161, 143],\n",
      "         [180, 161, 143]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[178, 155, 137],\n",
      "         [178, 155, 137],\n",
      "         [178, 155, 137]],\n",
      "\n",
      "        [[178, 159, 144],\n",
      "         [178, 159, 144],\n",
      "         [178, 159, 144]],\n",
      "\n",
      "        [[168, 149, 136],\n",
      "         [168, 149, 136],\n",
      "         [168, 149, 136]]]], dtype=uint8), array([[[[159, 141, 136],\n",
      "         [159, 141, 136],\n",
      "         [159, 141, 136]],\n",
      "\n",
      "        [[182, 148, 136],\n",
      "         [182, 148, 136],\n",
      "         [182, 148, 136]],\n",
      "\n",
      "        [[175, 143, 133],\n",
      "         [175, 143, 133],\n",
      "         [175, 143, 133]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[186, 150, 136],\n",
      "         [186, 150, 136],\n",
      "         [186, 150, 136]],\n",
      "\n",
      "        [[181, 148, 137],\n",
      "         [181, 148, 137],\n",
      "         [181, 148, 137]],\n",
      "\n",
      "        [[117,  96,  91],\n",
      "         [117,  96,  91],\n",
      "         [117,  96,  91]]],\n",
      "\n",
      "\n",
      "       [[[176, 147, 138],\n",
      "         [176, 147, 138],\n",
      "         [176, 147, 138]],\n",
      "\n",
      "        [[185, 152, 141],\n",
      "         [185, 152, 141],\n",
      "         [185, 152, 141]],\n",
      "\n",
      "        [[176, 143, 135],\n",
      "         [176, 143, 135],\n",
      "         [176, 143, 135]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[188, 152, 138],\n",
      "         [188, 152, 138],\n",
      "         [188, 152, 138]],\n",
      "\n",
      "        [[182, 148, 136],\n",
      "         [182, 148, 136],\n",
      "         [182, 148, 136]],\n",
      "\n",
      "        [[158, 129, 122],\n",
      "         [158, 129, 122],\n",
      "         [158, 129, 122]]],\n",
      "\n",
      "\n",
      "       [[[183, 155, 146],\n",
      "         [183, 155, 146],\n",
      "         [183, 155, 146]],\n",
      "\n",
      "        [[182, 153, 144],\n",
      "         [182, 153, 144],\n",
      "         [182, 153, 144]],\n",
      "\n",
      "        [[178, 145, 135],\n",
      "         [178, 145, 135],\n",
      "         [178, 145, 135]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[190, 153, 138],\n",
      "         [190, 153, 138],\n",
      "         [190, 153, 138]],\n",
      "\n",
      "        [[183, 147, 132],\n",
      "         [183, 147, 132],\n",
      "         [183, 147, 132]],\n",
      "\n",
      "        [[174, 145, 138],\n",
      "         [174, 145, 138],\n",
      "         [174, 145, 138]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[180, 151, 143],\n",
      "         [180, 151, 143],\n",
      "         [180, 151, 143]],\n",
      "\n",
      "        [[178, 143, 129],\n",
      "         [178, 143, 129],\n",
      "         [178, 143, 129]],\n",
      "\n",
      "        [[189, 153, 141],\n",
      "         [189, 153, 141],\n",
      "         [189, 153, 141]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[192, 147, 126],\n",
      "         [192, 147, 126],\n",
      "         [192, 147, 126]],\n",
      "\n",
      "        [[187, 145, 127],\n",
      "         [187, 145, 127],\n",
      "         [187, 145, 127]],\n",
      "\n",
      "        [[178, 144, 131],\n",
      "         [178, 144, 131],\n",
      "         [178, 144, 131]]],\n",
      "\n",
      "\n",
      "       [[[174, 145, 139],\n",
      "         [174, 145, 139],\n",
      "         [174, 145, 139]],\n",
      "\n",
      "        [[180, 149, 139],\n",
      "         [180, 149, 139],\n",
      "         [180, 149, 139]],\n",
      "\n",
      "        [[188, 152, 139],\n",
      "         [188, 152, 139],\n",
      "         [188, 152, 139]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[190, 145, 126],\n",
      "         [190, 145, 126],\n",
      "         [190, 145, 126]],\n",
      "\n",
      "        [[185, 148, 133],\n",
      "         [185, 148, 133],\n",
      "         [185, 148, 133]],\n",
      "\n",
      "        [[169, 133, 122],\n",
      "         [169, 133, 122],\n",
      "         [169, 133, 122]]],\n",
      "\n",
      "\n",
      "       [[[168, 140, 134],\n",
      "         [168, 140, 134],\n",
      "         [168, 140, 134]],\n",
      "\n",
      "        [[178, 147, 137],\n",
      "         [178, 147, 137],\n",
      "         [178, 147, 137]],\n",
      "\n",
      "        [[184, 145, 131],\n",
      "         [184, 145, 131],\n",
      "         [184, 145, 131]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[190, 150, 136],\n",
      "         [190, 150, 136],\n",
      "         [190, 150, 136]],\n",
      "\n",
      "        [[185, 152, 141],\n",
      "         [185, 152, 141],\n",
      "         [185, 152, 141]],\n",
      "\n",
      "        [[161, 125, 113],\n",
      "         [161, 125, 113],\n",
      "         [161, 125, 113]]]], dtype=uint8)]\n",
      "(40, 30, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "#load Benign\n",
    "listCancer = []\n",
    "listResults = []\n",
    "for elem in listFiles:\n",
    "    img = readImage('data/melanoma/'+elem)\n",
    "    listCancer += [img]\n",
    "    listResults += [np.array([1])]\n",
    "print(listCancer[:3])\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07997934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ISIC_0001769.jpg', 'ISIC_0001852.jpg', 'ISIC_0001871.jpg', 'ISIC_0003462.jpg', 'ISIC_0003539.jpg']\n"
     ]
    }
   ],
   "source": [
    "listFiles = os.listdir('data/nervus/')\n",
    "print(listFiles[:5])\n",
    "for elem in listFiles:\n",
    "    img = readImage('data/nervus/'+elem)\n",
    "    listResults += [np.array([0])]\n",
    "    listCancer += [img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea1a0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ISIC_0012143.jpg', 'ISIC_0012204.jpg', 'ISIC_0012210.jpg', 'ISIC_0012254.jpg', 'ISIC_0012380.jpg']\n"
     ]
    }
   ],
   "source": [
    "listFiles = os.listdir('data/seborrheic_keratosis/')\n",
    "print(listFiles[:5])\n",
    "for elem in listFiles:\n",
    "    img = readImage('data/seborrheic_keratosis/'+elem)\n",
    "    listResults += [np.array([2])]\n",
    "    listCancer += [img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36904b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print(len(listCancer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60e9785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting LR for different number of Epochs\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    lr *= float(epoch) / 100\n",
    "    print('Learning rate: ', epoch / 300)\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c652817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic ResNet Building Block\n",
    "def resnet_layer(inputs, conv_first = False,\n",
    "\t\t\t\tnum_filters = 16,\n",
    "\t\t\t\tkernel_size = 3,\n",
    "\t\t\t\tstrides = 1,\n",
    "\t\t\t\tactivation ='relu',\n",
    "\t\t\t\tbatch_normalization = True):\n",
    "\tconv = Conv2D(num_filters,\n",
    "\t\t\t\tkernel_size = kernel_size,\n",
    "\t\t\t\tstrides = strides,\n",
    "\t\t\t\tpadding ='same',\n",
    "\t\t\t\tkernel_initializer ='he_normal',\n",
    "\t\t\t\tkernel_regularizer = l2(1e-4))\n",
    "\n",
    "\tx = inputs\n",
    "\tif conv_first:\n",
    "\t\tx = conv(x)\n",
    "\t\tif batch_normalization:\n",
    "\t\t\tx = BatchNormalization()(x)\n",
    "\t\tif activation is not None:\n",
    "\t\t\tx = Activation(activation)(x)\n",
    "\telse:\n",
    "\t\tif batch_normalization:\n",
    "\t\t\tx = BatchNormalization()(x)\n",
    "\t\tif activation is not None:\n",
    "\t\t\tx = Activation(activation)(x)\n",
    "\t\tx = conv(x)\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9463dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet V2 architecture\n",
    "def resnet_v2(input_shape, depth, num_classes = 10):\n",
    "\tif (depth - 2) % 9 != 0:\n",
    "\t\traise ValueError('depth should be 9n + 2 (eg 56 or 110 in [b])')\n",
    "\t# Start model definition.\n",
    "\tnum_filters_in = 16\n",
    "\tnum_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "\tinputs = Input(shape = input_shape)\n",
    "\t# v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "\tx = resnet_layer(inputs = inputs,num_filters = num_filters_in,conv_first = True)\n",
    "\n",
    "\t# Instantiate the stack of residual units\n",
    "\tfor stage in range(3):\n",
    "\t\tfor res_block in range(num_res_blocks):\n",
    "\t\t\tactivation = 'relu'\n",
    "\t\t\tbatch_normalization = True\n",
    "\t\t\tstrides = 1\n",
    "\t\t\tif stage == 0:\n",
    "\t\t\t\tnum_filters_out = num_filters_in * 4\n",
    "\t\t\t\tif res_block == 0: # first layer and first stage\n",
    "\t\t\t\t\tactivation = None\n",
    "\t\t\t\t\tbatch_normalization = False\n",
    "\t\t\telse:\n",
    "\t\t\t\tnum_filters_out = num_filters_in * 2\n",
    "\t\t\t\tif res_block == 0: # first layer but not first stage\n",
    "\t\t\t\t\tstrides = 2 # downsample\n",
    "\n",
    "\t\t\t# bottleneck residual unit\n",
    "\t\t\ty = resnet_layer(inputs = x,\n",
    "\t\t\t\t\t\t\tnum_filters = num_filters_in,\n",
    "\t\t\t\t\t\t\tkernel_size = 1,\n",
    "\t\t\t\t\t\t\tstrides = strides,\n",
    "\t\t\t\t\t\t\tactivation = activation,\n",
    "\t\t\t\t\t\t\tbatch_normalization = batch_normalization)\n",
    "\t\t\ty = resnet_layer(inputs = y,\n",
    "\t\t\t\t\t\t\tnum_filters = num_filters_in,\n",
    "\t\t\t\t\t\t\tconv_first = False)\n",
    "\t\t\ty = resnet_layer(inputs = y,\n",
    "\t\t\t\t\t\t\tnum_filters = num_filters_out,\n",
    "\t\t\t\t\t\t\tkernel_size = 1,\n",
    "\t\t\t\t\t\t\tconv_first = False)\n",
    "\t\t\tif res_block == 0:\n",
    "\t\t\t\t# linear projection residual shortcut connection to match\n",
    "\t\t\t\t# changed dims\n",
    "\t\t\t\tx = resnet_layer(inputs = x,\n",
    "\t\t\t\t\t\t\t\tnum_filters = num_filters_out,\n",
    "\t\t\t\t\t\t\t\tkernel_size = 1,\n",
    "\t\t\t\t\t\t\t\tstrides = strides,\n",
    "\t\t\t\t\t\t\t\tactivation = None,\n",
    "\t\t\t\t\t\t\t\tbatch_normalization = False)\n",
    "\t\t\tx = keras.layers.add([x, y])\n",
    "\n",
    "\t\tnum_filters_in = num_filters_out\n",
    "\n",
    "\t# Add classifier on top.\n",
    "\t# v2 has BN-ReLU before Pooling\n",
    "\tx = BatchNormalization()(x)\n",
    "\tx = Activation('relu')(x)\n",
    "\tx = AveragePooling2D(pool_size = 8)(x)\n",
    "\ty = Flatten()(x)\n",
    "\toutputs = Dense(num_classes,\n",
    "\t\t\t\t\tactivation ='softmax',\n",
    "\t\t\t\t\tkernel_initializer ='he_normal')(y)\n",
    "\n",
    "\t# Instantiate model.\n",
    "\tmodel = Model(inputs = inputs, outputs = outputs)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb24867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=[]\n",
    "y_train=[]\n",
    "accs=[]\n",
    "def main(pz):\n",
    "    \n",
    "    per = np.random.permutation(len(listCancer))\n",
    "    ln = int(len(listCancer) * 0.6)\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    positions = []\n",
    "    #for i in range(ln):\n",
    "    #    positions += [per[i]]\n",
    "    val = 1\n",
    "    while ln > 0:\n",
    "        val = 1+val\n",
    "        if val == 3:\n",
    "            val=0\n",
    "        ln-=1\n",
    "        for i in range(len(per)):\n",
    "            if per[i] in positions:\n",
    "                continue\n",
    "            if listResults[per[i]] == np.array([val]):\n",
    "                positions += [per[i]]\n",
    "                break\n",
    "    for i in range(len(listCancer)):\n",
    "        if i in positions:\n",
    "            x_train += [listCancer[i]]\n",
    "            y_train += [listResults[i]]\n",
    "        else:\n",
    "            x_test += [listCancer[i]]\n",
    "            y_test += [listResults[i]]\n",
    "            \n",
    "    ln = len(x_test) // 2\n",
    "    \n",
    "    x_valid = x_test[ln:]\n",
    "    y_valid = y_test[ln:]\n",
    "    \n",
    "    x_test = x_test[:ln]\n",
    "    y_test = y_test[:ln]\n",
    "    \n",
    "    \n",
    "    x_train = np.array(x_train)\n",
    "    x_train = x_train.reshape(len(x_train), 40, 30, 9)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "\n",
    "    x_test = np.array(x_test)\n",
    "    x_test = x_test.reshape(len(x_test), 40, 30, 9)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    x_valid = np.array(x_valid)\n",
    "    x_valid = x_valid.reshape(len(x_valid), 40, 30, 9)\n",
    "    y_valid = np.array(y_valid)\n",
    "    \n",
    "    \n",
    "    batch_size = 32 # original ResNet paper uses batch_size = 128 for training\n",
    "    epochs = 10\n",
    "    data_augmentation = True\n",
    "    num_classes = 10\n",
    "\n",
    "    # Data Preprocessing\n",
    "    subtract_pixel_mean = True\n",
    "    n = 3\n",
    "\n",
    "    # Select ResNet Version\n",
    "    version = 2\n",
    "\n",
    "    # Computed depth of\n",
    "    if version == 1:\n",
    "        depth = n * 6 + 2\n",
    "    elif version == 2:\n",
    "        depth = n * 9 + 2\n",
    "\n",
    "    # Model name, depth and version\n",
    "    model_type = 'ResNet % dv % d' % (depth, version)\n",
    "\n",
    "    # use the data\n",
    "    print(x_train[:3])\n",
    "    print(y_train[:3])\n",
    "    print(type(x_train[0]))\n",
    "    print(type(x_train))\n",
    "    print(type(y_train[0]))\n",
    "    print(type(y_train))\n",
    "\n",
    "    # Input image dimensions.\n",
    "    input_shape = x_train.shape[1:]\n",
    "    print(input_shape)\n",
    "\n",
    "    # Normalize data.\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "    x_valid = x_test.astype('float32') / 255\n",
    "\n",
    "    # If subtract pixel mean is enabled\n",
    "    if subtract_pixel_mean:\n",
    "        x_train_mean = np.mean(x_train, axis = 0)\n",
    "        x_train -= x_train_mean\n",
    "        x_test -= x_train_mean\n",
    "\n",
    "    # Print Training and Test Samples\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    print('y_train shape:', y_train.shape)\n",
    "\n",
    "    # Convert class vectors to binary class matrices.\n",
    "    y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
    "    y_valid = keras.utils.np_utils.to_categorical(y_valid, num_classes)\n",
    "    \n",
    "    \n",
    "    model = resnet_v2(input_shape = input_shape, depth = depth)\n",
    "\n",
    "    model.compile(loss ='categorical_crossentropy',\n",
    "                optimizer = Adam(learning_rate = lr_schedule(0)),\n",
    "                metrics =['accuracy'])\n",
    "    model.summary()\n",
    "    print(model_type)\n",
    "\n",
    "    # Prepare model model saving directory.\n",
    "    save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "    model_name = 'cifar10_% s_model.{epoch:03d}.h5' % model_type\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "    # Prepare callbacks for model saving and for learning rate adjustment.\n",
    "    checkpoint = ModelCheckpoint(filepath = filepath,\n",
    "                                monitor ='val_acc',\n",
    "                                verbose = 1,\n",
    "                                save_best_only = True)\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "    lr_reducer = ReduceLROnPlateau(factor = np.sqrt(0.1),\n",
    "                                cooldown = 0,\n",
    "                                patience = 5,\n",
    "                                min_lr = 0.5e-6)\n",
    "\n",
    "    callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "\n",
    "    # Run training, with or without data augmentation.\n",
    "    if not data_augmentation:\n",
    "        print('Not using data augmentation.')\n",
    "        model.fit(x_train, y_train,\n",
    "                batch_size = batch_size,\n",
    "                epochs = epochs,\n",
    "                validation_data =(x_test, y_test),\n",
    "                shuffle = True,\n",
    "                callbacks = callbacks)\n",
    "    else:\n",
    "        print('Using real-time data augmentation.')\n",
    "        # This will do preprocessing and realtime data augmentation:\n",
    "        datagen = ImageDataGenerator(\n",
    "            # set input mean to 0 over the dataset\n",
    "            featurewise_center = False,\n",
    "            # set each sample mean to 0\n",
    "            samplewise_center = False,\n",
    "            # divide inputs by std of dataset\n",
    "            featurewise_std_normalization = False,\n",
    "            # divide each input by its std\n",
    "            samplewise_std_normalization = False,\n",
    "            # apply ZCA whitening\n",
    "            zca_whitening = False,\n",
    "            # epsilon for ZCA whitening\n",
    "            zca_epsilon = 1e-06,\n",
    "            # randomly rotate images in the range (deg 0 to 180)\n",
    "            rotation_range = 0,\n",
    "            # randomly shift images horizontally\n",
    "            width_shift_range = 0.1,\n",
    "            # randomly shift images vertically\n",
    "            height_shift_range = 0.1,\n",
    "            # set range for random shear\n",
    "            shear_range = 0.,\n",
    "            # set range for random zoom\n",
    "            zoom_range = 0.,\n",
    "            # set range for random channel shifts\n",
    "            channel_shift_range = 0.,\n",
    "            # set mode for filling points outside the input boundaries\n",
    "            fill_mode ='nearest',\n",
    "            # value used for fill_mode = \"constant\"\n",
    "            cval = 0.,\n",
    "            # randomly flip images\n",
    "            horizontal_flip = True,\n",
    "            # randomly flip images\n",
    "            vertical_flip = False,\n",
    "            # set rescaling factor (applied before any other transformation)\n",
    "            rescale = None,\n",
    "            # set function that will be applied on each input\n",
    "            preprocessing_function = None,\n",
    "            # image data format, either \"channels_first\" or \"channels_last\"\n",
    "            data_format = None,\n",
    "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "            validation_split = 0.1)\n",
    "\n",
    "        # Compute quantities required for featurewise normalization\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "        print('------------------')\n",
    "        print(x_train)\n",
    "        datagen.fit(x_train)\n",
    "\n",
    "        # Fit the model on the batches generated by datagen.flow().\n",
    "        model.fit_generator(datagen.flow(x_train, y_train, batch_size = batch_size),\n",
    "                            validation_data =(x_valid, y_valid),\n",
    "                            epochs = 100, verbose = 1, workers = 4,\n",
    "                            callbacks = callbacks)\n",
    "\n",
    "    # Score trained model.\n",
    "    scores = model.evaluate(x_test, y_test, verbose = 1)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    ret = model.predict(x_test)\n",
    "    result = []\n",
    "    pred = []\n",
    "    for a,b in zip(ret, y_test):\n",
    "        pred+=[a.argmax(axis=-1)]\n",
    "        result+=[b.argmax(axis=-1)]\n",
    "    print(pred)\n",
    "    print(result)\n",
    "    precision = precision_score(result, pred, average='weighted')\n",
    "    recall = recall_score(result, pred, average='weighted')\n",
    "     # calculates f1 for 1:100 dataset with 95tp, 5fn, 55fp\n",
    "    fmeasure = f1_score(result, pred, average='weighted')\n",
    "    print('Precision: ', precision)\n",
    "    print('Recall: ', recall)\n",
    "    print('F1 Score: ', fmeasure)\n",
    "    #precisions += [precision]\n",
    "    #recalls += [recall]\n",
    "    if scores[1] > 0.5:\n",
    "        model.save('saved_models/skinResNetV'+str(pz)+'.h5')\n",
    "    return [scores[1], precision, recall, fmeasure]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31f8fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[156 130 112 ... 156 130 112]\n",
      "   [155 132 114 ... 155 132 114]\n",
      "   [162 144 131 ... 162 144 131]\n",
      "   ...\n",
      "   [173 152 138 ... 173 152 138]\n",
      "   [177 161 153 ... 177 161 153]\n",
      "   [188 176 178 ... 188 176 178]]\n",
      "\n",
      "  [[154 127 107 ... 154 127 107]\n",
      "   [158 136 121 ... 158 136 121]\n",
      "   [162 142 126 ... 162 142 126]\n",
      "   ...\n",
      "   [174 153 142 ... 174 153 142]\n",
      "   [175 153 140 ... 175 153 140]\n",
      "   [189 177 179 ... 189 177 179]]\n",
      "\n",
      "  [[145 112  88 ... 145 112  88]\n",
      "   [161 142 129 ... 161 142 129]\n",
      "   [158 132 110 ... 158 132 110]\n",
      "   ...\n",
      "   [175 155 147 ... 175 155 147]\n",
      "   [168 139 119 ... 168 139 119]\n",
      "   [181 162 155 ... 181 162 155]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[199 191 190 ... 199 191 190]\n",
      "   [191 179 172 ... 191 179 172]\n",
      "   [173 149 129 ... 173 149 129]\n",
      "   ...\n",
      "   [181 160 145 ... 181 160 145]\n",
      "   [181 164 150 ... 181 164 150]\n",
      "   [177 158 146 ... 177 158 146]]\n",
      "\n",
      "  [[192 179 175 ... 192 179 175]\n",
      "   [197 189 186 ... 197 189 186]\n",
      "   [173 149 127 ... 173 149 127]\n",
      "   ...\n",
      "   [180 160 144 ... 180 160 144]\n",
      "   [179 160 144 ... 179 160 144]\n",
      "   [172 154 140 ... 172 154 140]]\n",
      "\n",
      "  [[186 168 164 ... 186 168 164]\n",
      "   [201 194 194 ... 201 194 194]\n",
      "   [180 161 143 ... 180 161 143]\n",
      "   ...\n",
      "   [178 155 137 ... 178 155 137]\n",
      "   [178 159 144 ... 178 159 144]\n",
      "   [168 149 136 ... 168 149 136]]]\n",
      "\n",
      "\n",
      " [[[159 141 136 ... 159 141 136]\n",
      "   [182 148 136 ... 182 148 136]\n",
      "   [175 143 133 ... 175 143 133]\n",
      "   ...\n",
      "   [186 150 136 ... 186 150 136]\n",
      "   [181 148 137 ... 181 148 137]\n",
      "   [117  96  91 ... 117  96  91]]\n",
      "\n",
      "  [[176 147 138 ... 176 147 138]\n",
      "   [185 152 141 ... 185 152 141]\n",
      "   [176 143 135 ... 176 143 135]\n",
      "   ...\n",
      "   [188 152 138 ... 188 152 138]\n",
      "   [182 148 136 ... 182 148 136]\n",
      "   [158 129 122 ... 158 129 122]]\n",
      "\n",
      "  [[183 155 146 ... 183 155 146]\n",
      "   [182 153 144 ... 182 153 144]\n",
      "   [178 145 135 ... 178 145 135]\n",
      "   ...\n",
      "   [190 153 138 ... 190 153 138]\n",
      "   [183 147 132 ... 183 147 132]\n",
      "   [174 145 138 ... 174 145 138]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[180 151 143 ... 180 151 143]\n",
      "   [178 143 129 ... 178 143 129]\n",
      "   [189 153 141 ... 189 153 141]\n",
      "   ...\n",
      "   [192 147 126 ... 192 147 126]\n",
      "   [187 145 127 ... 187 145 127]\n",
      "   [178 144 131 ... 178 144 131]]\n",
      "\n",
      "  [[174 145 139 ... 174 145 139]\n",
      "   [180 149 139 ... 180 149 139]\n",
      "   [188 152 139 ... 188 152 139]\n",
      "   ...\n",
      "   [190 145 126 ... 190 145 126]\n",
      "   [185 148 133 ... 185 148 133]\n",
      "   [169 133 122 ... 169 133 122]]\n",
      "\n",
      "  [[168 140 134 ... 168 140 134]\n",
      "   [178 147 137 ... 178 147 137]\n",
      "   [184 145 131 ... 184 145 131]\n",
      "   ...\n",
      "   [190 150 136 ... 190 150 136]\n",
      "   [185 152 141 ... 185 152 141]\n",
      "   [161 125 113 ... 161 125 113]]]\n",
      "\n",
      "\n",
      " [[[178 153 153 ... 178 153 153]\n",
      "   [164 108 129 ... 164 108 129]\n",
      "   [182 134 133 ... 182 134 133]\n",
      "   ...\n",
      "   [191 164 144 ... 191 164 144]\n",
      "   [194 174 162 ... 194 174 162]\n",
      "   [195 171 160 ... 195 171 160]]\n",
      "\n",
      "  [[178 148 152 ... 178 148 152]\n",
      "   [169 111 129 ... 169 111 129]\n",
      "   [187 144 139 ... 187 144 139]\n",
      "   ...\n",
      "   [192 162 142 ... 192 162 142]\n",
      "   [193 171 157 ... 193 171 157]\n",
      "   [197 177 167 ... 197 177 167]]\n",
      "\n",
      "  [[179 140 147 ... 179 140 147]\n",
      "   [175 120 130 ... 175 120 130]\n",
      "   [191 153 147 ... 191 153 147]\n",
      "   ...\n",
      "   [194 163 143 ... 194 163 143]\n",
      "   [193 168 152 ... 193 168 152]\n",
      "   [198 180 171 ... 198 180 171]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[192 151 138 ... 192 151 138]\n",
      "   [195 150 137 ... 195 150 137]\n",
      "   [199 158 145 ... 199 158 145]\n",
      "   ...\n",
      "   [195 158 140 ... 195 158 140]\n",
      "   [203 176 162 ... 203 176 162]\n",
      "   [216 203 197 ... 216 203 197]]\n",
      "\n",
      "  [[192 159 148 ... 192 159 148]\n",
      "   [195 153 137 ... 195 153 137]\n",
      "   [198 159 146 ... 198 159 146]\n",
      "   ...\n",
      "   [196 163 145 ... 196 163 145]\n",
      "   [208 188 177 ... 208 188 177]\n",
      "   [209 192 185 ... 209 192 185]]\n",
      "\n",
      "  [[194 165 156 ... 194 165 156]\n",
      "   [194 156 140 ... 194 156 140]\n",
      "   [198 159 144 ... 198 159 144]\n",
      "   ...\n",
      "   [197 168 150 ... 197 168 150]\n",
      "   [210 195 187 ... 210 195 187]\n",
      "   [200 176 168 ... 200 176 168]]]]\n",
      "[1 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (140, 40, 30, 9)\n",
      "140 train samples\n",
      "47 test samples\n",
      "y_train shape: (140,)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_279 (Conv2D)             (None, 40, 30, 16)   1312        input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_252 (BatchN (None, 40, 30, 16)   64          conv2d_279[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_252 (Activation)     (None, 40, 30, 16)   0           batch_normalization_252[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_280 (Conv2D)             (None, 40, 30, 16)   272         activation_252[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_253 (BatchN (None, 40, 30, 16)   64          conv2d_280[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_253 (Activation)     (None, 40, 30, 16)   0           batch_normalization_253[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_281 (Conv2D)             (None, 40, 30, 16)   2320        activation_253[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_254 (BatchN (None, 40, 30, 16)   64          conv2d_281[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_254 (Activation)     (None, 40, 30, 16)   0           batch_normalization_254[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_283 (Conv2D)             (None, 40, 30, 64)   1088        activation_252[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_282 (Conv2D)             (None, 40, 30, 64)   1088        activation_254[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_81 (Add)                    (None, 40, 30, 64)   0           conv2d_283[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_255 (BatchN (None, 40, 30, 64)   256         add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_255 (Activation)     (None, 40, 30, 64)   0           batch_normalization_255[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_284 (Conv2D)             (None, 40, 30, 16)   1040        activation_255[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_256 (BatchN (None, 40, 30, 16)   64          conv2d_284[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_256 (Activation)     (None, 40, 30, 16)   0           batch_normalization_256[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_285 (Conv2D)             (None, 40, 30, 16)   2320        activation_256[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_257 (BatchN (None, 40, 30, 16)   64          conv2d_285[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_257 (Activation)     (None, 40, 30, 16)   0           batch_normalization_257[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_286 (Conv2D)             (None, 40, 30, 64)   1088        activation_257[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_82 (Add)                    (None, 40, 30, 64)   0           add_81[0][0]                     \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_258 (BatchN (None, 40, 30, 64)   256         add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_258 (Activation)     (None, 40, 30, 64)   0           batch_normalization_258[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_287 (Conv2D)             (None, 40, 30, 16)   1040        activation_258[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_259 (BatchN (None, 40, 30, 16)   64          conv2d_287[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_259 (Activation)     (None, 40, 30, 16)   0           batch_normalization_259[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_288 (Conv2D)             (None, 40, 30, 16)   2320        activation_259[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_260 (BatchN (None, 40, 30, 16)   64          conv2d_288[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_260 (Activation)     (None, 40, 30, 16)   0           batch_normalization_260[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_289 (Conv2D)             (None, 40, 30, 64)   1088        activation_260[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_83 (Add)                    (None, 40, 30, 64)   0           add_82[0][0]                     \n",
      "                                                                 conv2d_289[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_261 (BatchN (None, 40, 30, 64)   256         add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_261 (Activation)     (None, 40, 30, 64)   0           batch_normalization_261[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_290 (Conv2D)             (None, 20, 15, 64)   4160        activation_261[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_262 (BatchN (None, 20, 15, 64)   256         conv2d_290[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_262 (Activation)     (None, 20, 15, 64)   0           batch_normalization_262[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_291 (Conv2D)             (None, 20, 15, 64)   36928       activation_262[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_263 (BatchN (None, 20, 15, 64)   256         conv2d_291[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_263 (Activation)     (None, 20, 15, 64)   0           batch_normalization_263[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_293 (Conv2D)             (None, 20, 15, 128)  8320        add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_292 (Conv2D)             (None, 20, 15, 128)  8320        activation_263[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_84 (Add)                    (None, 20, 15, 128)  0           conv2d_293[0][0]                 \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_264 (BatchN (None, 20, 15, 128)  512         add_84[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_264 (Activation)     (None, 20, 15, 128)  0           batch_normalization_264[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_294 (Conv2D)             (None, 20, 15, 64)   8256        activation_264[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_265 (BatchN (None, 20, 15, 64)   256         conv2d_294[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_265 (Activation)     (None, 20, 15, 64)   0           batch_normalization_265[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_295 (Conv2D)             (None, 20, 15, 64)   36928       activation_265[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_266 (BatchN (None, 20, 15, 64)   256         conv2d_295[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_266 (Activation)     (None, 20, 15, 64)   0           batch_normalization_266[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_296 (Conv2D)             (None, 20, 15, 128)  8320        activation_266[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_85 (Add)                    (None, 20, 15, 128)  0           add_84[0][0]                     \n",
      "                                                                 conv2d_296[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_267 (BatchN (None, 20, 15, 128)  512         add_85[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_267 (Activation)     (None, 20, 15, 128)  0           batch_normalization_267[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_297 (Conv2D)             (None, 20, 15, 64)   8256        activation_267[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_268 (BatchN (None, 20, 15, 64)   256         conv2d_297[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_268 (Activation)     (None, 20, 15, 64)   0           batch_normalization_268[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_298 (Conv2D)             (None, 20, 15, 64)   36928       activation_268[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_269 (BatchN (None, 20, 15, 64)   256         conv2d_298[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_269 (Activation)     (None, 20, 15, 64)   0           batch_normalization_269[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_299 (Conv2D)             (None, 20, 15, 128)  8320        activation_269[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_86 (Add)                    (None, 20, 15, 128)  0           add_85[0][0]                     \n",
      "                                                                 conv2d_299[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_270 (BatchN (None, 20, 15, 128)  512         add_86[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_270 (Activation)     (None, 20, 15, 128)  0           batch_normalization_270[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_300 (Conv2D)             (None, 10, 8, 128)   16512       activation_270[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_271 (BatchN (None, 10, 8, 128)   512         conv2d_300[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_271 (Activation)     (None, 10, 8, 128)   0           batch_normalization_271[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_301 (Conv2D)             (None, 10, 8, 128)   147584      activation_271[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_272 (BatchN (None, 10, 8, 128)   512         conv2d_301[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_272 (Activation)     (None, 10, 8, 128)   0           batch_normalization_272[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_303 (Conv2D)             (None, 10, 8, 256)   33024       add_86[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_302 (Conv2D)             (None, 10, 8, 256)   33024       activation_272[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_87 (Add)                    (None, 10, 8, 256)   0           conv2d_303[0][0]                 \n",
      "                                                                 conv2d_302[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_273 (BatchN (None, 10, 8, 256)   1024        add_87[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_273 (Activation)     (None, 10, 8, 256)   0           batch_normalization_273[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_304 (Conv2D)             (None, 10, 8, 128)   32896       activation_273[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_274 (BatchN (None, 10, 8, 128)   512         conv2d_304[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_274 (Activation)     (None, 10, 8, 128)   0           batch_normalization_274[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_305 (Conv2D)             (None, 10, 8, 128)   147584      activation_274[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_275 (BatchN (None, 10, 8, 128)   512         conv2d_305[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_275 (Activation)     (None, 10, 8, 128)   0           batch_normalization_275[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_306 (Conv2D)             (None, 10, 8, 256)   33024       activation_275[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_88 (Add)                    (None, 10, 8, 256)   0           add_87[0][0]                     \n",
      "                                                                 conv2d_306[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_276 (BatchN (None, 10, 8, 256)   1024        add_88[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_276 (Activation)     (None, 10, 8, 256)   0           batch_normalization_276[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_307 (Conv2D)             (None, 10, 8, 128)   32896       activation_276[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_277 (BatchN (None, 10, 8, 128)   512         conv2d_307[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_277 (Activation)     (None, 10, 8, 128)   0           batch_normalization_277[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_308 (Conv2D)             (None, 10, 8, 128)   147584      activation_277[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_278 (BatchN (None, 10, 8, 128)   512         conv2d_308[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_278 (Activation)     (None, 10, 8, 128)   0           batch_normalization_278[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_309 (Conv2D)             (None, 10, 8, 256)   33024       activation_278[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_89 (Add)                    (None, 10, 8, 256)   0           add_88[0][0]                     \n",
      "                                                                 conv2d_309[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_279 (BatchN (None, 10, 8, 256)   1024        add_89[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_279 (Activation)     (None, 10, 8, 256)   0           batch_normalization_279[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 1, 1, 256)    0           activation_279[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 256)          0           average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 10)           2570        flatten_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[ 1.37059003e-01  7.09244907e-02  1.17928088e-02 ...  1.37059003e-01\n",
      "     7.09244907e-02  1.17928088e-02]\n",
      "   [ 6.19328022e-02  1.80111229e-02 -2.81231999e-02 ...  6.19328022e-02\n",
      "     1.80111229e-02 -2.81231999e-02]\n",
      "   [ 5.26331663e-02  6.41456842e-02  4.60785627e-02 ...  5.26331663e-02\n",
      "     6.41456842e-02  4.60785627e-02]\n",
      "   ...\n",
      "   [ 1.07395053e-01  7.81792402e-02  4.69467044e-02 ...  1.07395053e-01\n",
      "     7.81792402e-02  4.69467044e-02]\n",
      "   [ 1.58179462e-01  1.49747968e-01  1.22296929e-01 ...  1.58179462e-01\n",
      "     1.49747968e-01  1.22296929e-01]\n",
      "   [ 2.49579757e-01  2.34173775e-01  2.45322257e-01 ...  2.49579757e-01\n",
      "     2.34173775e-01  2.45322257e-01]]\n",
      "\n",
      "  [[ 9.97759700e-02  4.59944606e-02 -2.37534344e-02 ...  9.97759700e-02\n",
      "     4.59944606e-02 -2.37534344e-02]\n",
      "   [ 5.95519543e-02  3.54621708e-02 -9.80496407e-04 ...  5.95519543e-02\n",
      "     3.54621708e-02 -9.80496407e-04]\n",
      "   [ 4.56304550e-02  3.16807628e-02  3.36137414e-03 ...  4.56304550e-02\n",
      "     3.16807628e-02  3.36137414e-03]\n",
      "   ...\n",
      "   [ 8.99720788e-02  8.08402896e-02  5.61624765e-02 ...  8.99720788e-02\n",
      "     8.08402896e-02  5.61624765e-02]\n",
      "   [ 1.17731094e-01  1.00952268e-01  7.00559318e-02 ...  1.17731094e-01\n",
      "     1.00952268e-01  7.00559318e-02]\n",
      "   [ 2.32548892e-01  2.35574365e-01  2.57282943e-01 ...  2.32548892e-01\n",
      "     2.35574365e-01  2.57282943e-01]]\n",
      "\n",
      "  [[ 2.51262188e-02 -3.96359563e-02 -1.24873966e-01 ...  2.51262188e-02\n",
      "    -3.96359563e-02 -1.24873966e-01]\n",
      "   [ 6.27731681e-02  5.43137193e-02  2.74510086e-02 ...  6.27731681e-02\n",
      "     5.43137193e-02  2.74510086e-02]\n",
      "   [ 2.39498615e-02 -1.40054822e-02 -6.94117248e-02 ...  2.39498615e-02\n",
      "    -1.40054822e-02 -6.94117248e-02]\n",
      "   ...\n",
      "   [ 9.96360183e-02  7.42576122e-02  6.67228103e-02 ...  9.96360183e-02\n",
      "     7.42576122e-02  6.67228103e-02]\n",
      "   [ 8.00279379e-02  2.38934159e-02 -3.19328010e-02 ...  8.00279379e-02\n",
      "     2.38934159e-02 -3.19328010e-02]\n",
      "   [ 1.76694691e-01  1.55882448e-01  1.33949637e-01 ...  1.76694691e-01\n",
      "     1.55882448e-01  1.33949637e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 2.27647066e-01  2.30812252e-01  2.52044767e-01 ...  2.27647066e-01\n",
      "     2.30812252e-01  2.52044767e-01]\n",
      "   [ 1.62465036e-01  1.73529446e-01  1.63809538e-01 ...  1.62465036e-01\n",
      "     1.73529446e-01  1.63809538e-01]\n",
      "   [ 7.87675977e-02  5.63026071e-02  2.99721956e-03 ...  7.87675977e-02\n",
      "     5.63026071e-02  2.99721956e-03]\n",
      "   ...\n",
      "   [ 1.21708870e-01  9.24368501e-02  5.58823347e-02 ...  1.21708870e-01\n",
      "     9.24368501e-02  5.58823347e-02]\n",
      "   [ 1.42549038e-01  1.39439642e-01  1.17339015e-01 ...  1.42549038e-01\n",
      "     1.39439642e-01  1.17339015e-01]\n",
      "   [ 1.46946728e-01  1.48683608e-01  1.19215757e-01 ...  1.46946728e-01\n",
      "     1.48683608e-01  1.19215757e-01]]\n",
      "\n",
      "  [[ 2.02773154e-01  2.05294192e-01  1.99495882e-01 ...  2.02773154e-01\n",
      "     2.05294192e-01  1.99495882e-01]\n",
      "   [ 1.87254906e-01  2.18515396e-01  2.25546241e-01 ...  1.87254906e-01\n",
      "     2.18515396e-01  2.25546241e-01]\n",
      "   [ 8.99720788e-02  5.98599911e-02 -5.26615977e-03 ...  8.99720788e-02\n",
      "     5.98599911e-02 -5.26615977e-03]\n",
      "   ...\n",
      "   [ 1.30756319e-01  1.05266094e-01  7.03359544e-02 ...  1.30756319e-01\n",
      "     1.05266094e-01  7.03359544e-02]\n",
      "   [ 1.45238042e-01  1.39299601e-01  8.46779048e-02 ...  1.45238042e-01\n",
      "     1.39299601e-01  8.46779048e-02]\n",
      "   [ 1.51484668e-01  1.37338817e-01  1.12689078e-01 ...  1.51484668e-01\n",
      "     1.37338817e-01  1.12689078e-01]]\n",
      "\n",
      "  [[ 2.03669608e-01  1.80196196e-01  1.73165470e-01 ...  2.03669608e-01\n",
      "     1.80196196e-01  1.73165470e-01]\n",
      "   [ 2.21120477e-01  2.41848648e-01  2.50532150e-01 ...  2.21120477e-01\n",
      "     2.41848648e-01  2.50532150e-01]\n",
      "   [ 1.29663825e-01  1.16442502e-01  6.34172857e-02 ...  1.29663825e-01\n",
      "     1.16442502e-01  6.34172857e-02]\n",
      "   ...\n",
      "   [ 1.27591014e-01  9.43698287e-02  5.47338128e-02 ...  1.27591014e-01\n",
      "     9.43698287e-02  5.47338128e-02]\n",
      "   [ 1.38151228e-01  1.41400486e-01  1.01512611e-01 ...  1.38151228e-01\n",
      "     1.41400486e-01  1.01512611e-01]\n",
      "   [ 1.62352800e-01  1.55686259e-01  1.07030690e-01 ...  1.62352800e-01\n",
      "     1.55686259e-01  1.07030690e-01]]]\n",
      "\n",
      "\n",
      " [[[ 1.48823708e-01  1.14061743e-01  1.05910480e-01 ...  1.48823708e-01\n",
      "     1.14061743e-01  1.05910480e-01]\n",
      "   [ 1.67815149e-01  8.07562172e-02  5.81513345e-02 ...  1.67815149e-01\n",
      "     8.07562172e-02  5.81513345e-02]\n",
      "   [ 1.03613555e-01  6.02241158e-02  5.39216995e-02 ...  1.03613555e-01\n",
      "     6.02241158e-02  5.39216995e-02]\n",
      "   ...\n",
      "   [ 1.58375442e-01  7.03361034e-02  3.91035676e-02 ...  1.58375442e-01\n",
      "     7.03361034e-02  3.91035676e-02]\n",
      "   [ 1.73865736e-01  9.87675786e-02  5.95518351e-02 ...  1.73865736e-01\n",
      "     9.87675786e-02  5.95518351e-02]\n",
      "   [-2.88516283e-02 -7.95517266e-02 -9.58542228e-02 ... -2.88516283e-02\n",
      "    -7.95517266e-02 -9.58542228e-02]]\n",
      "\n",
      "  [[ 1.86050475e-01  1.24425858e-01  9.78152156e-02 ...  1.86050475e-01\n",
      "     1.24425858e-01  9.78152156e-02]\n",
      "   [ 1.65434301e-01  9.82072651e-02  7.74509013e-02 ...  1.65434301e-01\n",
      "     9.82072651e-02  7.74509013e-02]\n",
      "   [ 1.00532413e-01  3.56023312e-02  3.86555195e-02 ...  1.00532413e-01\n",
      "     3.56023312e-02  3.86555195e-02]\n",
      "   ...\n",
      "   [ 1.44874036e-01  7.69187212e-02  4.04762030e-02 ...  1.44874036e-01\n",
      "     7.69187212e-02  4.04762030e-02]\n",
      "   [ 1.45182073e-01  8.13444257e-02  5.43696582e-02 ...  1.45182073e-01\n",
      "     8.13444257e-02  5.43696582e-02]\n",
      "   [ 1.10980272e-01  4.73390818e-02  3.37535143e-02 ...  1.10980272e-01\n",
      "     4.73390818e-02  3.37535143e-02]]\n",
      "\n",
      "  [[ 1.74145818e-01  1.28991514e-01  1.02577031e-01 ...  1.74145818e-01\n",
      "     1.28991514e-01  1.02577031e-01]\n",
      "   [ 1.45126104e-01  9.74509716e-02  8.62745345e-02 ...  1.45126104e-01\n",
      "     9.74509716e-02  8.62745345e-02]\n",
      "   [ 1.02381229e-01  3.69749069e-02  2.86275148e-02 ...  1.02381229e-01\n",
      "     3.69749069e-02  2.86275148e-02]\n",
      "   ...\n",
      "   [ 1.58459544e-01  6.64144754e-02  3.14286947e-02 ...  1.58459544e-01\n",
      "     6.64144754e-02  3.14286947e-02]\n",
      "   [ 1.38851464e-01  5.52659631e-02  1.90476179e-02 ...  1.38851464e-01\n",
      "     5.52659631e-02  1.90476179e-02]\n",
      "   [ 1.49243712e-01  8.92157853e-02  6.72829747e-02 ...  1.49243712e-01\n",
      "     8.92157853e-02  6.72829747e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.53137267e-01  7.39495158e-02  6.77310526e-02 ...  1.53137267e-01\n",
      "     7.39495158e-02  6.77310526e-02]\n",
      "   [ 1.11484647e-01  3.23529840e-02 -4.81790304e-03 ...  1.11484647e-01\n",
      "     3.23529840e-02 -4.81790304e-03]\n",
      "   [ 1.41512692e-01  7.19888806e-02  5.00560403e-02 ...  1.41512692e-01\n",
      "     7.19888806e-02  5.00560403e-02]\n",
      "   ...\n",
      "   [ 1.64846122e-01  4.14564610e-02 -1.86274946e-02 ...  1.64846122e-01\n",
      "     4.14564610e-02 -1.86274946e-02]\n",
      "   [ 1.66078448e-01  6.49298429e-02  2.71429121e-02 ...  1.66078448e-01\n",
      "     6.49298429e-02  2.71429121e-02]\n",
      "   [ 1.50868297e-01  9.37816501e-02  6.03922307e-02 ...  1.50868297e-01\n",
      "     9.37816501e-02  6.03922307e-02]]\n",
      "\n",
      "  [[ 1.32184923e-01  7.19608665e-02  5.83194196e-02 ...  1.32184923e-01\n",
      "     7.19608665e-02  5.83194196e-02]\n",
      "   [ 1.20588243e-01  6.16526604e-02  4.12325263e-02 ...  1.20588243e-01\n",
      "     6.16526604e-02  4.12325263e-02]\n",
      "   [ 1.48795605e-01  7.16246963e-02  4.17926908e-02 ...  1.48795605e-01\n",
      "     7.16246963e-02  4.17926908e-02]\n",
      "   ...\n",
      "   [ 1.69972003e-01  4.64425683e-02 -2.52306461e-04 ...  1.69972003e-01\n",
      "     4.64425683e-02 -2.52306461e-04]\n",
      "   [ 1.68767452e-01  9.22407806e-02  4.15406525e-02 ...  1.68767452e-01\n",
      "     9.22407806e-02  4.15406525e-02]\n",
      "   [ 1.39719963e-01  5.49858809e-02  4.21008170e-02 ...  1.39719963e-01\n",
      "     5.49858809e-02  4.21008170e-02]]\n",
      "\n",
      "  [[ 1.33081377e-01  7.03922808e-02  5.55184186e-02 ...  1.33081377e-01\n",
      "     7.03922808e-02  5.55184186e-02]\n",
      "   [ 1.30924404e-01  5.75349331e-02  2.70027518e-02 ...  1.30924404e-01\n",
      "     5.75349331e-02  2.70027518e-02]\n",
      "   [ 1.45350099e-01  5.36974072e-02  1.63584650e-02 ...  1.45350099e-01\n",
      "     5.36974072e-02  1.63584650e-02]\n",
      "   ...\n",
      "   [ 1.74649835e-01  7.47619867e-02  5.08122444e-02 ...  1.74649835e-01\n",
      "     7.47619867e-02  5.08122444e-02]\n",
      "   [ 1.65602207e-01  1.13949507e-01  8.97479057e-02 ...  1.65602207e-01\n",
      "     1.13949507e-01  8.97479057e-02]\n",
      "   [ 1.34901822e-01  6.15685880e-02  1.68345869e-02 ...  1.34901822e-01\n",
      "     6.15685880e-02  1.68345869e-02]]]\n",
      "\n",
      "\n",
      " [[[ 2.23333508e-01  1.61120564e-01  1.72577143e-01 ...  2.23333508e-01\n",
      "     1.61120564e-01  1.72577143e-01]\n",
      "   [ 9.72269177e-02 -7.61065483e-02  3.07003558e-02 ...  9.72269177e-02\n",
      "    -7.61065483e-02  3.07003558e-02]\n",
      "   [ 1.31064534e-01  2.49300003e-02  5.39216995e-02 ...  1.31064534e-01\n",
      "     2.49300003e-02  5.39216995e-02]\n",
      "   ...\n",
      "   [ 1.77983284e-01  1.25238061e-01  7.04761147e-02 ...  1.77983284e-01\n",
      "     1.25238061e-01  7.04761147e-02]\n",
      "   [ 2.24846125e-01  2.00728357e-01  1.57591045e-01 ...  2.24846125e-01\n",
      "     2.00728357e-01  1.57591045e-01]\n",
      "   [ 2.77030736e-01  2.14565933e-01  1.74734026e-01 ...  2.77030736e-01\n",
      "     2.14565933e-01  1.74734026e-01]]\n",
      "\n",
      "  [[ 1.93893611e-01  1.28347427e-01  1.52717173e-01 ...  1.93893611e-01\n",
      "     1.28347427e-01  1.52717173e-01]\n",
      "   [ 1.02689207e-01 -6.25770688e-02  3.03920805e-02 ...  1.02689207e-01\n",
      "    -6.25770688e-02  3.03920805e-02]\n",
      "   [ 1.43669665e-01  3.95238996e-02  5.43417931e-02 ...  1.43669665e-01\n",
      "     3.95238996e-02  5.43417931e-02]\n",
      "   ...\n",
      "   [ 1.60560310e-01  1.16134405e-01  5.61624765e-02 ...  1.60560310e-01\n",
      "     1.16134405e-01  5.61624765e-02]\n",
      "   [ 1.88319325e-01  1.71540499e-01  1.36722594e-01 ...  1.88319325e-01\n",
      "     1.71540499e-01  1.36722594e-01]\n",
      "   [ 2.63921440e-01  2.35574365e-01  2.10224122e-01 ...  2.63921440e-01\n",
      "     2.35574365e-01  2.10224122e-01]]\n",
      "\n",
      "  [[ 1.58459544e-01  7.01679885e-02  1.06498599e-01 ...  1.58459544e-01\n",
      "     7.01679885e-02  1.06498599e-01]\n",
      "   [ 1.17675126e-01 -3.19608152e-02  3.13725770e-02 ...  1.17675126e-01\n",
      "    -3.19608152e-02  3.13725770e-02]\n",
      "   [ 1.53361619e-01  6.83474541e-02  7.56863356e-02 ...  1.53361619e-01\n",
      "     6.83474541e-02  7.56863356e-02]\n",
      "   ...\n",
      "   [ 1.74145818e-01  1.05630159e-01  5.10365367e-02 ...  1.74145818e-01\n",
      "     1.05630159e-01  5.10365367e-02]\n",
      "   [ 1.78067148e-01  1.37618899e-01  9.74789858e-02 ...  1.78067148e-01\n",
      "     1.37618899e-01  9.74789858e-02]\n",
      "   [ 2.43361354e-01  2.26470679e-01  1.96694732e-01 ...  2.43361354e-01\n",
      "     2.26470679e-01  1.96694732e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 2.00196087e-01  7.39495158e-02  4.81232107e-02 ...  2.00196087e-01\n",
      "     7.39495158e-02  4.81232107e-02]\n",
      "   [ 1.78151309e-01  5.98039627e-02  2.65546441e-02 ...  1.78151309e-01\n",
      "     5.98039627e-02  2.65546441e-02]\n",
      "   [ 1.80728376e-01  9.15967226e-02  6.57423139e-02 ...  1.80728376e-01\n",
      "     9.15967226e-02  6.57423139e-02]\n",
      "   ...\n",
      "   [ 1.76610827e-01  8.45937133e-02  3.62744927e-02 ...  1.76610827e-01\n",
      "     8.45937133e-02  3.62744927e-02]\n",
      "   [ 2.28823543e-01  1.86498463e-01  1.64397836e-01 ...  2.28823543e-01\n",
      "     1.86498463e-01  1.64397836e-01]\n",
      "   [ 2.99887896e-01  3.25154185e-01  3.19215745e-01 ...  2.99887896e-01\n",
      "     3.25154185e-01  3.19215745e-01]]\n",
      "\n",
      "  [[ 2.02773154e-01  1.26862824e-01  9.36135352e-02 ...  2.02773154e-01\n",
      "     1.26862824e-01  9.36135352e-02]\n",
      "   [ 1.79411769e-01  7.73389339e-02  3.33893895e-02 ...  1.79411769e-01\n",
      "     7.73389339e-02  3.33893895e-02]\n",
      "   [ 1.88011289e-01  9.90756750e-02  6.92436695e-02 ...  1.88011289e-01\n",
      "     9.90756750e-02  6.92436695e-02]\n",
      "   ...\n",
      "   [ 1.93501413e-01  1.17030799e-01  7.42575228e-02 ...  1.93501413e-01\n",
      "     1.17030799e-01  7.42575228e-02]\n",
      "   [ 2.58963525e-01  2.49103516e-01  2.14089662e-01 ...  2.58963525e-01\n",
      "     2.49103516e-01  2.14089662e-01]\n",
      "   [ 2.96582699e-01  2.86358416e-01  2.89159656e-01 ...  2.96582699e-01\n",
      "     2.86358416e-01  2.89159656e-01]]\n",
      "\n",
      "  [[ 2.35042155e-01  1.68431491e-01  1.41792923e-01 ...  2.35042155e-01\n",
      "     1.68431491e-01  1.41792923e-01]\n",
      "   [ 1.93669498e-01  9.28290486e-02  3.87674570e-02 ...  1.93669498e-01\n",
      "     9.28290486e-02  3.87674570e-02]\n",
      "   [ 2.00252056e-01  1.08599365e-01  6.73388541e-02 ...  2.00252056e-01\n",
      "     1.08599365e-01  6.73388541e-02]\n",
      "   ...\n",
      "   [ 2.02100813e-01  1.45350218e-01  1.05714202e-01 ...  2.02100813e-01\n",
      "     1.45350218e-01  1.05714202e-01]\n",
      "   [ 2.63641417e-01  2.82576948e-01  2.70140052e-01 ...  2.63641417e-01\n",
      "     2.82576948e-01  2.70140052e-01]\n",
      "   [ 2.87842989e-01  2.61568606e-01  2.32520878e-01 ...  2.87842989e-01\n",
      "     2.61568606e-01  2.32520878e-01]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 3.64509970e-01  2.94453889e-01  3.13753605e-01 ...  3.64509970e-01\n",
      "     2.94453889e-01  3.13753605e-01]\n",
      "   [ 3.63893569e-01  3.04285616e-01  3.40504259e-01 ...  3.63893569e-01\n",
      "     3.04285616e-01  3.40504259e-01]\n",
      "   [ 3.31064522e-01  3.19047630e-01  3.59804034e-01 ...  3.31064522e-01\n",
      "     3.19047630e-01  3.59804034e-01]\n",
      "   ...\n",
      "   [ 3.38767588e-01  3.01708639e-01  3.17534924e-01 ...  3.38767588e-01\n",
      "     3.01708639e-01  3.17534924e-01]\n",
      "   [ 3.54257882e-01  3.06610703e-01  3.06610644e-01 ...  3.54257882e-01\n",
      "     3.06610703e-01  3.06610644e-01]\n",
      "   [ 3.94677788e-01  3.12605143e-01  1.51204616e-01 ...  3.94677788e-01\n",
      "     3.12605143e-01  1.51204616e-01]]\n",
      "\n",
      "  [[ 3.58599484e-01  2.93053299e-01  3.17423046e-01 ...  3.58599484e-01\n",
      "     2.93053299e-01  3.17423046e-01]\n",
      "   [ 3.69355857e-01  3.41344506e-01  3.75490099e-01 ...  3.69355857e-01\n",
      "     3.41344506e-01  3.75490099e-01]\n",
      "   [ 3.24061811e-01  2.82661140e-01  3.17086875e-01 ...  3.24061811e-01\n",
      "     2.82661140e-01  3.17086875e-01]\n",
      "   ...\n",
      "   [ 3.29187751e-01  3.00448120e-01  3.07142854e-01 ...  3.29187751e-01\n",
      "     3.00448120e-01  3.07142854e-01]\n",
      "   [ 3.41260493e-01  3.12716961e-01  3.28879446e-01 ...  3.41260493e-01\n",
      "     3.12716961e-01  3.28879446e-01]\n",
      "   [ 3.65882218e-01  3.02241027e-01  3.16106468e-01 ...  3.65882218e-01\n",
      "     3.02241027e-01  3.16106468e-01]]\n",
      "\n",
      "  [[ 3.62381101e-01  3.25069934e-01  3.45714271e-01 ...  3.62381101e-01\n",
      "     3.25069934e-01  3.45714271e-01]\n",
      "   [ 3.56890798e-01  3.28823507e-01  3.68627459e-01 ...  3.56890798e-01\n",
      "     3.28823507e-01  3.68627459e-01]\n",
      "   [ 3.14145923e-01  2.60504305e-01  2.83529460e-01 ...  3.14145923e-01\n",
      "     2.60504305e-01  2.83529460e-01]\n",
      "   ...\n",
      "   [ 3.34930122e-01  2.70336032e-01  1.13781631e-01 ...  3.34930122e-01\n",
      "     2.70336032e-01  1.13781631e-01]\n",
      "   [ 3.38851452e-01  2.94481635e-01  3.13165247e-01 ...  3.38851452e-01\n",
      "     2.94481635e-01  3.13165247e-01]\n",
      "   [ 3.57086837e-01  3.16666752e-01  3.30028057e-01 ...  3.57086837e-01\n",
      "     3.16666752e-01  3.30028057e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 3.84509802e-01  3.36694598e-01  3.85378093e-01 ...  3.84509802e-01\n",
      "     3.36694598e-01  3.85378093e-01]\n",
      "   [ 3.46778750e-01  3.06862772e-01  3.32436979e-01 ...  3.46778750e-01\n",
      "     3.06862772e-01  3.32436979e-01]\n",
      "   [ 3.33669543e-01  1.15126133e-01  2.89271712e-01 ...  3.33669543e-01\n",
      "     1.15126133e-01  2.89271712e-01]\n",
      "   ...\n",
      "   [ 3.29551995e-01  8.45937133e-02  2.63725460e-01 ...  3.29551995e-01\n",
      "     8.45937133e-02  2.63725460e-01]\n",
      "   [ 3.26862752e-01  8.06161165e-02  1.09495878e-01 ...  3.26862752e-01\n",
      "     8.06161165e-02  1.09495878e-01]\n",
      "   [ 3.03809464e-01  2.46722817e-01  2.52549082e-01 ...  3.03809464e-01\n",
      "     2.46722817e-01  2.52549082e-01]]\n",
      "\n",
      "  [[ 3.91008437e-01  3.73921633e-01  4.07339007e-01 ...  3.91008437e-01\n",
      "     3.73921633e-01  4.07339007e-01]\n",
      "   [ 3.40196073e-01  3.04789901e-01  3.23585451e-01 ...  3.40196073e-01\n",
      "     3.04789901e-01  3.23585451e-01]\n",
      "   [ 3.44874024e-01  1.22605085e-01  1.35910332e-01 ...  3.44874024e-01\n",
      "     1.22605085e-01  1.35910332e-01]\n",
      "   ...\n",
      "   [ 3.42521012e-01  1.05266094e-01  2.82100648e-01 ...  3.42521012e-01\n",
      "     1.05266094e-01  2.82100648e-01]\n",
      "   [ 3.45238030e-01  1.15770191e-01  2.72913188e-01 ...  3.45238030e-01\n",
      "     1.15770191e-01  2.72913188e-01]\n",
      "   [ 3.16190541e-01  2.54985869e-01  2.89159656e-01 ...  3.16190541e-01\n",
      "     2.54985869e-01  2.89159656e-01]]\n",
      "\n",
      "  [[ 4.15434301e-01  4.03725594e-01  4.47675258e-01 ...  4.15434301e-01\n",
      "     4.03725594e-01  4.47675258e-01]\n",
      "   [ 3.46610665e-01  1.28123164e-01  1.32885098e-01 ...  3.46610665e-01\n",
      "     1.28123164e-01  1.32885098e-01]\n",
      "   [ 3.53193223e-01  1.32128775e-01  1.41848654e-01 ...  3.53193223e-01\n",
      "     1.32128775e-01  1.41848654e-01]\n",
      "   ...\n",
      "   [ 3.47198844e-01  1.02212965e-01  2.82184780e-01 ...  3.47198844e-01\n",
      "     1.02212965e-01  2.82184780e-01]\n",
      "   [ 3.34229648e-01  2.82576948e-01  3.01512599e-01 ...  3.34229648e-01\n",
      "     2.82576948e-01  3.01512599e-01]\n",
      "   [ 3.34901810e-01  3.04705858e-01  3.18795383e-01 ...  3.34901810e-01\n",
      "     3.04705858e-01  3.18795383e-01]]]\n",
      "\n",
      "\n",
      " [[[-3.60980242e-01 -3.68291229e-01 -3.56834650e-01 ... -3.60980242e-01\n",
      "    -3.68291229e-01 -3.56834650e-01]\n",
      "   [-2.51792699e-01 -2.91792810e-01 -2.71260440e-01 ... -2.51792699e-01\n",
      "    -2.91792810e-01 -2.71260440e-01]\n",
      "   [-2.68935472e-01 -2.77030826e-01 -2.51960695e-01 ... -2.68935472e-01\n",
      "    -2.77030826e-01 -2.51960695e-01]\n",
      "   ...\n",
      "   [-2.57310838e-01 -2.98291385e-01 -1.18768513e-02 ... -2.57310838e-01\n",
      "    -2.98291385e-01 -1.18768513e-02]\n",
      "   [-2.37898976e-01 -2.62016773e-01  8.57141614e-03 ... -2.37898976e-01\n",
      "    -2.62016773e-01  8.57141614e-03]\n",
      "   [-2.24930048e-01 -2.63865471e-01 -2.52716959e-01 ... -2.24930048e-01\n",
      "    -2.63865471e-01 -2.52716959e-01]]\n",
      "\n",
      "  [[-2.88459361e-01 -3.03025126e-01 -2.98263252e-01 ... -2.88459361e-01\n",
      "    -3.03025126e-01 -2.98263252e-01]\n",
      "   [-2.50251979e-01 -2.78263330e-01 -2.63725579e-01 ... -2.50251979e-01\n",
      "    -2.78263330e-01 -2.63725579e-01]\n",
      "   [-2.72016615e-01 -3.01652610e-01 -2.78991580e-01 ... -2.72016615e-01\n",
      "    -3.01652610e-01 -2.78991580e-01]\n",
      "   ...\n",
      "   [-2.74733812e-01 -2.95630336e-01 -1.44257843e-02 ... -2.74733812e-01\n",
      "    -2.95630336e-01 -1.44257843e-02]\n",
      "   [-2.58739501e-01 -2.71596789e-01  1.12323761e-02 ... -2.58739501e-01\n",
      "    -2.71596789e-01  1.12323761e-02]\n",
      "   [-2.30196208e-01 -2.54621744e-01  2.59103775e-02 ... -2.30196208e-01\n",
      "    -2.54621744e-01  2.59103775e-02]]\n",
      "\n",
      "  [[-2.57226735e-01 -2.67086923e-01 -2.58207321e-01 ... -2.57226735e-01\n",
      "    -2.67086923e-01 -2.58207321e-01]\n",
      "   [-2.54873902e-01 -2.71176517e-01  7.84313679e-03 ... -2.54873902e-01\n",
      "    -2.71176517e-01  7.84313679e-03]\n",
      "   [-2.74089366e-01 -3.04201603e-01 -2.81176448e-01 ... -2.74089366e-01\n",
      "    -3.04201603e-01 -2.81176448e-01]\n",
      "   ...\n",
      "   [-2.61148304e-01 -2.98291445e-01 -1.56301558e-02 ... -2.61148304e-01\n",
      "    -2.98291445e-01 -1.56301558e-02]\n",
      "   [-2.65070111e-01 -2.93753684e-01 -8.40339065e-03 ... -2.65070111e-01\n",
      "    -2.93753684e-01 -8.40339065e-03]\n",
      "   [-2.42913157e-01 -2.71568537e-01  6.16282225e-04 ... -2.42913157e-01\n",
      "    -2.71568537e-01  6.16282225e-04]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-2.86078423e-01 -3.18207383e-01 -3.03081870e-02 ... -2.86078423e-01\n",
      "    -3.18207383e-01 -3.03081870e-02]\n",
      "   [-2.84593791e-01 -3.00980389e-01 -2.44257748e-02 ... -2.84593791e-01\n",
      "    -3.00980389e-01 -2.44257748e-02]\n",
      "   [-2.82016724e-01 -3.04481745e-01 -3.02885175e-01 ... -2.82016724e-01\n",
      "    -3.04481745e-01 -3.02885175e-01]\n",
      "   ...\n",
      "   [-2.66526431e-01 -2.95798481e-01 -2.97058880e-01 ... -2.66526431e-01\n",
      "    -2.95798481e-01 -2.97058880e-01]\n",
      "   [-2.53529400e-01 -2.72325099e-01 -2.59131610e-01 ... -2.53529400e-01\n",
      "    -2.72325099e-01 -2.59131610e-01]\n",
      "   [-2.49131709e-01 -2.51316428e-01 -2.33725443e-01 ... -2.49131709e-01\n",
      "    -2.51316428e-01 -2.33725443e-01]]\n",
      "\n",
      "  [[-3.30560207e-01 -3.35882306e-01 -3.25994313e-01 ... -3.30560207e-01\n",
      "    -3.35882306e-01 -3.25994313e-01]\n",
      "   [-1.66666508e-02 -2.46218741e-02 -5.82632422e-03 ... -1.66666508e-02\n",
      "    -2.46218741e-02 -5.82632422e-03]\n",
      "   [-2.82576948e-01 -3.08767498e-01 -3.07226956e-01 ... -2.82576948e-01\n",
      "    -3.08767498e-01 -3.07226956e-01]\n",
      "   ...\n",
      "   [-2.61400551e-01 -2.98655510e-01 -2.90448368e-01 ... -2.61400551e-01\n",
      "    -2.98655510e-01 -2.90448368e-01]\n",
      "   [-2.54761964e-01 -2.72465110e-01 -2.83949554e-01 ... -2.54761964e-01\n",
      "    -2.72465110e-01 -2.83949554e-01]\n",
      "   [-2.32829064e-01 -2.46974930e-01 -2.12801144e-01 ... -2.32829064e-01\n",
      "    -2.46974930e-01 -2.12801144e-01]]\n",
      "\n",
      "  [[-4.00251985e-01 -4.00195986e-01 -3.87618870e-01 ... -4.00251985e-01\n",
      "    -4.00195986e-01 -3.87618870e-01]\n",
      "   [-6.33049011e-03 -2.87396014e-02 -2.39776671e-02 ... -6.33049011e-03\n",
      "    -2.87396014e-02 -2.39776671e-02]\n",
      "   [-2.74257749e-01 -3.03165376e-01 -3.01288605e-01 ... -2.74257749e-01\n",
      "    -3.03165376e-01 -3.01288605e-01]\n",
      "   ...\n",
      "   [-2.60644287e-01 -2.89943933e-01 -2.70756423e-01 ... -2.60644287e-01\n",
      "    -2.89943933e-01 -2.70756423e-01]\n",
      "   [-2.57927209e-01 -2.46834829e-01 -2.39663884e-01 ... -2.57927209e-01\n",
      "    -2.46834829e-01 -2.39663884e-01]\n",
      "   [-2.21960932e-01 -2.16862783e-01 -2.14537963e-01 ... -2.21960932e-01\n",
      "    -2.16862783e-01 -2.14537963e-01]]]\n",
      "\n",
      "\n",
      " [[[-3.80588084e-01 -3.76134366e-01 -3.68599355e-01 ... -3.80588084e-01\n",
      "    -3.76134366e-01 -3.68599355e-01]\n",
      "   [-1.96890742e-01 -2.56498694e-01 -2.79103577e-01 ... -1.96890742e-01\n",
      "    -2.56498694e-01 -2.79103577e-01]\n",
      "   [-4.14844751e-02 -1.08403355e-01 -1.38235182e-01 ... -4.14844751e-02\n",
      "    -1.08403355e-01 -1.38235182e-01]\n",
      "   ...\n",
      "   [-1.16134375e-01 -1.96330577e-01 -2.11876839e-01 ... -1.16134375e-01\n",
      "    -1.96330577e-01 -2.11876839e-01]\n",
      "   [-3.20251942e-01 -2.73781478e-01 -3.01232517e-01 ... -3.20251942e-01\n",
      "    -2.73781478e-01 -3.01232517e-01]\n",
      "   [-3.77871245e-01 -3.89355659e-01 -3.97815019e-01 ... -3.77871245e-01\n",
      "    -3.89355659e-01 -3.97815019e-01]]\n",
      "\n",
      "  [[-2.72773087e-01 -2.79495716e-01 -2.90420115e-01 ... -2.72773087e-01\n",
      "    -2.79495716e-01 -2.90420115e-01]\n",
      "   [-8.16245377e-02 -1.72380984e-01 -2.04902053e-01 ... -8.16245377e-02\n",
      "    -1.72380984e-01 -2.04902053e-01]\n",
      "   [ 1.42579079e-02 -5.06722033e-02 -6.33052886e-02 ...  1.42579079e-02\n",
      "    -5.06722033e-02 -6.33052886e-02]\n",
      "   ...\n",
      "   [-6.29690886e-02 -1.11316592e-01 -1.12464994e-01 ... -6.29690886e-02\n",
      "    -1.11316592e-01 -1.12464994e-01]\n",
      "   [-2.50896364e-01 -2.95126200e-01 -3.26022506e-01 ... -2.50896364e-01\n",
      "    -2.95126200e-01 -3.26022506e-01]\n",
      "   [-3.04706037e-01 -3.05602133e-01 -3.74089628e-01 ... -3.04706037e-01\n",
      "    -3.05602133e-01 -3.74089628e-01]]\n",
      "\n",
      "  [[-1.74873799e-01 -2.23949671e-01 -2.50364184e-01 ... -1.74873799e-01\n",
      "    -2.23949671e-01 -2.50364184e-01]\n",
      "   [-1.95797682e-02 -1.10392183e-01 -1.25490189e-01 ... -1.95797682e-02\n",
      "    -1.10392183e-01 -1.25490189e-01]\n",
      "   [ 5.53224087e-02 -1.40054822e-02 -2.62744725e-02 ...  5.53224087e-02\n",
      "    -1.40054822e-02 -2.62744725e-02]\n",
      "   ...\n",
      "   [-1.40894651e-02 -7.86835849e-02 -8.22968185e-02 ... -1.40894651e-02\n",
      "    -7.86835849e-02 -8.22968185e-02]\n",
      "   [-1.74874038e-01 -2.46694833e-01 -2.82913208e-01 ... -1.74874038e-01\n",
      "    -2.46694833e-01 -2.82913208e-01]\n",
      "   [-3.40952396e-01 -3.57843071e-01 -3.91540587e-01 ... -3.40952396e-01\n",
      "    -3.57843071e-01 -3.91540587e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.98039412e-02 -9.07563865e-02 -1.79327786e-01 ...  1.98039412e-02\n",
      "    -9.07563865e-02 -1.79327786e-01]\n",
      "   [-6.16240501e-03 -9.31372344e-02 -1.45994395e-01 ... -6.16240501e-03\n",
      "    -9.31372344e-02 -1.45994395e-01]\n",
      "   [ 4.34734821e-02 -2.99719274e-02 -7.93557465e-02 ...  4.34734821e-02\n",
      "    -2.99719274e-02 -7.93557465e-02]\n",
      "   ...\n",
      "   [ 4.32775021e-02  5.32211661e-02  4.01960611e-02 ...  4.32775021e-02\n",
      "     5.32211661e-02  4.01960611e-02]\n",
      "   [-6.13725185e-02 -1.07619196e-01 -1.25798255e-01 ... -6.13725185e-02\n",
      "    -1.07619196e-01 -1.25798255e-01]\n",
      "   [-1.31484658e-01 -1.72885031e-01 -1.82745039e-01 ... -1.31484658e-01\n",
      "    -1.72885031e-01 -1.82745039e-01]]\n",
      "\n",
      "  [[-1.07030779e-01 -1.98627383e-01 -2.39719823e-01 ... -1.07030779e-01\n",
      "    -1.98627383e-01 -2.39719823e-01]\n",
      "   [-2.84313560e-02 -1.10896379e-01 -1.58767492e-01 ... -2.84313560e-02\n",
      "    -1.10896379e-01 -1.58767492e-01]\n",
      "   [ 2.72269845e-02 -4.99439538e-02 -1.07226938e-01 ...  2.72269845e-02\n",
      "    -4.99439538e-02 -1.07226938e-01]\n",
      "   ...\n",
      "   [ 1.70308352e-02 -3.98319662e-02 -5.12326956e-02 ...  1.70308352e-02\n",
      "    -3.98319662e-02 -5.12326956e-02]\n",
      "   [-5.47619462e-02 -9.59945321e-02 -1.27086818e-01 ... -5.47619462e-02\n",
      "    -9.59945321e-02 -1.27086818e-01]\n",
      "   [-1.97534949e-01 -2.43053362e-01 -2.48095259e-01 ... -1.97534949e-01\n",
      "    -2.43053362e-01 -2.48095259e-01]]\n",
      "\n",
      "  [[-1.45350009e-01 -1.64901853e-01 -2.73893356e-01 ... -1.45350009e-01\n",
      "    -1.64901853e-01 -2.73893356e-01]\n",
      "   [-4.94677424e-02 -1.42465085e-01 -1.84761971e-01 ... -4.94677424e-02\n",
      "    -1.42465085e-01 -1.84761971e-01]\n",
      "   [ 8.09520483e-03 -6.78712428e-02 -1.01288617e-01 ...  8.09520483e-03\n",
      "    -6.78712428e-02 -1.01288617e-01]\n",
      "   ...\n",
      "   [-9.66387987e-03 -5.46498001e-02 -6.29132688e-02 ... -9.66387987e-03\n",
      "    -5.46498001e-02 -6.29132688e-02]\n",
      "   [-8.92997682e-02 -1.21344626e-01 -1.45546228e-01 ... -8.92997682e-02\n",
      "    -1.21344626e-01 -1.45546228e-01]\n",
      "   [-2.18039364e-01 -2.44313762e-01 -2.77283072e-01 ... -2.18039364e-01\n",
      "    -2.44313762e-01 -2.77283072e-01]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 11s 1s/step - loss: 2.7383 - accuracy: 0.1286 - val_loss: 2.8878 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/100\n",
      "Learning rate:  0.0033333333333333335\n",
      "5/5 [==============================] - 6s 1s/step - loss: 2.7462 - accuracy: 0.1357 - val_loss: 2.8791 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/100\n",
      "Learning rate:  0.006666666666666667\n",
      "5/5 [==============================] - 4s 854ms/step - loss: 2.7055 - accuracy: 0.1571 - val_loss: 2.8742 - val_accuracy: 0.4043\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/100\n",
      "Learning rate:  0.01\n",
      "5/5 [==============================] - 5s 1s/step - loss: 2.6679 - accuracy: 0.1714 - val_loss: 2.8660 - val_accuracy: 0.4043\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/100\n",
      "Learning rate:  0.013333333333333334\n",
      "5/5 [==============================] - 5s 1s/step - loss: 2.6235 - accuracy: 0.2357 - val_loss: 2.8564 - val_accuracy: 0.4043\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/100\n",
      "Learning rate:  0.016666666666666666\n",
      "5/5 [==============================] - 4s 789ms/step - loss: 2.5600 - accuracy: 0.3071 - val_loss: 2.8552 - val_accuracy: 0.4043\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/100\n",
      "Learning rate:  0.02\n",
      "5/5 [==============================] - 5s 1s/step - loss: 2.5111 - accuracy: 0.3214 - val_loss: 2.8620 - val_accuracy: 0.4043\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/100\n",
      "Learning rate:  0.023333333333333334\n",
      "5/5 [==============================] - 5s 841ms/step - loss: 2.4320 - accuracy: 0.4214 - val_loss: 2.8658 - val_accuracy: 0.4043\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/100\n",
      "Learning rate:  0.02666666666666667\n",
      "5/5 [==============================] - 4s 752ms/step - loss: 2.3606 - accuracy: 0.4286 - val_loss: 2.8669 - val_accuracy: 0.4043\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/100\n",
      "Learning rate:  0.03\n",
      "5/5 [==============================] - 4s 749ms/step - loss: 2.2824 - accuracy: 0.4857 - val_loss: 2.8741 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/100\n",
      "Learning rate:  0.03333333333333333\n",
      "5/5 [==============================] - 4s 736ms/step - loss: 2.1872 - accuracy: 0.4929 - val_loss: 2.8840 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/100\n",
      "Learning rate:  0.03666666666666667\n",
      "5/5 [==============================] - 4s 749ms/step - loss: 2.1266 - accuracy: 0.5000 - val_loss: 2.9027 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/100\n",
      "Learning rate:  0.04\n",
      "5/5 [==============================] - 4s 753ms/step - loss: 2.0752 - accuracy: 0.4929 - val_loss: 2.9283 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/100\n",
      "Learning rate:  0.043333333333333335\n",
      "5/5 [==============================] - 4s 749ms/step - loss: 2.0025 - accuracy: 0.5214 - val_loss: 2.9932 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/100\n",
      "Learning rate:  0.04666666666666667\n",
      "5/5 [==============================] - 4s 744ms/step - loss: 1.9226 - accuracy: 0.5714 - val_loss: 3.0614 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/100\n",
      "Learning rate:  0.05\n",
      "5/5 [==============================] - 4s 729ms/step - loss: 1.8653 - accuracy: 0.5214 - val_loss: 3.0951 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/100\n",
      "Learning rate:  0.05333333333333334\n",
      "5/5 [==============================] - 4s 839ms/step - loss: 1.7942 - accuracy: 0.5643 - val_loss: 3.1678 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/100\n",
      "Learning rate:  0.056666666666666664\n",
      "5/5 [==============================] - 4s 738ms/step - loss: 1.7180 - accuracy: 0.5857 - val_loss: 3.3111 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/100\n",
      "Learning rate:  0.06\n",
      "5/5 [==============================] - 4s 732ms/step - loss: 1.7261 - accuracy: 0.6071 - val_loss: 3.3772 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/100\n",
      "Learning rate:  0.06333333333333334\n",
      "5/5 [==============================] - 4s 758ms/step - loss: 1.6583 - accuracy: 0.6429 - val_loss: 3.4450 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 21/100\n",
      "Learning rate:  0.06666666666666667\n",
      "5/5 [==============================] - 4s 758ms/step - loss: 1.6239 - accuracy: 0.6286 - val_loss: 3.6773 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 22/100\n",
      "Learning rate:  0.07\n",
      "5/5 [==============================] - 4s 756ms/step - loss: 1.5571 - accuracy: 0.6714 - val_loss: 4.1294 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 23/100\n",
      "Learning rate:  0.07333333333333333\n",
      "5/5 [==============================] - 4s 743ms/step - loss: 1.5077 - accuracy: 0.6500 - val_loss: 4.6844 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 24/100\n",
      "Learning rate:  0.07666666666666666\n",
      "5/5 [==============================] - 4s 739ms/step - loss: 1.4579 - accuracy: 0.6571 - val_loss: 4.8769 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 25/100\n",
      "Learning rate:  0.08\n",
      "5/5 [==============================] - 4s 739ms/step - loss: 1.4203 - accuracy: 0.7000 - val_loss: 5.0351 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 26/100\n",
      "Learning rate:  0.08333333333333333\n",
      "5/5 [==============================] - 4s 853ms/step - loss: 1.4605 - accuracy: 0.6786 - val_loss: 5.0360 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 27/100\n",
      "Learning rate:  0.08666666666666667\n",
      "5/5 [==============================] - 4s 747ms/step - loss: 1.3409 - accuracy: 0.7143 - val_loss: 4.6480 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 28/100\n",
      "Learning rate:  0.09\n",
      "5/5 [==============================] - 4s 735ms/step - loss: 1.3506 - accuracy: 0.7214 - val_loss: 4.7633 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 29/100\n",
      "Learning rate:  0.09333333333333334\n",
      "5/5 [==============================] - 4s 734ms/step - loss: 1.3459 - accuracy: 0.6429 - val_loss: 5.2411 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 30/100\n",
      "Learning rate:  0.09666666666666666\n",
      "5/5 [==============================] - 4s 775ms/step - loss: 1.3781 - accuracy: 0.6500 - val_loss: 5.2810 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 31/100\n",
      "Learning rate:  0.1\n",
      "5/5 [==============================] - 4s 741ms/step - loss: 1.2891 - accuracy: 0.7571 - val_loss: 4.7699 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 32/100\n",
      "Learning rate:  0.10333333333333333\n",
      "5/5 [==============================] - 4s 853ms/step - loss: 1.3000 - accuracy: 0.7429 - val_loss: 4.2353 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 33/100\n",
      "Learning rate:  0.10666666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 4s 738ms/step - loss: 1.2367 - accuracy: 0.7571 - val_loss: 4.5669 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 34/100\n",
      "Learning rate:  0.11\n",
      "5/5 [==============================] - 4s 741ms/step - loss: 1.2403 - accuracy: 0.7571 - val_loss: 4.4370 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 35/100\n",
      "Learning rate:  0.11333333333333333\n",
      "5/5 [==============================] - 4s 744ms/step - loss: 1.1666 - accuracy: 0.8071 - val_loss: 5.0076 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 36/100\n",
      "Learning rate:  0.11666666666666667\n",
      "5/5 [==============================] - 4s 751ms/step - loss: 1.1534 - accuracy: 0.7786 - val_loss: 4.8112 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 37/100\n",
      "Learning rate:  0.12\n",
      "5/5 [==============================] - 4s 741ms/step - loss: 1.1478 - accuracy: 0.7929 - val_loss: 3.3346 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 38/100\n",
      "Learning rate:  0.12333333333333334\n",
      "5/5 [==============================] - 4s 871ms/step - loss: 1.2032 - accuracy: 0.7857 - val_loss: 3.2537 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 39/100\n",
      "Learning rate:  0.12666666666666668\n",
      "5/5 [==============================] - 4s 773ms/step - loss: 1.1686 - accuracy: 0.7500 - val_loss: 4.4997 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 40/100\n",
      "Learning rate:  0.13\n",
      "5/5 [==============================] - 4s 862ms/step - loss: 1.1781 - accuracy: 0.7500 - val_loss: 5.1547 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 41/100\n",
      "Learning rate:  0.13333333333333333\n",
      "5/5 [==============================] - 4s 748ms/step - loss: 1.0946 - accuracy: 0.8214 - val_loss: 4.0662 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 42/100\n",
      "Learning rate:  0.13666666666666666\n",
      "5/5 [==============================] - 4s 853ms/step - loss: 1.0982 - accuracy: 0.8000 - val_loss: 4.9642 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 43/100\n",
      "Learning rate:  0.14\n",
      "5/5 [==============================] - 4s 751ms/step - loss: 1.2179 - accuracy: 0.7929 - val_loss: 6.5235 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 44/100\n",
      "Learning rate:  0.14333333333333334\n",
      "5/5 [==============================] - 4s 729ms/step - loss: 1.1310 - accuracy: 0.7929 - val_loss: 7.4278 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 45/100\n",
      "Learning rate:  0.14666666666666667\n",
      "5/5 [==============================] - 4s 735ms/step - loss: 1.0450 - accuracy: 0.8357 - val_loss: 7.2417 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 46/100\n",
      "Learning rate:  0.15\n",
      "5/5 [==============================] - 4s 762ms/step - loss: 1.0611 - accuracy: 0.8214 - val_loss: 8.5174 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 47/100\n",
      "Learning rate:  0.15333333333333332\n",
      "5/5 [==============================] - 4s 952ms/step - loss: 0.9913 - accuracy: 0.8714 - val_loss: 8.6789 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 48/100\n",
      "Learning rate:  0.15666666666666668\n",
      "5/5 [==============================] - 6s 1s/step - loss: 1.0658 - accuracy: 0.8214 - val_loss: 9.4731 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 49/100\n",
      "Learning rate:  0.16\n",
      "5/5 [==============================] - 4s 802ms/step - loss: 1.0226 - accuracy: 0.8286 - val_loss: 9.7967 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 50/100\n",
      "Learning rate:  0.16333333333333333\n",
      "5/5 [==============================] - 4s 759ms/step - loss: 0.9713 - accuracy: 0.8571 - val_loss: 9.4957 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 51/100\n",
      "Learning rate:  0.16666666666666666\n",
      "5/5 [==============================] - 6s 1s/step - loss: 1.1175 - accuracy: 0.7929 - val_loss: 10.9578 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 52/100\n",
      "Learning rate:  0.17\n",
      "5/5 [==============================] - 5s 1s/step - loss: 1.1041 - accuracy: 0.7929 - val_loss: 9.0447 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 53/100\n",
      "Learning rate:  0.17333333333333334\n",
      "5/5 [==============================] - 4s 857ms/step - loss: 1.0448 - accuracy: 0.8071 - val_loss: 8.9436 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 54/100\n",
      "Learning rate:  0.17666666666666667\n",
      "5/5 [==============================] - 4s 718ms/step - loss: 1.1060 - accuracy: 0.8357 - val_loss: 9.9168 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 55/100\n",
      "Learning rate:  0.18\n",
      "5/5 [==============================] - 4s 734ms/step - loss: 0.9728 - accuracy: 0.8429 - val_loss: 10.4743 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 56/100\n",
      "Learning rate:  0.18333333333333332\n",
      "5/5 [==============================] - 4s 725ms/step - loss: 1.0134 - accuracy: 0.8143 - val_loss: 11.0682 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 57/100\n",
      "Learning rate:  0.18666666666666668\n",
      "5/5 [==============================] - 4s 726ms/step - loss: 0.9833 - accuracy: 0.8357 - val_loss: 10.4981 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 58/100\n",
      "Learning rate:  0.19\n",
      "5/5 [==============================] - 4s 718ms/step - loss: 0.9566 - accuracy: 0.8714 - val_loss: 9.6950 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 59/100\n",
      "Learning rate:  0.19333333333333333\n",
      "5/5 [==============================] - 4s 711ms/step - loss: 1.0313 - accuracy: 0.7857 - val_loss: 12.1439 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 60/100\n",
      "Learning rate:  0.19666666666666666\n",
      "5/5 [==============================] - 4s 840ms/step - loss: 1.1140 - accuracy: 0.7643 - val_loss: 9.1441 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 61/100\n",
      "Learning rate:  0.2\n",
      "5/5 [==============================] - 4s 848ms/step - loss: 1.0250 - accuracy: 0.7857 - val_loss: 12.5150 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 62/100\n",
      "Learning rate:  0.20333333333333334\n",
      "5/5 [==============================] - 4s 718ms/step - loss: 0.9970 - accuracy: 0.8500 - val_loss: 10.5455 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 63/100\n",
      "Learning rate:  0.20666666666666667\n",
      "5/5 [==============================] - 4s 724ms/step - loss: 0.9257 - accuracy: 0.8571 - val_loss: 7.1833 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 64/100\n",
      "Learning rate:  0.21\n",
      "5/5 [==============================] - 4s 749ms/step - loss: 1.0088 - accuracy: 0.8643 - val_loss: 6.3259 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 65/100\n",
      "Learning rate:  0.21333333333333335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 4s 835ms/step - loss: 1.0313 - accuracy: 0.8143 - val_loss: 7.8269 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 66/100\n",
      "Learning rate:  0.21666666666666667\n",
      "5/5 [==============================] - 4s 722ms/step - loss: 1.0003 - accuracy: 0.8429 - val_loss: 8.1524 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 67/100\n",
      "Learning rate:  0.22\n",
      "5/5 [==============================] - 4s 729ms/step - loss: 0.9493 - accuracy: 0.8857 - val_loss: 6.7237 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 68/100\n",
      "Learning rate:  0.22333333333333333\n",
      "5/5 [==============================] - 4s 734ms/step - loss: 0.9213 - accuracy: 0.8571 - val_loss: 7.6688 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 69/100\n",
      "Learning rate:  0.22666666666666666\n",
      "5/5 [==============================] - 4s 740ms/step - loss: 0.9299 - accuracy: 0.8429 - val_loss: 9.9127 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 70/100\n",
      "Learning rate:  0.23\n",
      "5/5 [==============================] - 4s 773ms/step - loss: 0.9687 - accuracy: 0.8786 - val_loss: 9.6624 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 71/100\n",
      "Learning rate:  0.23333333333333334\n",
      "5/5 [==============================] - 4s 880ms/step - loss: 0.9552 - accuracy: 0.8571 - val_loss: 10.8586 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 72/100\n",
      "Learning rate:  0.23666666666666666\n",
      "5/5 [==============================] - 4s 753ms/step - loss: 0.9782 - accuracy: 0.8571 - val_loss: 9.0697 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 73/100\n",
      "Learning rate:  0.24\n",
      "5/5 [==============================] - 4s 752ms/step - loss: 0.9441 - accuracy: 0.8786 - val_loss: 5.5096 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 74/100\n",
      "Learning rate:  0.24333333333333335\n",
      "5/5 [==============================] - 4s 858ms/step - loss: 0.9979 - accuracy: 0.8571 - val_loss: 7.0764 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 75/100\n",
      "Learning rate:  0.24666666666666667\n",
      "5/5 [==============================] - 4s 742ms/step - loss: 1.0191 - accuracy: 0.8000 - val_loss: 8.4441 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 76/100\n",
      "Learning rate:  0.25\n",
      "5/5 [==============================] - 4s 842ms/step - loss: 0.9452 - accuracy: 0.8500 - val_loss: 9.6253 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 77/100\n",
      "Learning rate:  0.25333333333333335\n",
      "5/5 [==============================] - 4s 828ms/step - loss: 0.9146 - accuracy: 0.8643 - val_loss: 11.0765 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 78/100\n",
      "Learning rate:  0.25666666666666665\n",
      "5/5 [==============================] - 4s 717ms/step - loss: 0.8670 - accuracy: 0.8571 - val_loss: 10.3458 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 79/100\n",
      "Learning rate:  0.26\n",
      "5/5 [==============================] - 4s 707ms/step - loss: 0.8646 - accuracy: 0.8786 - val_loss: 8.1572 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 80/100\n",
      "Learning rate:  0.2633333333333333\n",
      "5/5 [==============================] - 4s 807ms/step - loss: 0.9177 - accuracy: 0.8857 - val_loss: 4.9127 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 81/100\n",
      "Learning rate:  0.26666666666666666\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.9532 - accuracy: 0.8643 - val_loss: 7.1079 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 82/100\n",
      "Learning rate:  0.27\n",
      "5/5 [==============================] - 4s 846ms/step - loss: 0.8890 - accuracy: 0.8786 - val_loss: 7.1322 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 83/100\n",
      "Learning rate:  0.2733333333333333\n",
      "5/5 [==============================] - 4s 971ms/step - loss: 0.9041 - accuracy: 0.8786 - val_loss: 7.1968 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 84/100\n",
      "Learning rate:  0.27666666666666667\n",
      "5/5 [==============================] - 4s 794ms/step - loss: 0.9826 - accuracy: 0.8571 - val_loss: 7.2829 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 85/100\n",
      "Learning rate:  0.28\n",
      "5/5 [==============================] - 4s 717ms/step - loss: 0.7739 - accuracy: 0.9357 - val_loss: 7.2639 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 86/100\n",
      "Learning rate:  0.2833333333333333\n",
      "5/5 [==============================] - 4s 725ms/step - loss: 0.8378 - accuracy: 0.8714 - val_loss: 7.4442 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 87/100\n",
      "Learning rate:  0.2866666666666667\n",
      "5/5 [==============================] - 4s 712ms/step - loss: 0.8661 - accuracy: 0.9000 - val_loss: 7.7331 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 88/100\n",
      "Learning rate:  0.29\n",
      "5/5 [==============================] - 4s 704ms/step - loss: 0.8329 - accuracy: 0.9071 - val_loss: 8.0744 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 89/100\n",
      "Learning rate:  0.29333333333333333\n",
      "5/5 [==============================] - 4s 695ms/step - loss: 0.7729 - accuracy: 0.9214 - val_loss: 8.4599 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 90/100\n",
      "Learning rate:  0.2966666666666667\n",
      "5/5 [==============================] - 4s 699ms/step - loss: 0.7882 - accuracy: 0.9429 - val_loss: 8.8502 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 91/100\n",
      "Learning rate:  0.3\n",
      "5/5 [==============================] - 4s 718ms/step - loss: 0.7765 - accuracy: 0.9214 - val_loss: 9.0130 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 92/100\n",
      "Learning rate:  0.30333333333333334\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.7949 - accuracy: 0.9286 - val_loss: 9.1560 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 93/100\n",
      "Learning rate:  0.30666666666666664\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.8831 - accuracy: 0.8857 - val_loss: 9.4584 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 94/100\n",
      "Learning rate:  0.31\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.7689 - accuracy: 0.9214 - val_loss: 9.6123 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 95/100\n",
      "Learning rate:  0.31333333333333335\n",
      "5/5 [==============================] - 4s 978ms/step - loss: 0.8177 - accuracy: 0.9000 - val_loss: 9.8835 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 96/100\n",
      "Learning rate:  0.31666666666666665\n",
      "5/5 [==============================] - 4s 726ms/step - loss: 0.7698 - accuracy: 0.9357 - val_loss: 10.1282 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 97/100\n",
      "Learning rate:  0.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 4s 708ms/step - loss: 0.7970 - accuracy: 0.9143 - val_loss: 10.3109 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 98/100\n",
      "Learning rate:  0.3233333333333333\n",
      "5/5 [==============================] - 4s 651ms/step - loss: 0.8297 - accuracy: 0.9000 - val_loss: 10.4458 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 99/100\n",
      "Learning rate:  0.32666666666666666\n",
      "5/5 [==============================] - 4s 662ms/step - loss: 0.7787 - accuracy: 0.9214 - val_loss: 10.1900 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 100/100\n",
      "Learning rate:  0.33\n",
      "5/5 [==============================] - 3s 655ms/step - loss: 0.7792 - accuracy: 0.9357 - val_loss: 9.9471 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.0352 - accuracy: 0.4468\n",
      "Test loss: 2.0352301597595215\n",
      "Test accuracy: 0.44680851697921753\n",
      "[2, 1, 1, 2, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 2, 2, 2, 2, 0, 0, 2, 1, 1, 0, 2, 2, 0, 0, 2, 2, 0, 2, 0, 1, 1, 2, 1, 0, 2, 0, 2, 2, 0, 1, 0, 0, 2]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2]\n",
      "Precision:  0.6534000834376303\n",
      "Recall:  0.44680851063829785\n",
      "F1 Score:  0.49964318752477865\n",
      "[[[[159 141 136 ... 159 141 136]\n",
      "   [182 148 136 ... 182 148 136]\n",
      "   [175 143 133 ... 175 143 133]\n",
      "   ...\n",
      "   [186 150 136 ... 186 150 136]\n",
      "   [181 148 137 ... 181 148 137]\n",
      "   [117  96  91 ... 117  96  91]]\n",
      "\n",
      "  [[176 147 138 ... 176 147 138]\n",
      "   [185 152 141 ... 185 152 141]\n",
      "   [176 143 135 ... 176 143 135]\n",
      "   ...\n",
      "   [188 152 138 ... 188 152 138]\n",
      "   [182 148 136 ... 182 148 136]\n",
      "   [158 129 122 ... 158 129 122]]\n",
      "\n",
      "  [[183 155 146 ... 183 155 146]\n",
      "   [182 153 144 ... 182 153 144]\n",
      "   [178 145 135 ... 178 145 135]\n",
      "   ...\n",
      "   [190 153 138 ... 190 153 138]\n",
      "   [183 147 132 ... 183 147 132]\n",
      "   [174 145 138 ... 174 145 138]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[180 151 143 ... 180 151 143]\n",
      "   [178 143 129 ... 178 143 129]\n",
      "   [189 153 141 ... 189 153 141]\n",
      "   ...\n",
      "   [192 147 126 ... 192 147 126]\n",
      "   [187 145 127 ... 187 145 127]\n",
      "   [178 144 131 ... 178 144 131]]\n",
      "\n",
      "  [[174 145 139 ... 174 145 139]\n",
      "   [180 149 139 ... 180 149 139]\n",
      "   [188 152 139 ... 188 152 139]\n",
      "   ...\n",
      "   [190 145 126 ... 190 145 126]\n",
      "   [185 148 133 ... 185 148 133]\n",
      "   [169 133 122 ... 169 133 122]]\n",
      "\n",
      "  [[168 140 134 ... 168 140 134]\n",
      "   [178 147 137 ... 178 147 137]\n",
      "   [184 145 131 ... 184 145 131]\n",
      "   ...\n",
      "   [190 150 136 ... 190 150 136]\n",
      "   [185 152 141 ... 185 152 141]\n",
      "   [161 125 113 ... 161 125 113]]]\n",
      "\n",
      "\n",
      " [[[102  98 100 ... 102  98 100]\n",
      "   [184 164 154 ... 184 164 154]\n",
      "   [196 169 155 ... 196 169 155]\n",
      "   ...\n",
      "   [ 97  85  82 ...  97  85  82]\n",
      "   [ 59  58  60 ...  59  58  60]\n",
      "   [ 77  76  78 ...  77  76  78]]\n",
      "\n",
      "  [[140 130 128 ... 140 130 128]\n",
      "   [198 174 163 ... 198 174 163]\n",
      "   [198 170 156 ... 198 170 156]\n",
      "   ...\n",
      "   [156 134 125 ... 156 134 125]\n",
      "   [ 61  60  61 ...  61  60  61]\n",
      "   [ 84  82  84 ...  84  82  84]]\n",
      "\n",
      "  [[175 157 150 ... 175 157 150]\n",
      "   [198 171 160 ... 198 171 160]\n",
      "   [199 169 156 ... 199 169 156]\n",
      "   ...\n",
      "   [193 164 150 ... 193 164 150]\n",
      "   [ 76  70  70 ...  76  70  70]\n",
      "   [ 82  81  83 ...  82  81  83]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[201 177 166 ... 201 177 166]\n",
      "   [201 174 160 ... 201 174 160]\n",
      "   [195 162 145 ... 195 162 145]\n",
      "   ...\n",
      "   [188 154 143 ... 188 154 143]\n",
      "   [195 169 160 ... 195 169 160]\n",
      "   [153 135 129 ... 153 135 129]]\n",
      "\n",
      "  [[194 171 159 ... 194 171 159]\n",
      "   [196 168 154 ... 196 168 154]\n",
      "   [197 165 149 ... 197 165 149]\n",
      "   ...\n",
      "   [189 157 146 ... 189 157 146]\n",
      "   [192 165 156 ... 192 165 156]\n",
      "   [123 112 109 ... 123 112 109]]\n",
      "\n",
      "  [[193 170 159 ... 193 170 159]\n",
      "   [195 168 155 ... 195 168 155]\n",
      "   [196 164 147 ... 196 164 147]\n",
      "   ...\n",
      "   [190 159 149 ... 190 159 149]\n",
      "   [183 158 150 ... 183 158 150]\n",
      "   [ 94  87  86 ...  94  87  86]]]\n",
      "\n",
      "\n",
      " [[[ 67  72  90 ...  67  72  90]\n",
      "   [132 115 124 ... 132 115 124]\n",
      "   [152 127 133 ... 152 127 133]\n",
      "   ...\n",
      "   [140 119 120 ... 140 119 120]\n",
      "   [132 106 107 ... 132 106 107]\n",
      "   [ 87  79  88 ...  87  79  88]]\n",
      "\n",
      "  [[ 84  83  98 ...  84  83  98]\n",
      "   [147 126 133 ... 147 126 133]\n",
      "   [153 125 128 ... 153 125 128]\n",
      "   ...\n",
      "   [148 134 138 ... 148 134 138]\n",
      "   [141 120 124 ... 141 120 124]\n",
      "   [111  93  98 ... 111  93  98]]\n",
      "\n",
      "  [[109 100 114 ... 109 100 114]\n",
      "   [154 133 140 ... 154 133 140]\n",
      "   [156 131 132 ... 156 131 132]\n",
      "   ...\n",
      "   [154 142 148 ... 154 142 148]\n",
      "   [148 133 141 ... 148 133 141]\n",
      "   [127 101 101 ... 127 101 101]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[101  94 105 ... 101  94 105]\n",
      "   [150 136 142 ... 150 136 142]\n",
      "   [156 142 147 ... 156 142 147]\n",
      "   ...\n",
      "   [157 138 139 ... 157 138 139]\n",
      "   [153 138 141 ... 153 138 141]\n",
      "   [139 128 133 ... 139 128 133]]\n",
      "\n",
      "  [[ 65  64  76 ...  65  64  76]\n",
      "   [143 128 134 ... 143 128 134]\n",
      "   [153 140 146 ... 153 140 146]\n",
      "   ...\n",
      "   [155 135 136 ... 155 135 136]\n",
      "   [152 138 140 ... 152 138 140]\n",
      "   [116 108 116 ... 116 108 116]]\n",
      "\n",
      "  [[ 46  49  64 ...  46  49  64]\n",
      "   [126 114 122 ... 126 114 122]\n",
      "   [151 138 141 ... 151 138 141]\n",
      "   ...\n",
      "   [154 133 136 ... 154 133 136]\n",
      "   [147 130 131 ... 147 130 131]\n",
      "   [ 83  79  88 ...  83  79  88]]]]\n",
      "[1 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (140, 40, 30, 9)\n",
      "140 train samples\n",
      "47 test samples\n",
      "y_train shape: (140,)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_310 (Conv2D)             (None, 40, 30, 16)   1312        input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_280 (BatchN (None, 40, 30, 16)   64          conv2d_310[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_280 (Activation)     (None, 40, 30, 16)   0           batch_normalization_280[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_311 (Conv2D)             (None, 40, 30, 16)   272         activation_280[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_281 (BatchN (None, 40, 30, 16)   64          conv2d_311[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_281 (Activation)     (None, 40, 30, 16)   0           batch_normalization_281[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_312 (Conv2D)             (None, 40, 30, 16)   2320        activation_281[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_282 (BatchN (None, 40, 30, 16)   64          conv2d_312[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_282 (Activation)     (None, 40, 30, 16)   0           batch_normalization_282[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_314 (Conv2D)             (None, 40, 30, 64)   1088        activation_280[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_313 (Conv2D)             (None, 40, 30, 64)   1088        activation_282[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_90 (Add)                    (None, 40, 30, 64)   0           conv2d_314[0][0]                 \n",
      "                                                                 conv2d_313[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_283 (BatchN (None, 40, 30, 64)   256         add_90[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_283 (Activation)     (None, 40, 30, 64)   0           batch_normalization_283[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_315 (Conv2D)             (None, 40, 30, 16)   1040        activation_283[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_284 (BatchN (None, 40, 30, 16)   64          conv2d_315[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_284 (Activation)     (None, 40, 30, 16)   0           batch_normalization_284[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_316 (Conv2D)             (None, 40, 30, 16)   2320        activation_284[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_285 (BatchN (None, 40, 30, 16)   64          conv2d_316[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_285 (Activation)     (None, 40, 30, 16)   0           batch_normalization_285[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_317 (Conv2D)             (None, 40, 30, 64)   1088        activation_285[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_91 (Add)                    (None, 40, 30, 64)   0           add_90[0][0]                     \n",
      "                                                                 conv2d_317[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_286 (BatchN (None, 40, 30, 64)   256         add_91[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_286 (Activation)     (None, 40, 30, 64)   0           batch_normalization_286[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_318 (Conv2D)             (None, 40, 30, 16)   1040        activation_286[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_287 (BatchN (None, 40, 30, 16)   64          conv2d_318[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_287 (Activation)     (None, 40, 30, 16)   0           batch_normalization_287[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_319 (Conv2D)             (None, 40, 30, 16)   2320        activation_287[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_288 (BatchN (None, 40, 30, 16)   64          conv2d_319[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_288 (Activation)     (None, 40, 30, 16)   0           batch_normalization_288[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_320 (Conv2D)             (None, 40, 30, 64)   1088        activation_288[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_92 (Add)                    (None, 40, 30, 64)   0           add_91[0][0]                     \n",
      "                                                                 conv2d_320[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_289 (BatchN (None, 40, 30, 64)   256         add_92[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_289 (Activation)     (None, 40, 30, 64)   0           batch_normalization_289[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_321 (Conv2D)             (None, 20, 15, 64)   4160        activation_289[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_290 (BatchN (None, 20, 15, 64)   256         conv2d_321[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_290 (Activation)     (None, 20, 15, 64)   0           batch_normalization_290[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_322 (Conv2D)             (None, 20, 15, 64)   36928       activation_290[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_291 (BatchN (None, 20, 15, 64)   256         conv2d_322[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_291 (Activation)     (None, 20, 15, 64)   0           batch_normalization_291[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_324 (Conv2D)             (None, 20, 15, 128)  8320        add_92[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_323 (Conv2D)             (None, 20, 15, 128)  8320        activation_291[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_93 (Add)                    (None, 20, 15, 128)  0           conv2d_324[0][0]                 \n",
      "                                                                 conv2d_323[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_292 (BatchN (None, 20, 15, 128)  512         add_93[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_292 (Activation)     (None, 20, 15, 128)  0           batch_normalization_292[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_325 (Conv2D)             (None, 20, 15, 64)   8256        activation_292[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_293 (BatchN (None, 20, 15, 64)   256         conv2d_325[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_293 (Activation)     (None, 20, 15, 64)   0           batch_normalization_293[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_326 (Conv2D)             (None, 20, 15, 64)   36928       activation_293[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_294 (BatchN (None, 20, 15, 64)   256         conv2d_326[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_294 (Activation)     (None, 20, 15, 64)   0           batch_normalization_294[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_327 (Conv2D)             (None, 20, 15, 128)  8320        activation_294[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_94 (Add)                    (None, 20, 15, 128)  0           add_93[0][0]                     \n",
      "                                                                 conv2d_327[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_295 (BatchN (None, 20, 15, 128)  512         add_94[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_295 (Activation)     (None, 20, 15, 128)  0           batch_normalization_295[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_328 (Conv2D)             (None, 20, 15, 64)   8256        activation_295[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_296 (BatchN (None, 20, 15, 64)   256         conv2d_328[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_296 (Activation)     (None, 20, 15, 64)   0           batch_normalization_296[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_329 (Conv2D)             (None, 20, 15, 64)   36928       activation_296[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_297 (BatchN (None, 20, 15, 64)   256         conv2d_329[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_297 (Activation)     (None, 20, 15, 64)   0           batch_normalization_297[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_330 (Conv2D)             (None, 20, 15, 128)  8320        activation_297[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_95 (Add)                    (None, 20, 15, 128)  0           add_94[0][0]                     \n",
      "                                                                 conv2d_330[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_298 (BatchN (None, 20, 15, 128)  512         add_95[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_298 (Activation)     (None, 20, 15, 128)  0           batch_normalization_298[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_331 (Conv2D)             (None, 10, 8, 128)   16512       activation_298[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_299 (BatchN (None, 10, 8, 128)   512         conv2d_331[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_299 (Activation)     (None, 10, 8, 128)   0           batch_normalization_299[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_332 (Conv2D)             (None, 10, 8, 128)   147584      activation_299[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_300 (BatchN (None, 10, 8, 128)   512         conv2d_332[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_300 (Activation)     (None, 10, 8, 128)   0           batch_normalization_300[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_334 (Conv2D)             (None, 10, 8, 256)   33024       add_95[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_333 (Conv2D)             (None, 10, 8, 256)   33024       activation_300[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_96 (Add)                    (None, 10, 8, 256)   0           conv2d_334[0][0]                 \n",
      "                                                                 conv2d_333[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_301 (BatchN (None, 10, 8, 256)   1024        add_96[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_301 (Activation)     (None, 10, 8, 256)   0           batch_normalization_301[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_335 (Conv2D)             (None, 10, 8, 128)   32896       activation_301[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_302 (BatchN (None, 10, 8, 128)   512         conv2d_335[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_302 (Activation)     (None, 10, 8, 128)   0           batch_normalization_302[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_336 (Conv2D)             (None, 10, 8, 128)   147584      activation_302[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_303 (BatchN (None, 10, 8, 128)   512         conv2d_336[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_303 (Activation)     (None, 10, 8, 128)   0           batch_normalization_303[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_337 (Conv2D)             (None, 10, 8, 256)   33024       activation_303[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_97 (Add)                    (None, 10, 8, 256)   0           add_96[0][0]                     \n",
      "                                                                 conv2d_337[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_304 (BatchN (None, 10, 8, 256)   1024        add_97[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_304 (Activation)     (None, 10, 8, 256)   0           batch_normalization_304[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_338 (Conv2D)             (None, 10, 8, 128)   32896       activation_304[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_305 (BatchN (None, 10, 8, 128)   512         conv2d_338[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_305 (Activation)     (None, 10, 8, 128)   0           batch_normalization_305[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_339 (Conv2D)             (None, 10, 8, 128)   147584      activation_305[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_306 (BatchN (None, 10, 8, 128)   512         conv2d_339[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_306 (Activation)     (None, 10, 8, 128)   0           batch_normalization_306[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_340 (Conv2D)             (None, 10, 8, 256)   33024       activation_306[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_98 (Add)                    (None, 10, 8, 256)   0           add_97[0][0]                     \n",
      "                                                                 conv2d_340[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_307 (BatchN (None, 10, 8, 256)   1024        add_98[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_307 (Activation)     (None, 10, 8, 256)   0           batch_normalization_307[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, 1, 1, 256)    0           activation_307[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 256)          0           average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 10)           2570        flatten_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[ 1.65518343e-01  1.35210335e-01  1.27086967e-01 ...  1.65518343e-01\n",
      "     1.35210335e-01  1.27086967e-01]\n",
      "   [ 1.58543527e-01  9.79831815e-02  7.96919167e-02 ...  1.58543527e-01\n",
      "     9.79831815e-02  7.96919167e-02]\n",
      "   [ 8.76751542e-02  6.17926121e-02  5.45938611e-02 ...  8.76751542e-02\n",
      "     6.17926121e-02  5.45938611e-02]\n",
      "   ...\n",
      "   [ 1.30364299e-01  6.82351589e-02  4.84873652e-02 ...  1.30364299e-01\n",
      "     6.82351589e-02  4.84873652e-02]\n",
      "   [ 1.61344826e-01  8.87674093e-02  5.63585460e-02 ...  1.61344826e-01\n",
      "     8.87674093e-02  5.63585460e-02]\n",
      "   [-2.47618854e-02 -8.17367136e-02 -9.23808515e-02 ... -2.47618854e-02\n",
      "    -8.17367136e-02 -9.23808515e-02]]\n",
      "\n",
      "  [[ 1.97787076e-01  1.44958138e-01  1.22885317e-01 ...  1.97787076e-01\n",
      "     1.44958138e-01  1.22885317e-01]\n",
      "   [ 1.61008477e-01  1.08347386e-01  9.34732854e-02 ...  1.61008477e-01\n",
      "     1.08347386e-01  9.34732854e-02]\n",
      "   [ 8.61626863e-02  3.91875505e-02  4.54900265e-02 ...  8.61626863e-02\n",
      "     3.91875505e-02  4.54900265e-02]\n",
      "   ...\n",
      "   [ 1.23669565e-01  6.20727539e-02  4.61343527e-02 ...  1.23669565e-01\n",
      "     6.20727539e-02  4.61343527e-02]\n",
      "   [ 1.37983322e-01  7.48177767e-02  5.87393939e-02 ...  1.37983322e-01\n",
      "     7.48177767e-02  5.87393939e-02]\n",
      "   [ 1.05434120e-01  4.59103882e-02  3.50419879e-02 ...  1.05434120e-01\n",
      "     4.59103882e-02  3.50419879e-02]]\n",
      "\n",
      "  [[ 1.76890790e-01  1.42072767e-01  1.22997314e-01 ...  1.76890790e-01\n",
      "     1.42072767e-01  1.22997314e-01]\n",
      "   [ 1.23473525e-01  9.88515615e-02  9.36416388e-02 ...  1.23473525e-01\n",
      "     9.88515615e-02  9.36416388e-02]\n",
      "   [ 8.58265758e-02  4.13165092e-02  4.07841802e-02 ...  8.58265758e-02\n",
      "     4.13165092e-02  4.07841802e-02]\n",
      "   ...\n",
      "   [ 1.25182092e-01  5.37814498e-02  3.95517349e-02 ...  1.25182092e-01\n",
      "     5.37814498e-02  3.95517349e-02]\n",
      "   [ 1.20476365e-01  4.99438047e-02  2.58823633e-02 ...  1.20476365e-01\n",
      "     4.99438047e-02  2.58823633e-02]\n",
      "   [ 1.43333435e-01  7.95237720e-02  6.00841045e-02 ...  1.43333435e-01\n",
      "     7.95237720e-02  6.00841045e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.16610646e-01  7.31930733e-02  6.31932616e-02 ...  1.16610646e-01\n",
      "     7.31930733e-02  6.31932616e-02]\n",
      "   [ 7.89916515e-02  3.35013270e-02 -1.60224438e-02 ...  7.89916515e-02\n",
      "     3.35013270e-02 -1.60224438e-02]\n",
      "   [ 1.07030809e-01  6.09244704e-02  4.72549200e-02 ...  1.07030809e-01\n",
      "     6.09244704e-02  4.72549200e-02]\n",
      "   ...\n",
      "   [ 1.37422979e-01  3.03081870e-02 -2.69468129e-02 ...  1.37422979e-01\n",
      "     3.03081870e-02 -2.69468129e-02]\n",
      "   [ 1.35266304e-01  5.41177392e-02  6.52664900e-03 ...  1.35266304e-01\n",
      "     5.41177392e-02  6.52664900e-03]\n",
      "   [ 1.39635921e-01  8.32494795e-02  4.34734523e-02 ...  1.39635921e-01\n",
      "     8.32494795e-02  4.34734523e-02]]\n",
      "\n",
      "  [[ 1.21232629e-01  7.11764693e-02  5.41457534e-02 ...  1.21232629e-01\n",
      "     7.11764693e-02  5.41457534e-02]\n",
      "   [ 8.92157555e-02  5.61063290e-02  3.12604904e-02 ...  8.92157555e-02\n",
      "     5.61063290e-02  3.12604904e-02]\n",
      "   [ 1.11372769e-01  6.31933212e-02  4.04201150e-02 ...  1.11372769e-01\n",
      "     6.31933212e-02  4.04201150e-02]\n",
      "   ...\n",
      "   [ 1.43501461e-01  3.36135626e-02 -1.32493675e-02 ...  1.43501461e-01\n",
      "     3.36135626e-02 -1.32493675e-02]\n",
      "   [ 1.41316712e-01  8.42016339e-02  3.17927599e-02 ...  1.41316712e-01\n",
      "     8.42016339e-02  3.17927599e-02]\n",
      "   [ 1.27479196e-01  5.20167649e-02  2.84593701e-02 ...  1.27479196e-01\n",
      "     5.20167649e-02  2.84593701e-02]]\n",
      "\n",
      "  [[ 1.30084097e-01  6.98880553e-02  5.73951602e-02 ...  1.30084097e-01\n",
      "     6.98880553e-02  5.73951602e-02]\n",
      "   [ 9.87393856e-02  5.41174412e-02  2.47899294e-02 ...  9.87393856e-02\n",
      "     5.41174412e-02  2.47899294e-02]\n",
      "   [ 1.07170939e-01  4.72547412e-02  6.27440214e-03 ...  1.07170939e-01\n",
      "     4.72547412e-02  6.27440214e-03]\n",
      "   ...\n",
      "   [ 1.49551928e-01  6.43978119e-02  3.64145339e-02 ...  1.49551928e-01\n",
      "     6.43978119e-02  3.64145339e-02]\n",
      "   [ 1.53221428e-01  1.09635770e-01  7.71148205e-02 ...  1.53221428e-01\n",
      "     1.09635770e-01  7.71148205e-02]\n",
      "   [ 1.35742247e-01  5.34733832e-02  1.89355016e-02 ...  1.35742247e-01\n",
      "     5.34733832e-02  1.89355016e-02]]]\n",
      "\n",
      "\n",
      " [[[-5.80110848e-02 -3.34171355e-02 -1.40895247e-02 ... -5.80110848e-02\n",
      "    -3.34171355e-02 -1.40895247e-02]\n",
      "   [ 1.66386664e-01  1.60728276e-01  1.50280148e-01 ...  1.66386664e-01\n",
      "     1.60728276e-01  1.50280148e-01]\n",
      "   [ 1.70028090e-01  1.63753390e-01  1.40868366e-01 ...  1.70028090e-01\n",
      "     1.63753390e-01  1.40868366e-01]\n",
      "   ...\n",
      "   [-2.18655318e-01 -1.86666816e-01 -1.63277358e-01 ... -2.18655318e-01\n",
      "    -1.86666816e-01 -1.63277358e-01]\n",
      "   [-3.17086577e-01 -2.64173806e-01 -2.45602265e-01 ... -3.17086577e-01\n",
      "    -2.64173806e-01 -2.45602265e-01]\n",
      "   [-1.81624621e-01 -1.60168082e-01 -1.43361241e-01 ... -1.81624621e-01\n",
      "    -1.60168082e-01 -1.43361241e-01]]\n",
      "\n",
      "  [[ 5.66106141e-02  7.82914758e-02  8.36696327e-02 ...  5.66106141e-02\n",
      "     7.82914758e-02  8.36696327e-02]\n",
      "   [ 2.11988866e-01  1.94621891e-01  1.79747790e-01 ...  2.11988866e-01\n",
      "     1.94621891e-01  1.79747790e-01]\n",
      "   [ 1.72437191e-01  1.45069897e-01  1.27842963e-01 ...  1.72437191e-01\n",
      "     1.45069897e-01  1.27842963e-01]\n",
      "   ...\n",
      "   [-1.82062387e-03 -8.51547718e-03 -4.84606624e-03 ... -1.82062387e-03\n",
      "    -8.51547718e-03 -4.84606624e-03]\n",
      "   [-3.36526513e-01 -2.70280302e-01 -2.35378280e-01 ... -3.36526513e-01\n",
      "    -2.70280302e-01 -2.35378280e-01]\n",
      "   [-1.84761971e-01 -1.38403356e-01 -1.13977611e-01 ... -1.84761971e-01\n",
      "    -1.38403356e-01 -1.13977611e-01]]\n",
      "\n",
      "  [[ 1.45518243e-01  1.49915904e-01  1.38683587e-01 ...  1.45518243e-01\n",
      "     1.49915904e-01  1.38683587e-01]\n",
      "   [ 1.86218619e-01  1.69439793e-01  1.56386733e-01 ...  1.86218619e-01\n",
      "     1.69439793e-01  1.56386733e-01]\n",
      "   [ 1.68179512e-01  1.35434151e-01  1.23137116e-01 ...  1.68179512e-01\n",
      "     1.35434151e-01  1.23137116e-01]\n",
      "   ...\n",
      "   [ 1.36946797e-01  9.69187021e-02  8.66105556e-02 ...  1.36946797e-01\n",
      "     9.69187021e-02  8.66105556e-02]\n",
      "   [-2.99131483e-01 -2.52016991e-01 -2.17254907e-01 ... -2.99131483e-01\n",
      "    -2.52016991e-01 -2.17254907e-01]\n",
      "   [-2.17450887e-01 -1.71456635e-01 -1.55602187e-01 ... -2.17450887e-01\n",
      "    -1.71456635e-01 -1.55602187e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.98963583e-01  1.75153852e-01  1.53389335e-01 ...  1.98963583e-01\n",
      "     1.75153852e-01  1.53389335e-01]\n",
      "   [ 1.69187725e-01  1.55069947e-01  1.05546176e-01 ...  1.69187725e-01\n",
      "     1.55069947e-01  1.05546176e-01]\n",
      "   [ 1.30560219e-01  9.62185860e-02  6.29411936e-02 ...  1.30560219e-01\n",
      "     9.62185860e-02  6.29411936e-02]\n",
      "   ...\n",
      "   [ 1.21736705e-01  5.77591658e-02  3.97198796e-02 ...  1.21736705e-01\n",
      "     5.77591658e-02  3.97198796e-02]\n",
      "   [ 1.66638851e-01  1.48235381e-01  1.35938436e-01 ...  1.66638851e-01\n",
      "     1.48235381e-01  1.35938436e-01]\n",
      "   [ 4.15967107e-02  4.79553640e-02  3.56303155e-02 ...  4.15967107e-02\n",
      "     4.79553640e-02  3.56303155e-02]]\n",
      "\n",
      "  [[ 1.99663997e-01  1.73137248e-01  1.32577121e-01 ...  1.99663997e-01\n",
      "     1.73137248e-01  1.32577121e-01]\n",
      "   [ 1.51960850e-01  1.30616128e-01  9.00840163e-02 ...  1.51960850e-01\n",
      "     1.30616128e-01  9.00840163e-02]\n",
      "   [ 1.46666884e-01  1.14173710e-01  7.96357989e-02 ...  1.46666884e-01\n",
      "     1.14173710e-01  7.96357989e-02]\n",
      "   ...\n",
      "   [ 1.39579892e-01  8.06723833e-02  6.51820302e-02 ...  1.39579892e-01\n",
      "     8.06723833e-02  6.51820302e-02]\n",
      "   [ 1.68767691e-01  1.50868297e-01  1.21988833e-01 ...  1.68767691e-01\n",
      "     1.50868297e-01  1.21988833e-01]\n",
      "   [-5.29129803e-02 -3.03362012e-02 -2.25210190e-02 ... -5.29129803e-02\n",
      "    -3.03362012e-02 -2.25210190e-02]]\n",
      "\n",
      "  [[ 2.28123307e-01  1.87535107e-01  1.55434370e-01 ...  2.28123307e-01\n",
      "     1.87535107e-01  1.55434370e-01]\n",
      "   [ 1.65406048e-01  1.36470377e-01  9.53781605e-02 ...  1.65406048e-01\n",
      "     1.36470377e-01  9.53781605e-02]\n",
      "   [ 1.54229760e-01  1.21764541e-01  6.90194964e-02 ...  1.54229760e-01\n",
      "     1.21764541e-01  6.90194964e-02]\n",
      "   ...\n",
      "   [ 1.49551928e-01  9.96919274e-02  8.73949230e-02 ...  1.49551928e-01\n",
      "     9.96919274e-02  8.73949230e-02]\n",
      "   [ 1.45378292e-01  1.33165181e-01  1.12408936e-01 ...  1.45378292e-01\n",
      "     1.33165181e-01  1.12408936e-01]\n",
      "   [-1.27002865e-01 -9.55462158e-02 -8.69468451e-02 ... -1.27002865e-01\n",
      "    -9.55462158e-02 -8.69468451e-02]]]\n",
      "\n",
      "\n",
      " [[[-1.95265979e-01 -1.35377914e-01 -5.33052087e-02 ... -1.95265979e-01\n",
      "    -1.35377914e-01 -5.33052087e-02]\n",
      "   [-3.75348926e-02 -3.14286053e-02  3.26330662e-02 ... -3.75348926e-02\n",
      "    -3.14286053e-02  3.26330662e-02]\n",
      "   [-2.52091885e-03 -9.52512026e-04  5.45938611e-02 ... -2.52091885e-03\n",
      "    -9.52512026e-04  5.45938611e-02]\n",
      "   ...\n",
      "   [-5.00278473e-02 -5.33334911e-02 -1.42577589e-02 ... -5.00278473e-02\n",
      "    -5.33334911e-02 -1.42577589e-02]\n",
      "   [-3.08120251e-02 -7.59384930e-02 -6.12885356e-02 ... -3.08120251e-02\n",
      "    -7.59384930e-02 -6.12885356e-02]\n",
      "   [-1.42408937e-01 -1.48403376e-01 -1.04145557e-01 ... -1.42408937e-01\n",
      "    -1.48403376e-01 -1.04145557e-01]]\n",
      "\n",
      "  [[-1.62997246e-01 -1.06022269e-01 -3.39774489e-02 ... -1.62997246e-01\n",
      "    -1.06022269e-01 -3.39774489e-02]\n",
      "   [ 1.19888783e-02  6.38657808e-03  6.21007383e-02 ...  1.19888783e-02\n",
      "     6.38657808e-03  6.21007383e-02]\n",
      "   [-4.03338671e-03 -3.14007103e-02  1.80390477e-02 ... -4.03338671e-03\n",
      "    -3.14007103e-02  1.80390477e-02]\n",
      "   ...\n",
      "   [-3.31931710e-02 -8.51547718e-03  4.61343527e-02 ... -3.31931710e-02\n",
      "    -8.51547718e-03  4.61343527e-02]\n",
      "   [-2.28009820e-02 -3.49861681e-02  1.16805434e-02 ... -2.28009820e-02\n",
      "    -3.49861681e-02  1.16805434e-02]\n",
      "   [-7.88796246e-02 -9.52661037e-02 -5.90756536e-02 ... -7.88796246e-02\n",
      "    -9.52661037e-02 -5.90756536e-02]]\n",
      "\n",
      "  [[-1.13305300e-01 -7.36135244e-02 -2.49290466e-03 ... -1.13305300e-01\n",
      "    -7.36135244e-02 -2.49290466e-03]\n",
      "   [ 1.36696100e-02  2.04201937e-02  7.79553652e-02 ...  1.36696100e-02\n",
      "     2.04201937e-02  7.79553652e-02]\n",
      "   [-4.47928905e-04 -1.35854483e-02  2.90194750e-02 ... -4.47928905e-04\n",
      "    -1.35854483e-02  2.90194750e-02]\n",
      "   ...\n",
      "   [-1.59943700e-02  1.06441975e-02  7.87674189e-02 ... -1.59943700e-02\n",
      "     1.06441975e-02  7.87674189e-02]\n",
      "   [-1.67785287e-02 -4.95815277e-03  6.11764789e-02 ... -1.67785287e-02\n",
      "    -4.95815277e-03  6.11764789e-02]\n",
      "   [-4.09803092e-02 -9.30252671e-02 -8.50139558e-02 ... -4.09803092e-02\n",
      "    -9.30252671e-02 -8.50139558e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1.93193287e-01 -1.50336355e-01 -8.58263671e-02 ... -1.93193287e-01\n",
      "    -1.50336355e-01 -8.58263671e-02]\n",
      "   [-3.08122635e-02  6.05034828e-03  3.49579453e-02 ... -3.08122635e-02\n",
      "     6.05034828e-03  3.49579453e-02]\n",
      "   [-2.23809481e-02  1.77872181e-02  7.07843304e-02 ... -2.23809481e-02\n",
      "     1.77872181e-02  7.07843304e-02]\n",
      "   ...\n",
      "   [ 1.68085098e-04 -4.98592854e-03  2.40336061e-02 ...  1.68085098e-04\n",
      "    -4.98592854e-03  2.40336061e-02]\n",
      "   [ 1.93297863e-03  2.66667604e-02  6.14286363e-02 ...  1.93297863e-03\n",
      "     2.66667604e-02  6.14286363e-02]\n",
      "   [-1.33052468e-02  2.05043852e-02  5.13165891e-02 ... -1.33052468e-02\n",
      "     2.05043852e-02  5.13165891e-02]]\n",
      "\n",
      "  [[-3.06218356e-01 -2.46470600e-01 -1.92913085e-01 ... -3.06218356e-01\n",
      "    -2.46470600e-01 -1.92913085e-01]\n",
      "   [-5.58822751e-02 -2.62466073e-02  1.16526484e-02 ... -5.58822751e-02\n",
      "    -2.62466073e-02  1.16526484e-02]\n",
      "   [-2.58821249e-02  1.61345005e-02  6.78710938e-02 ... -2.58821249e-02\n",
      "     1.61345005e-02  6.78710938e-02]\n",
      "   ...\n",
      "   [ 6.24656677e-03 -5.60212135e-03  2.59663463e-02 ...  6.24656677e-03\n",
      "    -5.60212135e-03  2.59663463e-02]\n",
      "   [ 1.19049549e-02  4.49859500e-02  5.92437387e-02 ...  1.19049549e-02\n",
      "     4.49859500e-02  5.92437387e-02]\n",
      "   [-8.03639591e-02 -4.60224748e-02  4.92995977e-03 ... -8.03639591e-02\n",
      "    -4.60224748e-02  4.92995977e-03]]\n",
      "\n",
      "  [[-3.48347306e-01 -2.86974728e-01 -2.17114657e-01 ... -3.48347306e-01\n",
      "    -2.86974728e-01 -2.17114657e-01]\n",
      "   [-1.05182201e-01 -7.52943456e-02 -3.40336263e-02 ... -1.05182201e-01\n",
      "    -7.52943456e-02 -3.40336263e-02]\n",
      "   [-2.22408175e-02  1.98037624e-02  4.54900861e-02 ... -2.22408175e-02\n",
      "     1.98037624e-02  4.54900861e-02]\n",
      "   ...\n",
      "   [ 8.37546587e-03 -2.26885080e-03  3.64145339e-02 ...  8.37546587e-03\n",
      "    -2.26885080e-03  3.64145339e-02]\n",
      "   [ 4.20182943e-03  2.33612657e-02  3.78991365e-02 ...  4.20182943e-03\n",
      "     2.33612657e-02  3.78991365e-02]\n",
      "   [-1.70140117e-01 -1.26918763e-01 -7.91037083e-02 ... -1.70140117e-01\n",
      "    -1.26918763e-01 -7.91037083e-02]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-6.19326532e-02 -7.65543878e-02 -6.50699139e-02 ... -6.19326532e-02\n",
      "    -7.65543878e-02 -6.50699139e-02]\n",
      "   [ 3.30533385e-02  1.95518136e-02  3.65546346e-02 ...  3.30533385e-02\n",
      "     1.95518136e-02  3.65546346e-02]\n",
      "   [ 9.24378633e-03  2.25769281e-02  4.28291559e-02 ...  9.24378633e-03\n",
      "     2.25769281e-02  4.28291559e-02]\n",
      "   ...\n",
      "   [ 5.58544993e-02  3.23137105e-01  3.38683426e-01 ...  5.58544993e-02\n",
      "     3.23137105e-01  3.38683426e-01]\n",
      "   [ 6.72271848e-02  6.91595674e-02  7.20448196e-02 ...  6.72271848e-02\n",
      "     6.91595674e-02  7.20448196e-02]\n",
      "   [-1.11036390e-01 -9.74229872e-02 -6.10083044e-02 ... -1.11036390e-01\n",
      "    -9.74229872e-02 -6.10083044e-02]]\n",
      "\n",
      "  [[ 2.52380669e-02  7.70321488e-03  2.09245086e-02 ...  2.52380669e-02\n",
      "     7.70321488e-03  2.09245086e-02]\n",
      "   [ 4.33614254e-02  3.77591550e-02  5.42576015e-02 ...  4.33614254e-02\n",
      "     3.77591550e-02  5.42576015e-02]\n",
      "   [ 2.34175920e-02  2.35012770e-02  4.54900265e-02 ...  2.34175920e-02\n",
      "     2.35012770e-02  4.54900265e-02]\n",
      "   ...\n",
      "   [ 3.00140142e-01  3.16974699e-01  3.36330414e-01 ...  3.00140142e-01\n",
      "     3.16974699e-01  3.36330414e-01]\n",
      "   [ 3.14453900e-01  3.29719722e-01  9.79550779e-02 ...  3.14453900e-01\n",
      "     3.29719722e-01  9.79550779e-02]\n",
      "   [-2.39776671e-02 -8.99159908e-03  1.54341459e-02 ... -2.39776671e-02\n",
      "    -8.99159908e-03  1.54341459e-02]]\n",
      "\n",
      "  [[ 3.57143283e-02  2.44256854e-02  4.06443477e-02 ...  3.57143283e-02\n",
      "     2.44256854e-02  4.06443477e-02]\n",
      "   [ 2.93558836e-02  4.00280356e-02  5.44259548e-02 ...  2.93558836e-02\n",
      "     4.00280356e-02  5.44259548e-02]\n",
      "   [ 2.70030499e-02  3.73949409e-02  6.03920221e-02 ...  2.70030499e-02\n",
      "     3.73949409e-02  6.03920221e-02]\n",
      "   ...\n",
      "   [ 3.05574238e-01  3.12604964e-01  3.37590933e-01 ...  3.05574238e-01\n",
      "     3.12604964e-01  3.37590933e-01]\n",
      "   [ 5.38097024e-02  3.16610456e-01  8.47058892e-02 ...  5.38097024e-02\n",
      "     3.16610456e-01  8.47058892e-02]\n",
      "   [ 5.70589304e-02  5.20727932e-02  6.00841045e-02 ...  5.70589304e-02\n",
      "     5.20727932e-02  6.00841045e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 6.17086887e-02  6.14283681e-02  9.84873772e-02 ...  6.17086887e-02\n",
      "     6.14283681e-02  9.84873772e-02]\n",
      "   [ 2.98599482e-01  5.31091690e-02  3.05546165e-01 ...  2.98599482e-01\n",
      "     5.31091690e-02  3.05546165e-01]\n",
      "   [ 2.95266092e-01  4.13166285e-02  6.68627620e-02 ...  2.95266092e-01\n",
      "     4.13166285e-02  6.68627620e-02]\n",
      "   ...\n",
      "   [ 3.06050420e-01  2.77366996e-01  2.79551744e-02 ...  3.06050420e-01\n",
      "     2.77366996e-01  2.79551744e-02]\n",
      "   [ 3.07815313e-01  3.84314656e-02  4.96639311e-02 ...  3.07815313e-01\n",
      "     3.84314656e-02  4.96639311e-02]\n",
      "   [ 5.72829843e-02  2.83475220e-02  4.73950207e-02 ...  5.72829843e-02\n",
      "     2.83475220e-02  4.73950207e-02]]\n",
      "\n",
      "  [[ 7.02522397e-02  7.50980377e-02  9.72830057e-02 ...  7.02522397e-02\n",
      "     7.50980377e-02  9.72830057e-02]\n",
      "   [ 2.93137312e-01  5.21847606e-02  3.17534983e-01 ...  2.93137312e-01\n",
      "     5.21847606e-02  3.17534983e-01]\n",
      "   [ 3.03529620e-01  5.53501844e-02  7.57142305e-02 ...  3.03529620e-01\n",
      "     5.53501844e-02  7.57142305e-02]\n",
      "   ...\n",
      "   [ 3.16050470e-01  2.88515508e-01  4.16526198e-02 ...  3.16050470e-01\n",
      "     2.88515508e-01  4.16526198e-02]\n",
      "   [ 5.50422072e-02  2.14565396e-02  2.39496231e-02 ...  5.50422072e-02\n",
      "     2.14565396e-02  2.39496231e-02]\n",
      "   [ 6.86556697e-02  3.63304913e-02  6.76750839e-02 ...  6.86556697e-02\n",
      "     3.63304913e-02  6.76750839e-02]]\n",
      "\n",
      "  [[ 8.69468451e-02  7.77311921e-02  1.12297118e-01 ...  8.69468451e-02\n",
      "     7.77311921e-02  1.12297118e-01]\n",
      "   [ 2.94817805e-01  5.41174412e-02  6.40056133e-02 ...  2.94817805e-01\n",
      "     5.41174412e-02  6.40056133e-02]\n",
      "   [ 3.03249359e-01  5.90194464e-02  6.50979280e-02 ...  3.03249359e-01\n",
      "     5.90194464e-02  6.50979280e-02]\n",
      "   ...\n",
      "   [ 3.14257801e-01  2.80084074e-01  4.03361022e-02 ...  3.14257801e-01\n",
      "     2.80084074e-01  4.03361022e-02]\n",
      "   [ 5.91037869e-02  1.94396973e-02  3.39775681e-02 ...  5.91037869e-02\n",
      "     1.94396973e-02  3.39775681e-02]\n",
      "   [ 8.08402896e-02  5.73949516e-02  8.16806257e-02 ...  8.08402896e-02\n",
      "     5.73949516e-02  8.16806257e-02]]]\n",
      "\n",
      "\n",
      " [[[ 3.81204605e-01  3.15602481e-01  3.34930092e-01 ...  3.81204605e-01\n",
      "     3.15602481e-01  3.34930092e-01]\n",
      "   [ 3.54621947e-01  3.21512580e-01  3.62044841e-01 ...  3.54621947e-01\n",
      "     3.21512580e-01  3.62044841e-01]\n",
      "   [ 3.15126121e-01  3.20616126e-01  3.60476196e-01 ...  3.15126121e-01\n",
      "     3.20616126e-01  3.60476196e-01]\n",
      "   ...\n",
      "   [ 3.10756445e-01  2.99607694e-01  3.26918721e-01 ...  3.10756445e-01\n",
      "     2.99607694e-01  3.26918721e-01]\n",
      "   [ 3.41736972e-01  2.96610534e-01  3.03417355e-01 ...  3.41736972e-01\n",
      "     2.96610534e-01  3.03417355e-01]\n",
      "   [ 3.98767531e-01  3.10420156e-01  1.54677987e-01 ...  3.98767531e-01\n",
      "     3.10420156e-01  1.54677987e-01]]\n",
      "\n",
      "  [[ 3.70336086e-01  3.13585579e-01  3.42493147e-01 ...  3.70336086e-01\n",
      "     3.13585579e-01  3.42493147e-01]\n",
      "   [ 3.64930034e-01  3.51484627e-01  3.91512483e-01 ...  3.64930034e-01\n",
      "     3.51484627e-01  3.91512483e-01]\n",
      "   [ 3.09692085e-01  2.86246359e-01  3.23921382e-01 ...  3.09692085e-01\n",
      "     2.86246359e-01  3.23921382e-01]\n",
      "   ...\n",
      "   [ 3.07983279e-01  2.85602152e-01  3.12801003e-01 ...  3.07983279e-01\n",
      "     2.85602152e-01  3.12801003e-01]\n",
      "   [ 3.34061742e-01  3.06190312e-01  3.33249182e-01 ...  3.34061742e-01\n",
      "     3.06190312e-01  3.33249182e-01]\n",
      "   [ 3.60336065e-01  3.00812334e-01  3.17394942e-01 ...  3.60336065e-01\n",
      "     3.00812334e-01  3.17394942e-01]]\n",
      "\n",
      "  [[ 3.65126073e-01  3.38151187e-01  3.66134554e-01 ...  3.65126073e-01\n",
      "     3.38151187e-01  3.66134554e-01]\n",
      "   [ 3.35238218e-01  3.30224097e-01  3.75994563e-01 ...  3.35238218e-01\n",
      "     3.30224097e-01  3.75994563e-01]\n",
      "   [ 2.97591269e-01  2.64845908e-01  2.95686126e-01 ...  2.97591269e-01\n",
      "     2.64845908e-01  2.95686126e-01]\n",
      "   ...\n",
      "   [ 3.01652670e-01  2.57703006e-01  1.21904671e-01 ...  3.01652670e-01\n",
      "     2.57703006e-01  1.21904671e-01]\n",
      "   [ 3.20476353e-01  2.89159477e-01  3.19999993e-01 ...  3.20476353e-01\n",
      "     2.89159477e-01  3.19999993e-01]\n",
      "   [ 3.51176560e-01  3.06974739e-01  3.22829187e-01 ...  3.51176560e-01\n",
      "     3.06974739e-01  3.22829187e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 3.47983181e-01  3.35938156e-01  3.80840302e-01 ...  3.47983181e-01\n",
      "     3.35938156e-01  3.80840302e-01]\n",
      "   [ 3.14285755e-01  3.08011115e-01  3.21232438e-01 ...  3.14285755e-01\n",
      "     3.08011115e-01  3.21232438e-01]\n",
      "   [ 2.99187660e-01  1.04061723e-01  2.86470592e-01 ...  2.99187660e-01\n",
      "     1.04061723e-01  2.86470592e-01]\n",
      "   ...\n",
      "   [ 3.02128851e-01  7.34454393e-02  2.55406141e-01 ...  3.02128851e-01\n",
      "     7.34454393e-02  2.55406141e-01]\n",
      "   [ 2.96050608e-01  6.98040128e-02  8.88796151e-02 ...  2.96050608e-01\n",
      "     6.98040128e-02  8.88796151e-02]\n",
      "   [ 2.92577088e-01  2.36190647e-01  2.35630304e-01 ...  2.92577088e-01\n",
      "     2.36190647e-01  2.35630304e-01]]\n",
      "\n",
      "  [[ 3.80056143e-01  3.73137236e-01  4.03165340e-01 ...  3.80056143e-01\n",
      "     3.73137236e-01  4.03165340e-01]\n",
      "   [ 3.08823586e-01  2.99243569e-01  3.13613415e-01 ...  3.08823586e-01\n",
      "     2.99243569e-01  3.13613415e-01]\n",
      "   [ 3.07451189e-01  1.14173710e-01  1.34537756e-01 ...  3.07451189e-01\n",
      "     1.14173710e-01  1.34537756e-01]\n",
      "   ...\n",
      "   [ 3.16050470e-01  9.24370885e-02  2.69103587e-01 ...  3.16050470e-01\n",
      "     9.24370885e-02  2.69103587e-01]\n",
      "   [ 3.17787290e-01  1.07731044e-01  2.63165295e-01 ...  3.17787290e-01\n",
      "     1.07731044e-01  2.63165295e-01]\n",
      "   [ 3.03949773e-01  2.52016753e-01  2.75518209e-01 ...  3.03949773e-01\n",
      "     2.52016753e-01  2.75518209e-01]]\n",
      "\n",
      "  [[ 4.12437022e-01  4.03221369e-01  4.49552000e-01 ...  4.12437022e-01\n",
      "     4.03221369e-01  4.49552000e-01]\n",
      "   [ 3.14425647e-01  1.24705672e-01  1.30672276e-01 ...  3.14425647e-01\n",
      "     1.24705672e-01  1.30672276e-01]\n",
      "   [ 3.15014064e-01  1.25686109e-01  1.31764591e-01 ...  3.15014064e-01\n",
      "     1.25686109e-01  1.31764591e-01]\n",
      "   ...\n",
      "   [ 3.22100937e-01  9.18487906e-02  2.67787069e-01 ...  3.22100937e-01\n",
      "     9.18487906e-02  2.67787069e-01]\n",
      "   [ 3.21848869e-01  2.78263211e-01  2.88879514e-01 ...  3.21848869e-01\n",
      "     2.78263211e-01  2.88879514e-01]\n",
      "   [ 3.35742235e-01  2.96610653e-01  3.20896298e-01 ...  3.35742235e-01\n",
      "     2.96610653e-01  3.20896298e-01]]]\n",
      "\n",
      "\n",
      " [[[-9.72267687e-02 -3.31456363e-01 -3.23893458e-01 ... -9.72267687e-02\n",
      "    -3.31456363e-01 -3.23893458e-01]\n",
      "   [-4.02240813e-01 -3.56918812e-01 -3.39915961e-01 ... -4.02240813e-01\n",
      "    -3.56918812e-01 -3.39915961e-01]\n",
      "   [-2.41736621e-01 -2.20560342e-01 -2.04229683e-01 ... -2.41736621e-01\n",
      "    -2.20560342e-01 -2.04229683e-01]\n",
      "   ...\n",
      "   [-1.51988655e-01 -1.70980543e-01 -1.35826379e-01 ... -1.51988655e-01\n",
      "    -1.70980543e-01 -1.35826379e-01]\n",
      "   [-2.03361064e-01 -2.24958092e-01 -2.25994408e-01 ... -2.03361064e-01\n",
      "    -2.24958092e-01 -2.25994408e-01]\n",
      "   [-2.67899156e-01 -2.93501437e-01 -2.96302438e-01 ... -2.67899156e-01\n",
      "    -2.93501437e-01 -2.96302438e-01]]\n",
      "\n",
      "  [[-4.02212948e-01 -3.45237970e-01 -3.35938245e-01 ... -4.02212948e-01\n",
      "    -3.45237970e-01 -3.35938245e-01]\n",
      "   [-3.17422926e-01 -2.99495757e-01 -2.90840447e-01 ... -3.17422926e-01\n",
      "    -2.99495757e-01 -2.90840447e-01]\n",
      "   [-1.84425563e-01 -1.80420309e-01 -1.46666855e-01 ... -1.84425563e-01\n",
      "    -1.80420309e-01 -1.46666855e-01]\n",
      "   ...\n",
      "   [-1.31232411e-01 -1.57535106e-01 -1.14649981e-01 ... -1.31232411e-01\n",
      "    -1.57535106e-01 -1.14649981e-01]\n",
      "   [-1.87506884e-01 -2.03613609e-01 -1.84397876e-01 ... -1.87506884e-01\n",
      "    -2.03613609e-01 -1.84397876e-01]\n",
      "   [-2.27899224e-01 -2.40364149e-01 -2.31624678e-01 ... -2.27899224e-01\n",
      "    -2.40364149e-01 -2.31624678e-01]]\n",
      "\n",
      "  [[-4.19187665e-01 -3.59888047e-01 -3.51512522e-01 ... -4.19187665e-01\n",
      "    -3.59888047e-01 -3.51512522e-01]\n",
      "   [-2.49075502e-01 -2.38403350e-01 -2.27927014e-01 ... -2.49075502e-01\n",
      "    -2.38403350e-01 -2.27927014e-01]\n",
      "   [-1.41624421e-01 -1.31232530e-01 -8.47060382e-02 ... -1.41624421e-01\n",
      "    -1.31232530e-01 -8.47060382e-02]\n",
      "   ...\n",
      "   [-1.17955148e-01 -1.38375431e-01 -8.59384835e-02 ... -1.17955148e-01\n",
      "    -1.38375431e-01 -8.59384835e-02]\n",
      "   [-1.69719726e-01 -1.93193465e-01 -1.62352949e-01 ... -1.69719726e-01\n",
      "    -1.93193465e-01 -1.62352949e-01]\n",
      "   [-2.05686182e-01 -2.30280161e-01 -2.34033570e-01 ... -2.05686182e-01\n",
      "    -2.30280161e-01 -2.34033570e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-9.90756452e-02 -7.58265555e-02 -8.58263671e-02 ... -9.90756452e-02\n",
      "    -7.58265555e-02 -8.58263671e-02]\n",
      "   [-1.95518166e-01  8.44817162e-02  5.45657873e-02 ... -1.95518166e-01\n",
      "     8.44817162e-02  5.45657873e-02]\n",
      "   [-1.12577021e-01  1.86414659e-01  2.00196087e-01 ... -1.12577021e-01\n",
      "     1.86414659e-01  2.00196087e-01]\n",
      "   ...\n",
      "   [-7.04201460e-02  1.98935628e-01  2.08347321e-01 ... -7.04201460e-02\n",
      "     1.98935628e-01  2.08347321e-01]\n",
      "   [-1.07870966e-01  1.71764791e-01  1.71232551e-01 ... -1.07870966e-01\n",
      "     1.71764791e-01  1.71232551e-01]\n",
      "   [-1.46638602e-01  1.14622027e-01  7.48459995e-02 ... -1.46638602e-01\n",
      "     1.14622027e-01  7.48459995e-02]]\n",
      "\n",
      "  [[-4.27787006e-01 -1.17058843e-01 -1.22324854e-01 ... -4.27787006e-01\n",
      "    -1.17058843e-01 -1.22324854e-01]\n",
      "   [ 1.47059560e-02  3.25769186e-02  1.16526484e-02 ...  1.47059560e-02\n",
      "     3.25769186e-02  1.16526484e-02]\n",
      "   [-1.35686070e-01  1.57310963e-01  1.61988735e-01 ... -1.35686070e-01\n",
      "     1.57310963e-01  1.61988735e-01]\n",
      "   ...\n",
      "   [-8.78710747e-02  1.74790025e-01  1.82829082e-01 ... -8.78710747e-02\n",
      "     1.74790025e-01  1.82829082e-01]\n",
      "   [-1.29271537e-01 -1.27563089e-01 -1.60364121e-01 ... -1.29271537e-01\n",
      "    -1.27563089e-01 -1.60364121e-01]\n",
      "   [-1.66638464e-01 -1.83277369e-01 -2.10756317e-01 ... -1.66638464e-01\n",
      "    -1.83277369e-01 -2.10756317e-01]]\n",
      "\n",
      "  [[-4.50308084e-01 -4.00700212e-01 -3.97506833e-01 ... -4.50308084e-01\n",
      "    -4.00700212e-01 -3.97506833e-01]\n",
      "   [-5.02802134e-02 -2.82355249e-02 -5.36414683e-02 ... -5.02802134e-02\n",
      "    -2.82355249e-02 -5.36414683e-02]\n",
      "   [-1.63417310e-01  1.21764541e-01  1.00392044e-01 ... -1.63417310e-01\n",
      "     1.21764541e-01  1.00392044e-01]\n",
      "   ...\n",
      "   [-1.05350047e-01  1.62437022e-01  1.57983154e-01 ... -1.05350047e-01\n",
      "     1.62437022e-01  1.57983154e-01]\n",
      "   [-1.48739368e-01  1.13557339e-01  7.71148205e-02 ... -1.48739368e-01\n",
      "     1.13557339e-01  7.71148205e-02]\n",
      "   [-1.81904823e-01 -1.93585441e-01 -2.32044891e-01 ... -1.81904823e-01\n",
      "    -1.93585441e-01 -2.32044891e-01]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (140, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (140, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 12s 1s/step - loss: 2.9404 - accuracy: 0.2714 - val_loss: 2.8895 - val_accuracy: 0.4043\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/100\n",
      "Learning rate:  0.0033333333333333335\n",
      "5/5 [==============================] - 4s 735ms/step - loss: 2.9493 - accuracy: 0.2857 - val_loss: 2.8910 - val_accuracy: 0.4043\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/100\n",
      "Learning rate:  0.006666666666666667\n",
      "5/5 [==============================] - 4s 742ms/step - loss: 2.9012 - accuracy: 0.3071 - val_loss: 2.8962 - val_accuracy: 0.4043\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/100\n",
      "Learning rate:  0.01\n",
      "5/5 [==============================] - 4s 727ms/step - loss: 2.8092 - accuracy: 0.3143 - val_loss: 2.9034 - val_accuracy: 0.4043\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/100\n",
      "Learning rate:  0.013333333333333334\n",
      "5/5 [==============================] - 4s 730ms/step - loss: 2.7075 - accuracy: 0.3286 - val_loss: 2.9134 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/100\n",
      "Learning rate:  0.016666666666666666\n",
      "5/5 [==============================] - 4s 724ms/step - loss: 2.5993 - accuracy: 0.3714 - val_loss: 2.9138 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/100\n",
      "Learning rate:  0.02\n",
      "5/5 [==============================] - 4s 735ms/step - loss: 2.4729 - accuracy: 0.4357 - val_loss: 2.9155 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/100\n",
      "Learning rate:  0.023333333333333334\n",
      "5/5 [==============================] - 4s 726ms/step - loss: 2.4274 - accuracy: 0.4357 - val_loss: 2.9207 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/100\n",
      "Learning rate:  0.02666666666666667\n",
      "5/5 [==============================] - 4s 757ms/step - loss: 2.3218 - accuracy: 0.4929 - val_loss: 2.9165 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/100\n",
      "Learning rate:  0.03\n",
      "5/5 [==============================] - 4s 846ms/step - loss: 2.2315 - accuracy: 0.4857 - val_loss: 2.9042 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/100\n",
      "Learning rate:  0.03333333333333333\n",
      "5/5 [==============================] - 4s 743ms/step - loss: 2.1600 - accuracy: 0.5000 - val_loss: 2.8884 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/100\n",
      "Learning rate:  0.03666666666666667\n",
      "5/5 [==============================] - 4s 749ms/step - loss: 2.0831 - accuracy: 0.5357 - val_loss: 2.8684 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/100\n",
      "Learning rate:  0.04\n",
      "5/5 [==============================] - 4s 757ms/step - loss: 2.0262 - accuracy: 0.5357 - val_loss: 2.8452 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/100\n",
      "Learning rate:  0.043333333333333335\n",
      "5/5 [==============================] - 4s 845ms/step - loss: 1.9883 - accuracy: 0.5500 - val_loss: 2.7969 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/100\n",
      "Learning rate:  0.04666666666666667\n",
      "5/5 [==============================] - 4s 724ms/step - loss: 1.8990 - accuracy: 0.5571 - val_loss: 2.7287 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/100\n",
      "Learning rate:  0.05\n",
      "5/5 [==============================] - 4s 749ms/step - loss: 1.8360 - accuracy: 0.5786 - val_loss: 2.6542 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/100\n",
      "Learning rate:  0.05333333333333334\n",
      "5/5 [==============================] - 4s 736ms/step - loss: 1.8346 - accuracy: 0.5429 - val_loss: 2.5576 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/100\n",
      "Learning rate:  0.056666666666666664\n",
      "5/5 [==============================] - 4s 765ms/step - loss: 1.7506 - accuracy: 0.5929 - val_loss: 2.4755 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/100\n",
      "Learning rate:  0.06\n",
      "5/5 [==============================] - 4s 767ms/step - loss: 1.7049 - accuracy: 0.6214 - val_loss: 2.4347 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/100\n",
      "Learning rate:  0.06333333333333334\n",
      "5/5 [==============================] - 4s 713ms/step - loss: 1.6504 - accuracy: 0.6000 - val_loss: 2.4067 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 21/100\n",
      "Learning rate:  0.06666666666666667\n",
      "5/5 [==============================] - 4s 721ms/step - loss: 1.6337 - accuracy: 0.5929 - val_loss: 2.4291 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 22/100\n",
      "Learning rate:  0.07\n",
      "5/5 [==============================] - 4s 824ms/step - loss: 1.6079 - accuracy: 0.5714 - val_loss: 2.4968 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 23/100\n",
      "Learning rate:  0.07333333333333333\n",
      "5/5 [==============================] - 5s 1s/step - loss: 1.5104 - accuracy: 0.6786 - val_loss: 2.4361 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 24/100\n",
      "Learning rate:  0.07666666666666666\n",
      "5/5 [==============================] - 4s 754ms/step - loss: 1.4773 - accuracy: 0.7000 - val_loss: 2.4522 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 25/100\n",
      "Learning rate:  0.08\n",
      "5/5 [==============================] - 4s 747ms/step - loss: 1.4980 - accuracy: 0.6500 - val_loss: 2.2398 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 26/100\n",
      "Learning rate:  0.08333333333333333\n",
      "5/5 [==============================] - 4s 838ms/step - loss: 1.4253 - accuracy: 0.7143 - val_loss: 2.2432 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 27/100\n",
      "Learning rate:  0.08666666666666667\n",
      "5/5 [==============================] - 4s 715ms/step - loss: 1.4518 - accuracy: 0.7000 - val_loss: 2.5106 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 28/100\n",
      "Learning rate:  0.09\n",
      "5/5 [==============================] - 4s 727ms/step - loss: 1.4019 - accuracy: 0.6786 - val_loss: 2.4600 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 29/100\n",
      "Learning rate:  0.09333333333333334\n",
      "5/5 [==============================] - 4s 745ms/step - loss: 1.3697 - accuracy: 0.7143 - val_loss: 2.2921 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 30/100\n",
      "Learning rate:  0.09666666666666666\n",
      "5/5 [==============================] - 4s 720ms/step - loss: 1.3363 - accuracy: 0.7357 - val_loss: 2.5501 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 31/100\n",
      "Learning rate:  0.1\n",
      "5/5 [==============================] - 4s 721ms/step - loss: 1.3186 - accuracy: 0.7357 - val_loss: 2.8938 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 32/100\n",
      "Learning rate:  0.10333333333333333\n",
      "5/5 [==============================] - 4s 718ms/step - loss: 1.2690 - accuracy: 0.7500 - val_loss: 3.2295 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 33/100\n",
      "Learning rate:  0.10666666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 4s 717ms/step - loss: 1.2882 - accuracy: 0.7429 - val_loss: 3.4291 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 34/100\n",
      "Learning rate:  0.11\n",
      "5/5 [==============================] - 4s 720ms/step - loss: 1.3475 - accuracy: 0.7071 - val_loss: 2.5907 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 35/100\n",
      "Learning rate:  0.11333333333333333\n",
      "5/5 [==============================] - 4s 747ms/step - loss: 1.3448 - accuracy: 0.7143 - val_loss: 2.4641 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 36/100\n",
      "Learning rate:  0.11666666666666667\n",
      "5/5 [==============================] - 4s 730ms/step - loss: 1.2727 - accuracy: 0.7714 - val_loss: 3.1765 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 37/100\n",
      "Learning rate:  0.12\n",
      "5/5 [==============================] - 4s 735ms/step - loss: 1.2506 - accuracy: 0.7500 - val_loss: 3.4397 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 38/100\n",
      "Learning rate:  0.12333333333333334\n",
      "5/5 [==============================] - 4s 721ms/step - loss: 1.2044 - accuracy: 0.7714 - val_loss: 3.1535 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 39/100\n",
      "Learning rate:  0.12666666666666668\n",
      "5/5 [==============================] - 4s 753ms/step - loss: 1.2204 - accuracy: 0.7429 - val_loss: 2.4429 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 40/100\n",
      "Learning rate:  0.13\n",
      "5/5 [==============================] - 4s 730ms/step - loss: 1.2071 - accuracy: 0.7143 - val_loss: 3.0778 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 41/100\n",
      "Learning rate:  0.13333333333333333\n",
      "5/5 [==============================] - 5s 1s/step - loss: 1.2271 - accuracy: 0.7429 - val_loss: 3.0097 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 42/100\n",
      "Learning rate:  0.13666666666666666\n",
      "5/5 [==============================] - 4s 739ms/step - loss: 1.2035 - accuracy: 0.7786 - val_loss: 2.6704 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 43/100\n",
      "Learning rate:  0.14\n",
      "5/5 [==============================] - 4s 751ms/step - loss: 1.3120 - accuracy: 0.6929 - val_loss: 2.0045 - val_accuracy: 0.5957\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 44/100\n",
      "Learning rate:  0.14333333333333334\n",
      "5/5 [==============================] - 4s 718ms/step - loss: 1.2456 - accuracy: 0.7500 - val_loss: 2.1617 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 45/100\n",
      "Learning rate:  0.14666666666666667\n",
      "5/5 [==============================] - 4s 828ms/step - loss: 1.2110 - accuracy: 0.7500 - val_loss: 2.4641 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 46/100\n",
      "Learning rate:  0.15\n",
      "5/5 [==============================] - 4s 729ms/step - loss: 1.1407 - accuracy: 0.7929 - val_loss: 2.9671 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 47/100\n",
      "Learning rate:  0.15333333333333332\n",
      "5/5 [==============================] - 4s 738ms/step - loss: 1.1347 - accuracy: 0.7714 - val_loss: 3.2636 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 48/100\n",
      "Learning rate:  0.15666666666666668\n",
      "5/5 [==============================] - 4s 762ms/step - loss: 1.1280 - accuracy: 0.8000 - val_loss: 3.4284 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 49/100\n",
      "Learning rate:  0.16\n",
      "5/5 [==============================] - 4s 753ms/step - loss: 1.1361 - accuracy: 0.7714 - val_loss: 3.8522 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 50/100\n",
      "Learning rate:  0.16333333333333333\n",
      "5/5 [==============================] - 5s 997ms/step - loss: 1.1617 - accuracy: 0.7714 - val_loss: 3.2169 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 51/100\n",
      "Learning rate:  0.16666666666666666\n",
      "5/5 [==============================] - 4s 749ms/step - loss: 1.1013 - accuracy: 0.7571 - val_loss: 2.7096 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 52/100\n",
      "Learning rate:  0.17\n",
      "5/5 [==============================] - 4s 869ms/step - loss: 1.1227 - accuracy: 0.8571 - val_loss: 2.4338 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 53/100\n",
      "Learning rate:  0.17333333333333334\n",
      "5/5 [==============================] - 4s 991ms/step - loss: 1.1447 - accuracy: 0.7857 - val_loss: 2.0466 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 54/100\n",
      "Learning rate:  0.17666666666666667\n",
      "5/5 [==============================] - 4s 732ms/step - loss: 1.1637 - accuracy: 0.7786 - val_loss: 3.0205 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 55/100\n",
      "Learning rate:  0.18\n",
      "5/5 [==============================] - 4s 743ms/step - loss: 1.1394 - accuracy: 0.7643 - val_loss: 2.5087 - val_accuracy: 0.5532\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 56/100\n",
      "Learning rate:  0.18333333333333332\n",
      "5/5 [==============================] - 4s 715ms/step - loss: 1.0649 - accuracy: 0.8214 - val_loss: 2.1843 - val_accuracy: 0.5957\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 57/100\n",
      "Learning rate:  0.18666666666666668\n",
      "5/5 [==============================] - 4s 721ms/step - loss: 1.1155 - accuracy: 0.8000 - val_loss: 2.1617 - val_accuracy: 0.5957\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 58/100\n",
      "Learning rate:  0.19\n",
      "5/5 [==============================] - 4s 832ms/step - loss: 1.1745 - accuracy: 0.7857 - val_loss: 2.3884 - val_accuracy: 0.5957\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 59/100\n",
      "Learning rate:  0.19333333333333333\n",
      "5/5 [==============================] - 4s 728ms/step - loss: 1.0236 - accuracy: 0.8214 - val_loss: 3.2180 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 60/100\n",
      "Learning rate:  0.19666666666666666\n",
      "5/5 [==============================] - 4s 731ms/step - loss: 1.0099 - accuracy: 0.8786 - val_loss: 3.5827 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 61/100\n",
      "Learning rate:  0.2\n",
      "5/5 [==============================] - 5s 1s/step - loss: 1.0076 - accuracy: 0.8571 - val_loss: 3.3756 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 62/100\n",
      "Learning rate:  0.20333333333333334\n",
      "5/5 [==============================] - 4s 747ms/step - loss: 1.0606 - accuracy: 0.8000 - val_loss: 3.3783 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 63/100\n",
      "Learning rate:  0.20666666666666667\n",
      "5/5 [==============================] - 4s 727ms/step - loss: 1.0343 - accuracy: 0.8357 - val_loss: 5.3583 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 64/100\n",
      "Learning rate:  0.21\n",
      "5/5 [==============================] - 4s 862ms/step - loss: 1.2200 - accuracy: 0.7357 - val_loss: 5.5532 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 65/100\n",
      "Learning rate:  0.21333333333333335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 4s 836ms/step - loss: 1.0319 - accuracy: 0.8143 - val_loss: 4.3218 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 66/100\n",
      "Learning rate:  0.21666666666666667\n",
      "5/5 [==============================] - 4s 728ms/step - loss: 1.0574 - accuracy: 0.8214 - val_loss: 4.2883 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 67/100\n",
      "Learning rate:  0.22\n",
      "5/5 [==============================] - 4s 864ms/step - loss: 1.1000 - accuracy: 0.7571 - val_loss: 4.1950 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 68/100\n",
      "Learning rate:  0.22333333333333333\n",
      "5/5 [==============================] - 4s 735ms/step - loss: 1.0218 - accuracy: 0.7929 - val_loss: 3.2302 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 69/100\n",
      "Learning rate:  0.22666666666666666\n",
      "5/5 [==============================] - 5s 907ms/step - loss: 1.0704 - accuracy: 0.8000 - val_loss: 3.7055 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 70/100\n",
      "Learning rate:  0.23\n",
      "5/5 [==============================] - 5s 891ms/step - loss: 1.1143 - accuracy: 0.8071 - val_loss: 3.8710 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 71/100\n",
      "Learning rate:  0.23333333333333334\n",
      "5/5 [==============================] - 4s 821ms/step - loss: 1.0938 - accuracy: 0.7857 - val_loss: 2.7998 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 72/100\n",
      "Learning rate:  0.23666666666666666\n",
      "5/5 [==============================] - 4s 765ms/step - loss: 1.1451 - accuracy: 0.7500 - val_loss: 2.6655 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 73/100\n",
      "Learning rate:  0.24\n",
      "5/5 [==============================] - 4s 645ms/step - loss: 1.0413 - accuracy: 0.8571 - val_loss: 2.5765 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 74/100\n",
      "Learning rate:  0.24333333333333335\n",
      "5/5 [==============================] - 3s 634ms/step - loss: 0.9842 - accuracy: 0.8643 - val_loss: 3.4351 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 75/100\n",
      "Learning rate:  0.24666666666666667\n",
      "5/5 [==============================] - 3s 623ms/step - loss: 0.9347 - accuracy: 0.8714 - val_loss: 5.3811 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 76/100\n",
      "Learning rate:  0.25\n",
      "5/5 [==============================] - 3s 716ms/step - loss: 0.9795 - accuracy: 0.8571 - val_loss: 5.5158 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 77/100\n",
      "Learning rate:  0.25333333333333335\n",
      "5/5 [==============================] - 3s 613ms/step - loss: 0.9926 - accuracy: 0.8071 - val_loss: 3.9003 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 78/100\n",
      "Learning rate:  0.25666666666666665\n",
      "5/5 [==============================] - 3s 617ms/step - loss: 0.8825 - accuracy: 0.8929 - val_loss: 2.8063 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 79/100\n",
      "Learning rate:  0.26\n",
      "5/5 [==============================] - 3s 635ms/step - loss: 0.8544 - accuracy: 0.9214 - val_loss: 3.3059 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 80/100\n",
      "Learning rate:  0.2633333333333333\n",
      "5/5 [==============================] - 3s 632ms/step - loss: 0.9332 - accuracy: 0.8714 - val_loss: 3.6858 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 81/100\n",
      "Learning rate:  0.26666666666666666\n",
      "5/5 [==============================] - 4s 722ms/step - loss: 0.9641 - accuracy: 0.8643 - val_loss: 2.9721 - val_accuracy: 0.3404\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 82/100\n",
      "Learning rate:  0.27\n",
      "5/5 [==============================] - 4s 688ms/step - loss: 0.9155 - accuracy: 0.8643 - val_loss: 2.9708 - val_accuracy: 0.5957\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 83/100\n",
      "Learning rate:  0.2733333333333333\n",
      "5/5 [==============================] - 3s 713ms/step - loss: 0.9269 - accuracy: 0.8500 - val_loss: 2.9310 - val_accuracy: 0.5957\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 84/100\n",
      "Learning rate:  0.27666666666666667\n",
      "5/5 [==============================] - 3s 617ms/step - loss: 0.8748 - accuracy: 0.8929 - val_loss: 3.1256 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 85/100\n",
      "Learning rate:  0.28\n",
      "5/5 [==============================] - 3s 647ms/step - loss: 0.8585 - accuracy: 0.8929 - val_loss: 3.2720 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 86/100\n",
      "Learning rate:  0.2833333333333333\n",
      "5/5 [==============================] - 3s 624ms/step - loss: 0.9123 - accuracy: 0.8571 - val_loss: 3.3601 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 87/100\n",
      "Learning rate:  0.2866666666666667\n",
      "5/5 [==============================] - 3s 619ms/step - loss: 0.8185 - accuracy: 0.9357 - val_loss: 3.5705 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 88/100\n",
      "Learning rate:  0.29\n",
      "5/5 [==============================] - 3s 635ms/step - loss: 0.8660 - accuracy: 0.8714 - val_loss: 3.7744 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 89/100\n",
      "Learning rate:  0.29333333333333333\n",
      "5/5 [==============================] - 3s 644ms/step - loss: 0.8965 - accuracy: 0.8786 - val_loss: 3.9478 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 90/100\n",
      "Learning rate:  0.2966666666666667\n",
      "5/5 [==============================] - 3s 624ms/step - loss: 0.8166 - accuracy: 0.9286 - val_loss: 4.0146 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 91/100\n",
      "Learning rate:  0.3\n",
      "5/5 [==============================] - 3s 622ms/step - loss: 0.8452 - accuracy: 0.9071 - val_loss: 3.9722 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 92/100\n",
      "Learning rate:  0.30333333333333334\n",
      "5/5 [==============================] - 3s 616ms/step - loss: 0.8388 - accuracy: 0.9071 - val_loss: 3.9612 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 93/100\n",
      "Learning rate:  0.30666666666666664\n",
      "5/5 [==============================] - 3s 627ms/step - loss: 0.7886 - accuracy: 0.9357 - val_loss: 3.8665 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 94/100\n",
      "Learning rate:  0.31\n",
      "5/5 [==============================] - 3s 662ms/step - loss: 0.8062 - accuracy: 0.9357 - val_loss: 3.8479 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 95/100\n",
      "Learning rate:  0.31333333333333335\n",
      "5/5 [==============================] - 3s 662ms/step - loss: 0.9187 - accuracy: 0.8786 - val_loss: 3.8670 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 96/100\n",
      "Learning rate:  0.31666666666666665\n",
      "5/5 [==============================] - 4s 653ms/step - loss: 0.8169 - accuracy: 0.9286 - val_loss: 3.9771 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 97/100\n",
      "Learning rate:  0.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 4s 673ms/step - loss: 0.8326 - accuracy: 0.9357 - val_loss: 4.0948 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 98/100\n",
      "Learning rate:  0.3233333333333333\n",
      "5/5 [==============================] - 4s 689ms/step - loss: 0.7851 - accuracy: 0.9214 - val_loss: 4.0921 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 99/100\n",
      "Learning rate:  0.32666666666666666\n",
      "5/5 [==============================] - 4s 799ms/step - loss: 0.7759 - accuracy: 0.9429 - val_loss: 4.1580 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 100/100\n",
      "Learning rate:  0.33\n",
      "5/5 [==============================] - 4s 718ms/step - loss: 0.7788 - accuracy: 0.9500 - val_loss: 4.2847 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 1.6248 - accuracy: 0.5319\n",
      "Test loss: 1.624796748161316\n",
      "Test accuracy: 0.5319148898124695\n",
      "[0, 0, 1, 0, 0, 1, 0, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2]\n",
      "Precision:  0.577639116202946\n",
      "Recall:  0.5319148936170213\n",
      "F1 Score:  0.5501023284550334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[156 130 112 ... 156 130 112]\n",
      "   [155 132 114 ... 155 132 114]\n",
      "   [162 144 131 ... 162 144 131]\n",
      "   ...\n",
      "   [173 152 138 ... 173 152 138]\n",
      "   [177 161 153 ... 177 161 153]\n",
      "   [188 176 178 ... 188 176 178]]\n",
      "\n",
      "  [[154 127 107 ... 154 127 107]\n",
      "   [158 136 121 ... 158 136 121]\n",
      "   [162 142 126 ... 162 142 126]\n",
      "   ...\n",
      "   [174 153 142 ... 174 153 142]\n",
      "   [175 153 140 ... 175 153 140]\n",
      "   [189 177 179 ... 189 177 179]]\n",
      "\n",
      "  [[145 112  88 ... 145 112  88]\n",
      "   [161 142 129 ... 161 142 129]\n",
      "   [158 132 110 ... 158 132 110]\n",
      "   ...\n",
      "   [175 155 147 ... 175 155 147]\n",
      "   [168 139 119 ... 168 139 119]\n",
      "   [181 162 155 ... 181 162 155]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[199 191 190 ... 199 191 190]\n",
      "   [191 179 172 ... 191 179 172]\n",
      "   [173 149 129 ... 173 149 129]\n",
      "   ...\n",
      "   [181 160 145 ... 181 160 145]\n",
      "   [181 164 150 ... 181 164 150]\n",
      "   [177 158 146 ... 177 158 146]]\n",
      "\n",
      "  [[192 179 175 ... 192 179 175]\n",
      "   [197 189 186 ... 197 189 186]\n",
      "   [173 149 127 ... 173 149 127]\n",
      "   ...\n",
      "   [180 160 144 ... 180 160 144]\n",
      "   [179 160 144 ... 179 160 144]\n",
      "   [172 154 140 ... 172 154 140]]\n",
      "\n",
      "  [[186 168 164 ... 186 168 164]\n",
      "   [201 194 194 ... 201 194 194]\n",
      "   [180 161 143 ... 180 161 143]\n",
      "   ...\n",
      "   [178 155 137 ... 178 155 137]\n",
      "   [178 159 144 ... 178 159 144]\n",
      "   [168 149 136 ... 168 149 136]]]\n",
      "\n",
      "\n",
      " [[[165 132 108 ... 165 132 108]\n",
      "   [163 136 113 ... 163 136 113]\n",
      "   [173 142 110 ... 173 142 110]\n",
      "   ...\n",
      "   [177 147 120 ... 177 147 120]\n",
      "   [171 143 120 ... 171 143 120]\n",
      "   [170 151 141 ... 170 151 141]]\n",
      "\n",
      "  [[163 135 114 ... 163 135 114]\n",
      "   [166 136 109 ... 166 136 109]\n",
      "   [174 140 106 ... 174 140 106]\n",
      "   ...\n",
      "   [176 144 114 ... 176 144 114]\n",
      "   [177 150 124 ... 177 150 124]\n",
      "   [169 148 135 ... 169 148 135]]\n",
      "\n",
      "  [[163 136 116 ... 163 136 116]\n",
      "   [169 136 105 ... 169 136 105]\n",
      "   [174 141 106 ... 174 141 106]\n",
      "   ...\n",
      "   [176 145 114 ... 176 145 114]\n",
      "   [177 148 119 ... 177 148 119]\n",
      "   [172 150 135 ... 172 150 135]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[179 163 151 ... 179 163 151]\n",
      "   [177 152 127 ... 177 152 127]\n",
      "   [178 150 122 ... 178 150 122]\n",
      "   ...\n",
      "   [183 160 130 ... 183 160 130]\n",
      "   [179 152 118 ... 179 152 118]\n",
      "   [173 149 124 ... 173 149 124]]\n",
      "\n",
      "  [[180 163 151 ... 180 163 151]\n",
      "   [177 157 138 ... 177 157 138]\n",
      "   [178 152 125 ... 178 152 125]\n",
      "   ...\n",
      "   [182 158 126 ... 182 158 126]\n",
      "   [177 150 115 ... 177 150 115]\n",
      "   [171 148 126 ... 171 148 126]]\n",
      "\n",
      "  [[176 154 135 ... 176 154 135]\n",
      "   [175 157 142 ... 175 157 142]\n",
      "   [177 150 121 ... 177 150 121]\n",
      "   ...\n",
      "   [181 154 119 ... 181 154 119]\n",
      "   [175 148 114 ... 175 148 114]\n",
      "   [172 150 127 ... 172 150 127]]]\n",
      "\n",
      "\n",
      " [[[126 103  94 ... 126 103  94]\n",
      "   [164 144 129 ... 164 144 129]\n",
      "   [171 150 131 ... 171 150 131]\n",
      "   ...\n",
      "   [173 160 152 ... 173 160 152]\n",
      "   [110 119 135 ... 110 119 135]\n",
      "   [ 85 113 145 ...  85 113 145]]\n",
      "\n",
      "  [[152 127 113 ... 152 127 113]\n",
      "   [166 146 130 ... 166 146 130]\n",
      "   [173 151 131 ... 173 151 131]\n",
      "   ...\n",
      "   [176 161 151 ... 176 161 151]\n",
      "   [136 130 133 ... 136 130 133]\n",
      "   [ 93 120 152 ...  93 120 152]]\n",
      "\n",
      "  [[160 138 123 ... 160 138 123]\n",
      "   [168 144 124 ... 168 144 124]\n",
      "   [174 150 129 ... 174 150 129]\n",
      "   ...\n",
      "   [177 164 156 ... 177 164 156]\n",
      "   [157 145 139 ... 157 145 139]\n",
      "   [ 84 105 133 ...  84 105 133]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[173 162 159 ... 173 162 159]\n",
      "   [181 171 169 ... 181 171 169]\n",
      "   [183 171 162 ... 183 171 162]\n",
      "   ...\n",
      "   [175 164 152 ... 175 164 152]\n",
      "   [158 148 141 ... 158 148 141]\n",
      "   [ 46  48  50 ...  46  48  50]]\n",
      "\n",
      "  [[167 155 152 ... 167 155 152]\n",
      "   [177 165 157 ... 177 165 157]\n",
      "   [179 164 150 ... 179 164 150]\n",
      "   ...\n",
      "   [175 165 156 ... 175 165 156]\n",
      "   [130 121 116 ... 130 121 116]\n",
      "   [ 26  31  36 ...  26  31  36]]\n",
      "\n",
      "  [[157 147 148 ... 157 147 148]\n",
      "   [173 159 148 ... 173 159 148]\n",
      "   [177 161 146 ... 177 161 146]\n",
      "   ...\n",
      "   [171 160 151 ... 171 160 151]\n",
      "   [ 92  87  83 ...  92  87  83]\n",
      "   [ 21  29  37 ...  21  29  37]]]]\n",
      "[1 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 9)\n",
      "x_train shape: (140, 40, 30, 9)\n",
      "140 train samples\n",
      "47 test samples\n",
      "y_train shape: (140,)\n",
      "Learning rate:  0.0\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 40, 30, 9)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_341 (Conv2D)             (None, 40, 30, 16)   1312        input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_308 (BatchN (None, 40, 30, 16)   64          conv2d_341[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_308 (Activation)     (None, 40, 30, 16)   0           batch_normalization_308[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_342 (Conv2D)             (None, 40, 30, 16)   272         activation_308[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_309 (BatchN (None, 40, 30, 16)   64          conv2d_342[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_309 (Activation)     (None, 40, 30, 16)   0           batch_normalization_309[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_343 (Conv2D)             (None, 40, 30, 16)   2320        activation_309[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_310 (BatchN (None, 40, 30, 16)   64          conv2d_343[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_310 (Activation)     (None, 40, 30, 16)   0           batch_normalization_310[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_345 (Conv2D)             (None, 40, 30, 64)   1088        activation_308[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_344 (Conv2D)             (None, 40, 30, 64)   1088        activation_310[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_99 (Add)                    (None, 40, 30, 64)   0           conv2d_345[0][0]                 \n",
      "                                                                 conv2d_344[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_311 (BatchN (None, 40, 30, 64)   256         add_99[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_311 (Activation)     (None, 40, 30, 64)   0           batch_normalization_311[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_346 (Conv2D)             (None, 40, 30, 16)   1040        activation_311[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_312 (BatchN (None, 40, 30, 16)   64          conv2d_346[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_312 (Activation)     (None, 40, 30, 16)   0           batch_normalization_312[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_347 (Conv2D)             (None, 40, 30, 16)   2320        activation_312[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_313 (BatchN (None, 40, 30, 16)   64          conv2d_347[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_313 (Activation)     (None, 40, 30, 16)   0           batch_normalization_313[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_348 (Conv2D)             (None, 40, 30, 64)   1088        activation_313[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_100 (Add)                   (None, 40, 30, 64)   0           add_99[0][0]                     \n",
      "                                                                 conv2d_348[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_314 (BatchN (None, 40, 30, 64)   256         add_100[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_314 (Activation)     (None, 40, 30, 64)   0           batch_normalization_314[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_349 (Conv2D)             (None, 40, 30, 16)   1040        activation_314[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_315 (BatchN (None, 40, 30, 16)   64          conv2d_349[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_315 (Activation)     (None, 40, 30, 16)   0           batch_normalization_315[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_350 (Conv2D)             (None, 40, 30, 16)   2320        activation_315[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_316 (BatchN (None, 40, 30, 16)   64          conv2d_350[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_316 (Activation)     (None, 40, 30, 16)   0           batch_normalization_316[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_351 (Conv2D)             (None, 40, 30, 64)   1088        activation_316[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_101 (Add)                   (None, 40, 30, 64)   0           add_100[0][0]                    \n",
      "                                                                 conv2d_351[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_317 (BatchN (None, 40, 30, 64)   256         add_101[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_317 (Activation)     (None, 40, 30, 64)   0           batch_normalization_317[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_352 (Conv2D)             (None, 20, 15, 64)   4160        activation_317[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_318 (BatchN (None, 20, 15, 64)   256         conv2d_352[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_318 (Activation)     (None, 20, 15, 64)   0           batch_normalization_318[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_353 (Conv2D)             (None, 20, 15, 64)   36928       activation_318[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_319 (BatchN (None, 20, 15, 64)   256         conv2d_353[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_319 (Activation)     (None, 20, 15, 64)   0           batch_normalization_319[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_355 (Conv2D)             (None, 20, 15, 128)  8320        add_101[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_354 (Conv2D)             (None, 20, 15, 128)  8320        activation_319[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_102 (Add)                   (None, 20, 15, 128)  0           conv2d_355[0][0]                 \n",
      "                                                                 conv2d_354[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_320 (BatchN (None, 20, 15, 128)  512         add_102[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_320 (Activation)     (None, 20, 15, 128)  0           batch_normalization_320[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_356 (Conv2D)             (None, 20, 15, 64)   8256        activation_320[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_321 (BatchN (None, 20, 15, 64)   256         conv2d_356[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_321 (Activation)     (None, 20, 15, 64)   0           batch_normalization_321[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_357 (Conv2D)             (None, 20, 15, 64)   36928       activation_321[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_322 (BatchN (None, 20, 15, 64)   256         conv2d_357[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_322 (Activation)     (None, 20, 15, 64)   0           batch_normalization_322[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_358 (Conv2D)             (None, 20, 15, 128)  8320        activation_322[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_103 (Add)                   (None, 20, 15, 128)  0           add_102[0][0]                    \n",
      "                                                                 conv2d_358[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_323 (BatchN (None, 20, 15, 128)  512         add_103[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_323 (Activation)     (None, 20, 15, 128)  0           batch_normalization_323[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_359 (Conv2D)             (None, 20, 15, 64)   8256        activation_323[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_324 (BatchN (None, 20, 15, 64)   256         conv2d_359[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_324 (Activation)     (None, 20, 15, 64)   0           batch_normalization_324[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_360 (Conv2D)             (None, 20, 15, 64)   36928       activation_324[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_325 (BatchN (None, 20, 15, 64)   256         conv2d_360[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_325 (Activation)     (None, 20, 15, 64)   0           batch_normalization_325[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_361 (Conv2D)             (None, 20, 15, 128)  8320        activation_325[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_104 (Add)                   (None, 20, 15, 128)  0           add_103[0][0]                    \n",
      "                                                                 conv2d_361[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_326 (BatchN (None, 20, 15, 128)  512         add_104[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_326 (Activation)     (None, 20, 15, 128)  0           batch_normalization_326[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_362 (Conv2D)             (None, 10, 8, 128)   16512       activation_326[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_327 (BatchN (None, 10, 8, 128)   512         conv2d_362[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_327 (Activation)     (None, 10, 8, 128)   0           batch_normalization_327[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_363 (Conv2D)             (None, 10, 8, 128)   147584      activation_327[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_328 (BatchN (None, 10, 8, 128)   512         conv2d_363[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_328 (Activation)     (None, 10, 8, 128)   0           batch_normalization_328[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_365 (Conv2D)             (None, 10, 8, 256)   33024       add_104[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_364 (Conv2D)             (None, 10, 8, 256)   33024       activation_328[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_105 (Add)                   (None, 10, 8, 256)   0           conv2d_365[0][0]                 \n",
      "                                                                 conv2d_364[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_329 (BatchN (None, 10, 8, 256)   1024        add_105[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_329 (Activation)     (None, 10, 8, 256)   0           batch_normalization_329[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_366 (Conv2D)             (None, 10, 8, 128)   32896       activation_329[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_330 (BatchN (None, 10, 8, 128)   512         conv2d_366[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_330 (Activation)     (None, 10, 8, 128)   0           batch_normalization_330[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_367 (Conv2D)             (None, 10, 8, 128)   147584      activation_330[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_331 (BatchN (None, 10, 8, 128)   512         conv2d_367[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_331 (Activation)     (None, 10, 8, 128)   0           batch_normalization_331[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_368 (Conv2D)             (None, 10, 8, 256)   33024       activation_331[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_106 (Add)                   (None, 10, 8, 256)   0           add_105[0][0]                    \n",
      "                                                                 conv2d_368[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_332 (BatchN (None, 10, 8, 256)   1024        add_106[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_332 (Activation)     (None, 10, 8, 256)   0           batch_normalization_332[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_369 (Conv2D)             (None, 10, 8, 128)   32896       activation_332[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_333 (BatchN (None, 10, 8, 128)   512         conv2d_369[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_333 (Activation)     (None, 10, 8, 128)   0           batch_normalization_333[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_370 (Conv2D)             (None, 10, 8, 128)   147584      activation_333[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_334 (BatchN (None, 10, 8, 128)   512         conv2d_370[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_334 (Activation)     (None, 10, 8, 128)   0           batch_normalization_334[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_371 (Conv2D)             (None, 10, 8, 256)   33024       activation_334[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_107 (Add)                   (None, 10, 8, 256)   0           add_106[0][0]                    \n",
      "                                                                 conv2d_371[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_335 (BatchN (None, 10, 8, 256)   1024        add_107[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_335 (Activation)     (None, 10, 8, 256)   0           batch_normalization_335[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_11 (AveragePo (None, 1, 1, 256)    0           activation_335[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 256)          0           average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 10)           2570        flatten_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 849,866\n",
      "Trainable params: 844,650\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "ResNet  29v  2\n",
      "Using real-time data augmentation.\n",
      "------------------\n",
      "[[[[ 1.49691910e-01  8.54063928e-02  3.10925245e-02 ...  1.49691910e-01\n",
      "     8.54063928e-02  3.10925245e-02]\n",
      "   [ 7.08963275e-02  3.52661908e-02 -8.57147574e-03 ...  7.08963275e-02\n",
      "     3.52661908e-02 -8.57147574e-03]\n",
      "   [ 5.54342866e-02  6.89357519e-02  5.50981462e-02 ...  5.54342866e-02\n",
      "     6.89357519e-02  5.50981462e-02]\n",
      "   ...\n",
      "   [ 9.66105461e-02  8.34452510e-02  5.10084331e-02 ...  9.66105461e-02\n",
      "     8.34452510e-02  5.10084331e-02]\n",
      "   [ 1.56946957e-01  1.40980214e-01  1.22492909e-01 ...  1.56946957e-01\n",
      "     1.40980214e-01  1.22492909e-01]\n",
      "   [ 2.53165215e-01  2.30644435e-01  2.47563064e-01 ...  2.53165215e-01\n",
      "     2.30644435e-01  2.47563064e-01]]\n",
      "\n",
      "  [[ 1.06302559e-01  5.49020469e-02 -1.25769973e-02 ...  1.06302559e-01\n",
      "     5.49020469e-02 -1.25769973e-02]\n",
      "   [ 5.64426780e-02  4.94678915e-02  2.07281113e-02 ...  5.64426780e-02\n",
      "     4.94678915e-02  2.07281113e-02]\n",
      "   [ 3.45660448e-02  4.27732468e-02  1.68627203e-02 ...  3.45660448e-02\n",
      "     4.27732468e-02  1.68627203e-02]\n",
      "   ...\n",
      "   [ 9.09804106e-02  8.23528171e-02  5.80112040e-02 ...  9.09804106e-02\n",
      "     8.23528171e-02  5.80112040e-02]\n",
      "   [ 1.19859755e-01  1.08179122e-01  6.98878169e-02 ...  1.19859755e-01\n",
      "     1.08179122e-01  6.98878169e-02]\n",
      "   [ 2.29467630e-01  2.34958142e-01  2.62605071e-01 ...  2.29467630e-01\n",
      "     2.34958142e-01  2.62605071e-01]]\n",
      "\n",
      "  [[ 3.68347764e-02 -2.24371254e-02 -1.10476226e-01 ...  3.68347764e-02\n",
      "    -2.24371254e-02 -1.10476226e-01]\n",
      "   [ 5.54062724e-02  6.88236952e-02  4.59384620e-02 ...  5.54062724e-02\n",
      "     6.88236952e-02  4.59384620e-02]\n",
      "   [ 1.50983334e-02  1.96158886e-04 -5.17926514e-02 ...  1.50983334e-02\n",
      "     1.96158886e-04 -5.17926514e-02]\n",
      "   ...\n",
      "   [ 8.93557668e-02  7.64423609e-02  7.99999237e-02 ...  8.93557668e-02\n",
      "     7.64423609e-02  7.99999237e-02]\n",
      "   [ 8.59943032e-02  2.37533450e-02 -2.20168233e-02 ...  8.59943032e-02\n",
      "     2.37533450e-02 -2.20168233e-02]\n",
      "   [ 1.71652794e-01  1.51456624e-01  1.26974791e-01 ...  1.71652794e-01\n",
      "     1.51456624e-01  1.26974791e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 2.26526678e-01  2.35378027e-01  2.55798221e-01 ...  2.26526678e-01\n",
      "     2.35378027e-01  2.55798221e-01]\n",
      "   [ 1.61428511e-01  1.75602257e-01  1.81848705e-01 ...  1.61428511e-01\n",
      "     1.75602257e-01  1.81848705e-01]\n",
      "   [ 6.80112839e-02  7.15407729e-02  1.84595883e-02 ...  6.80112839e-02\n",
      "     7.15407729e-02  1.84595883e-02]\n",
      "   ...\n",
      "   [ 1.02969289e-01  1.08655512e-01  7.59664774e-02 ...  1.02969289e-01\n",
      "     1.08655512e-01  7.59664774e-02]\n",
      "   [ 1.30084097e-01  1.49607778e-01  1.19187623e-01 ...  1.30084097e-01\n",
      "     1.49607778e-01  1.19187623e-01]\n",
      "   [ 1.55938506e-01  1.46918923e-01  1.21176571e-01 ...  1.55938506e-01\n",
      "     1.46918923e-01  1.21176571e-01]]\n",
      "\n",
      "  [[ 2.08431423e-01  2.09859908e-01  2.05770314e-01 ...  2.08431423e-01\n",
      "     2.09859908e-01  2.05770314e-01]\n",
      "   [ 1.85546279e-01  2.17422903e-01  2.38739491e-01 ...  1.85546279e-01\n",
      "     2.17422903e-01  2.38739491e-01]\n",
      "   [ 8.04482102e-02  6.85155392e-02  5.74240088e-03 ...  8.04482102e-02\n",
      "     6.85155392e-02  5.74240088e-03]\n",
      "   ...\n",
      "   [ 1.11400545e-01  1.18823469e-01  8.15405548e-02 ...  1.11400545e-01\n",
      "     1.18823469e-01  8.15405548e-02]\n",
      "   [ 1.37198865e-01  1.42913163e-01  9.15406048e-02 ...  1.37198865e-01\n",
      "     1.42913163e-01  9.15406048e-02]\n",
      "   [ 1.53809607e-01  1.45266086e-01  1.17451012e-01 ...  1.53809607e-01\n",
      "     1.45266086e-01  1.17451012e-01]]\n",
      "\n",
      "  [[ 2.12857187e-01  1.99131757e-01  1.86890930e-01 ...  2.12857187e-01\n",
      "     1.99131757e-01  1.86890930e-01]\n",
      "   [ 2.16358542e-01  2.55658150e-01  2.66470581e-01 ...  2.16358542e-01\n",
      "     2.55658150e-01  2.66470581e-01]\n",
      "   [ 1.24397635e-01  1.25098050e-01  7.91596174e-02 ...  1.24397635e-01\n",
      "     1.25098050e-01  7.91596174e-02]\n",
      "   ...\n",
      "   [ 1.09075665e-01  1.01876676e-01  6.31372035e-02 ...  1.09075665e-01\n",
      "     1.01876676e-01  6.31372035e-02]\n",
      "   [ 1.49607778e-01  1.44593805e-01  1.06666654e-01 ...  1.49607778e-01\n",
      "     1.44593805e-01  1.06666654e-01]\n",
      "   [ 1.63389206e-01  1.53697491e-01  1.05406076e-01 ...  1.63389206e-01\n",
      "     1.53697491e-01  1.05406076e-01]]]\n",
      "\n",
      "\n",
      " [[[ 1.84986025e-01  9.32495296e-02  1.54062510e-02 ...  1.84986025e-01\n",
      "     9.32495296e-02  1.54062510e-02]\n",
      "   [ 1.02268875e-01  5.09524643e-02 -1.24930441e-02 ...  1.02268875e-01\n",
      "     5.09524643e-02 -1.24930441e-02]\n",
      "   [ 9.85715389e-02  6.10926151e-02 -2.72548199e-02 ...  9.85715389e-02\n",
      "     6.10926151e-02 -2.72548199e-02]\n",
      "   ...\n",
      "   [ 1.12296820e-01  6.38374090e-02 -1.95798278e-02 ...  1.12296820e-01\n",
      "     6.38374090e-02 -1.95798278e-02]\n",
      "   [ 1.33417547e-01  7.03919828e-02 -6.91887736e-03 ...  1.33417547e-01\n",
      "     7.03919828e-02 -6.91887736e-03]\n",
      "   [ 1.82576984e-01  1.32605225e-01  1.02465034e-01 ...  1.82576984e-01\n",
      "     1.32605225e-01  1.02465034e-01]]\n",
      "\n",
      "  [[ 1.41596675e-01  8.62746239e-02  1.48739815e-02 ...  1.41596675e-01\n",
      "     8.62746239e-02  1.48739815e-02]\n",
      "   [ 8.78152251e-02  4.94678915e-02 -2.63307095e-02 ...  8.78152251e-02\n",
      "     4.94678915e-02 -2.63307095e-02]\n",
      "   [ 8.16248655e-02  3.49301100e-02 -6.15686476e-02 ...  8.16248655e-02\n",
      "     3.49301100e-02 -6.15686476e-02]\n",
      "   ...\n",
      "   [ 9.88235474e-02  4.70587015e-02 -5.17927408e-02 ...  9.88235474e-02\n",
      "     4.70587015e-02 -5.17927408e-02]\n",
      "   [ 1.27702892e-01  9.64144170e-02  7.14269280e-03 ...  1.27702892e-01\n",
      "     9.64144170e-02  7.14269280e-03]\n",
      "   [ 1.51036263e-01  1.21232659e-01  9.00560617e-02 ...  1.51036263e-01\n",
      "     1.21232659e-01  9.00560617e-02]]\n",
      "\n",
      "  [[ 1.07423007e-01  7.16805458e-02 -6.72310591e-04 ...  1.07423007e-01\n",
      "     7.16805458e-02 -6.72310591e-04]\n",
      "   [ 8.67788196e-02  4.52942848e-02 -4.81792092e-02 ...  8.67788196e-02\n",
      "     4.52942848e-02 -4.81792092e-02]\n",
      "   [ 7.78434277e-02  3.54902744e-02 -6.74789250e-02 ...  7.78434277e-02\n",
      "     3.54902744e-02 -6.74789250e-02]\n",
      "   ...\n",
      "   [ 9.32773352e-02  3.72266769e-02 -4.94118631e-02 ...  9.32773352e-02\n",
      "     3.72266769e-02 -4.94118631e-02]\n",
      "   [ 1.21288419e-01  5.90474606e-02 -2.20168233e-02 ...  1.21288419e-01\n",
      "     5.90474606e-02 -2.20168233e-02]\n",
      "   [ 1.36358678e-01  1.04397804e-01  4.85434234e-02 ...  1.36358678e-01\n",
      "     1.04397804e-01  4.85434234e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.48095310e-01  1.25574112e-01  1.02857053e-01 ...  1.48095310e-01\n",
      "     1.25574112e-01  1.02857053e-01]\n",
      "   [ 1.06526554e-01  6.97199106e-02  5.37809730e-03 ...  1.06526554e-01\n",
      "     6.97199106e-02  5.37809730e-03]\n",
      "   [ 8.76191258e-02  7.54623413e-02 -8.99142027e-03 ...  8.76191258e-02\n",
      "     7.54623413e-02 -8.99142027e-03]\n",
      "   ...\n",
      "   [ 1.10812426e-01  1.08655512e-01  1.71429515e-02 ...  1.10812426e-01\n",
      "     1.08655512e-01  1.71429515e-02]\n",
      "   [ 1.22240961e-01  1.02548957e-01 -6.30259514e-03 ...  1.22240961e-01\n",
      "     1.02548957e-01 -6.30259514e-03]\n",
      "   [ 1.40252233e-01  1.11624807e-01  3.49020362e-02 ...  1.40252233e-01\n",
      "     1.11624807e-01  3.49020362e-02]]\n",
      "\n",
      "  [[ 1.61372602e-01  1.47114813e-01  1.11652672e-01 ...  1.61372602e-01\n",
      "     1.47114813e-01  1.11652672e-01]\n",
      "   [ 1.07114911e-01  9.19327140e-02  5.05042076e-02 ...  1.07114911e-01\n",
      "     9.19327140e-02  5.05042076e-02]\n",
      "   [ 1.00056052e-01  8.02802444e-02 -2.10073590e-03 ...  1.00056052e-01\n",
      "     8.02802444e-02 -2.10073590e-03]\n",
      "   ...\n",
      "   [ 1.19243681e-01  1.10980332e-01  1.09522939e-02 ...  1.19243681e-01\n",
      "     1.10980332e-01  1.09522939e-02]\n",
      "   [ 1.29355729e-01  1.03697479e-01 -2.21849084e-02 ...  1.29355729e-01\n",
      "     1.03697479e-01 -2.21849084e-02]\n",
      "   [ 1.49888039e-01  1.21736676e-01  6.25490248e-02 ...  1.49888039e-01\n",
      "     1.21736676e-01  6.25490248e-02]]\n",
      "\n",
      "  [[ 1.73641503e-01  1.44229800e-01  7.31654465e-02 ...  1.73641503e-01\n",
      "     1.44229800e-01  7.31654465e-02]\n",
      "   [ 1.14397764e-01  1.10560119e-01  6.25490248e-02 ...  1.14397764e-01\n",
      "     1.10560119e-01  6.25490248e-02]\n",
      "   [ 1.12632930e-01  8.19607973e-02 -7.11491704e-03 ...  1.12632930e-01\n",
      "     8.19607973e-02 -7.11491704e-03]\n",
      "   ...\n",
      "   [ 1.20840371e-01  9.79551077e-02 -7.45105743e-03 ...  1.20840371e-01\n",
      "     9.79551077e-02 -7.45105743e-03]\n",
      "   [ 1.37843072e-01  1.01456553e-01 -1.09804273e-02 ...  1.37843072e-01\n",
      "     1.01456553e-01 -1.09804273e-02]\n",
      "   [ 1.79075480e-01  1.57619059e-01  7.01119304e-02 ...  1.79075480e-01\n",
      "     1.57619059e-01  7.01119304e-02]]]\n",
      "\n",
      "\n",
      " [[[ 3.20448279e-02 -2.04759836e-02 -3.94957066e-02 ...  3.20448279e-02\n",
      "    -2.04759836e-02 -3.94957066e-02]\n",
      "   [ 1.06190443e-01  8.23250115e-02  5.02520800e-02 ...  1.06190443e-01\n",
      "     8.23250115e-02  5.02520800e-02]\n",
      "   [ 9.07284021e-02  9.24651623e-02  5.50981462e-02 ...  9.07284021e-02\n",
      "     9.24651623e-02  5.50981462e-02]\n",
      "   ...\n",
      "   [ 9.66105461e-02  1.14817798e-01  1.05910391e-01 ...  9.66105461e-02\n",
      "     1.14817798e-01  1.05910391e-01]\n",
      "   [-1.05798155e-01 -2.37256885e-02  5.19046783e-02 ... -1.05798155e-01\n",
      "    -2.37256885e-02  5.19046783e-02]\n",
      "   [-1.50756359e-01 -1.64144039e-02  1.18151307e-01 ... -1.50756359e-01\n",
      "    -1.64144039e-02  1.18151307e-01]]\n",
      "\n",
      "  [[ 9.84594226e-02  5.49020469e-02  1.09524131e-02 ...  9.84594226e-02\n",
      "     5.49020469e-02  1.09524131e-02]\n",
      "   [ 8.78152251e-02  8.86835754e-02  5.60222566e-02 ...  8.78152251e-02\n",
      "     8.86835754e-02  5.60222566e-02]\n",
      "   [ 7.77032971e-02  7.80673623e-02  3.64705920e-02 ...  7.77032971e-02\n",
      "     7.80673623e-02  3.64705920e-02]\n",
      "   ...\n",
      "   [ 9.88235474e-02  1.13725364e-01  9.33053195e-02 ...  9.88235474e-02\n",
      "     1.13725364e-01  9.33053195e-02]\n",
      "   [-3.30814123e-02  1.79830492e-02  4.24368382e-02 ... -3.30814123e-02\n",
      "     1.79830492e-02  4.24368382e-02]\n",
      "   [-1.47002965e-01  1.14287138e-02  1.56722724e-01 ... -1.47002965e-01\n",
      "     1.14287138e-02  1.56722724e-01]]\n",
      "\n",
      "  [[ 9.56583023e-02  7.95236826e-02  2.67786682e-02 ...  9.56583023e-02\n",
      "     7.95236826e-02  2.67786682e-02]\n",
      "   [ 8.28572512e-02  7.66668320e-02  2.63305902e-02 ...  8.28572512e-02\n",
      "     7.66668320e-02  2.63305902e-02]\n",
      "   [ 7.78434277e-02  7.07843900e-02  2.27171779e-02 ...  7.78434277e-02\n",
      "     7.07843900e-02  2.27171779e-02]\n",
      "   ...\n",
      "   [ 9.71989036e-02  1.11736476e-01  1.15294039e-01 ...  9.71989036e-02\n",
      "     1.11736476e-01  1.15294039e-01]\n",
      "   [ 4.28570509e-02  4.72827554e-02  5.64145744e-02 ...  4.28570509e-02\n",
      "     4.72827554e-02  5.64145744e-02]\n",
      "   [-2.08739370e-01 -7.20728040e-02  4.07002866e-02 ... -2.08739370e-01\n",
      "    -7.20728040e-02  4.07002866e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.24565899e-01  1.21652544e-01  1.34229600e-01 ...  1.24565899e-01\n",
      "     1.21652544e-01  1.34229600e-01]\n",
      "   [ 1.22212827e-01  1.44229710e-01  1.70084000e-01 ...  1.22212827e-01\n",
      "     1.44229710e-01  1.70084000e-01]\n",
      "   [ 1.07226968e-01  1.57815278e-01  1.47871345e-01 ...  1.07226968e-01\n",
      "     1.57815278e-01  1.47871345e-01]\n",
      "   ...\n",
      "   [ 7.94398785e-02  1.24341786e-01  1.03417456e-01 ...  7.94398785e-02\n",
      "     1.24341786e-01  1.03417456e-01]\n",
      "   [ 3.98880243e-02  8.68626833e-02  8.38935077e-02 ...  3.98880243e-02\n",
      "     8.68626833e-02  8.38935077e-02]\n",
      "   [-3.57787013e-01 -2.84453630e-01 -2.55294025e-01 ... -3.57787013e-01\n",
      "    -2.84453630e-01 -2.55294025e-01]]\n",
      "\n",
      "  [[ 1.10392213e-01  1.15742266e-01  1.15574241e-01 ...  1.10392213e-01\n",
      "     1.15742266e-01  1.15574241e-01]\n",
      "   [ 1.07114911e-01  1.23305261e-01  1.25014007e-01 ...  1.07114911e-01\n",
      "     1.23305261e-01  1.25014007e-01]\n",
      "   [ 1.03977621e-01  1.27339065e-01  9.59385037e-02 ...  1.03977621e-01\n",
      "     1.27339065e-01  9.59385037e-02]\n",
      "   ...\n",
      "   [ 9.17927027e-02  1.38431311e-01  1.28599375e-01 ...  9.17927027e-02\n",
      "     1.38431311e-01  1.28599375e-01]\n",
      "   [-5.49579859e-02 -1.00280344e-02 -1.82633400e-02 ... -5.49579859e-02\n",
      "    -1.00280344e-02 -1.82633400e-02]\n",
      "   [-4.18739438e-01 -3.37086886e-01 -2.90392160e-01 ... -4.18739438e-01\n",
      "    -3.37086886e-01 -2.90392160e-01]]\n",
      "\n",
      "  [[ 9.91317034e-02  1.16778821e-01  1.24145836e-01 ...  9.91317034e-02\n",
      "     1.16778821e-01  1.24145836e-01]\n",
      "   [ 1.06554627e-01  1.18403256e-01  8.60784352e-02 ...  1.06554627e-01\n",
      "     1.18403256e-01  8.60784352e-02]\n",
      "   [ 1.12632930e-01  1.25098050e-01  9.09243226e-02 ...  1.12632930e-01\n",
      "     1.25098050e-01  9.09243226e-02]\n",
      "   ...\n",
      "   [ 8.16246867e-02  1.21484518e-01  1.18039161e-01 ...  8.16246867e-02\n",
      "     1.21484518e-01  1.18039161e-01]\n",
      "   [-1.87647134e-01 -1.37759149e-01 -1.32549047e-01 ... -1.87647134e-01\n",
      "    -1.37759149e-01 -1.32549047e-01]\n",
      "   [-4.13081408e-01 -3.16890776e-01 -2.82829225e-01 ... -4.13081408e-01\n",
      "    -3.16890776e-01 -2.82829225e-01]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-3.48347336e-01 -3.53809327e-01 -3.37534934e-01 ... -3.48347336e-01\n",
      "    -3.53809327e-01 -3.37534934e-01]\n",
      "   [-2.42829174e-01 -2.74537742e-01 -2.51708746e-01 ... -2.42829174e-01\n",
      "    -2.74537742e-01 -2.51708746e-01]\n",
      "   [-2.66134351e-01 -2.72240758e-01 -2.42941096e-01 ... -2.66134351e-01\n",
      "    -2.72240758e-01 -2.42941096e-01]\n",
      "   ...\n",
      "   [-2.68095344e-01 -2.93025374e-01 -7.81512260e-03 ... -2.68095344e-01\n",
      "    -2.93025374e-01 -7.81512260e-03]\n",
      "   [-2.39131480e-01 -2.70784497e-01  8.76739621e-03 ... -2.39131480e-01\n",
      "    -2.70784497e-01  8.76739621e-03]\n",
      "   [-2.21344590e-01 -2.67394781e-01 -2.50476182e-01 ... -2.21344590e-01\n",
      "    -2.67394781e-01 -2.50476182e-01]]\n",
      "\n",
      "  [[-2.81932771e-01 -2.94117570e-01 -2.87086785e-01 ... -2.81932771e-01\n",
      "    -2.94117570e-01 -2.87086785e-01]\n",
      "   [-2.53361255e-01 -2.64257610e-01 -2.42016986e-01 ... -2.53361255e-01\n",
      "    -2.64257610e-01 -2.42016986e-01]\n",
      "   [-2.83081025e-01 -2.90560126e-01 -2.65490234e-01 ... -2.83081025e-01\n",
      "    -2.90560126e-01 -2.65490234e-01]\n",
      "   ...\n",
      "   [-2.73725480e-01 -2.94117808e-01 -1.25770569e-02 ... -2.73725480e-01\n",
      "    -2.94117808e-01 -1.25770569e-02]\n",
      "   [-2.56610841e-01 -2.64369905e-01  1.10642612e-02 ... -2.56610841e-01\n",
      "    -2.64369905e-01  1.10642612e-02]\n",
      "   [-2.33277470e-01 -2.55237937e-01  3.12325060e-02 ... -2.33277470e-01\n",
      "    -2.55237937e-01  3.12325060e-02]]\n",
      "\n",
      "  [[-2.45518178e-01 -2.49888107e-01 -2.43809566e-01 ... -2.45518178e-01\n",
      "    -2.49888107e-01 -2.43809566e-01]\n",
      "   [-2.62240797e-01 -2.56666541e-01  2.63305902e-02 ... -2.62240797e-01\n",
      "    -2.56666541e-01  2.63305902e-02]\n",
      "   [-2.82940894e-01 -2.89999962e-01 -2.63557374e-01 ... -2.82940894e-01\n",
      "    -2.89999962e-01 -2.63557374e-01]\n",
      "   ...\n",
      "   [-2.71428555e-01 -2.96106696e-01 -2.35304236e-03 ... -2.71428555e-01\n",
      "    -2.96106696e-01 -2.35304236e-03]\n",
      "   [-2.59103745e-01 -2.93893754e-01  1.51258707e-03 ... -2.59103745e-01\n",
      "    -2.93893754e-01  1.51258707e-03]\n",
      "   [-2.47955054e-01 -2.75994360e-01 -6.35856390e-03 ... -2.47955054e-01\n",
      "    -2.75994360e-01 -6.35856390e-03]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-2.87198812e-01 -3.13641608e-01 -2.65547335e-02 ... -2.87198812e-01\n",
      "    -3.13641608e-01 -2.65547335e-02]\n",
      "   [-2.85630316e-01 -2.98907578e-01 -6.38660789e-03 ... -2.85630316e-01\n",
      "    -2.98907578e-01 -6.38660789e-03]\n",
      "   [-2.92773038e-01 -2.89243579e-01 -2.87422776e-01 ... -2.92773038e-01\n",
      "    -2.89243579e-01 -2.87422776e-01]\n",
      "   ...\n",
      "   [-2.85266012e-01 -2.79579818e-01 -2.76974738e-01 ... -2.85266012e-01\n",
      "    -2.79579818e-01 -2.76974738e-01]\n",
      "   [-2.65994340e-01 -2.62156963e-01 -2.57282972e-01 ... -2.65994340e-01\n",
      "    -2.62156963e-01 -2.57282972e-01]\n",
      "   [-2.40139931e-01 -2.53081083e-01 -2.31764629e-01 ... -2.40139931e-01\n",
      "    -2.53081083e-01 -2.31764629e-01]]\n",
      "\n",
      "  [[-3.24901938e-01 -3.31316590e-01 -3.19719911e-01 ... -3.24901938e-01\n",
      "    -3.31316590e-01 -3.19719911e-01]\n",
      "   [-1.83752775e-02 -2.57143676e-02  7.36692548e-03 ... -1.83752775e-02\n",
      "    -2.57143676e-02  7.36692548e-03]\n",
      "   [-2.92100817e-01 -3.00111949e-01 -2.96218395e-01 ... -2.92100817e-01\n",
      "    -3.00111949e-01 -2.96218395e-01]\n",
      "   ...\n",
      "   [-2.80756325e-01 -2.85098135e-01 -2.79243767e-01 ... -2.80756325e-01\n",
      "    -2.85098135e-01 -2.79243767e-01]\n",
      "   [-2.62801141e-01 -2.68851578e-01 -2.77086854e-01 ... -2.62801141e-01\n",
      "    -2.68851578e-01 -2.77086854e-01]\n",
      "   [-2.30504125e-01 -2.39047661e-01 -2.08039209e-01 ... -2.30504125e-01\n",
      "    -2.39047661e-01 -2.08039209e-01]]\n",
      "\n",
      "  [[-3.91064405e-01 -3.81260425e-01 -3.73893410e-01 ... -3.91064405e-01\n",
      "    -3.81260425e-01 -3.73893410e-01]\n",
      "   [-1.10924244e-02 -1.49300992e-02 -8.03923607e-03 ... -1.10924244e-02\n",
      "    -1.49300992e-02 -8.03923607e-03]\n",
      "   [-2.79523939e-01 -2.94509828e-01 -2.85546303e-01 ... -2.79523939e-01\n",
      "    -2.94509828e-01 -2.85546303e-01]\n",
      "   ...\n",
      "   [-2.79159635e-01 -2.82437086e-01 -2.62353003e-01 ... -2.79159635e-01\n",
      "    -2.82437086e-01 -2.62353003e-01]\n",
      "   [-2.46470660e-01 -2.43641511e-01 -2.34509841e-01 ... -2.46470660e-01\n",
      "    -2.43641511e-01 -2.34509841e-01]\n",
      "   [-2.20924526e-01 -2.18851551e-01 -2.16162577e-01 ... -2.20924526e-01\n",
      "    -2.18851551e-01 -2.16162577e-01]]]\n",
      "\n",
      "\n",
      " [[[-3.67955178e-01 -3.61652464e-01 -3.49299639e-01 ... -3.67955178e-01\n",
      "    -3.61652464e-01 -3.49299639e-01]\n",
      "   [-1.87927216e-01 -2.39243641e-01 -2.59551883e-01 ... -1.87927216e-01\n",
      "    -2.39243641e-01 -2.59551883e-01]\n",
      "   [-3.86833549e-02 -1.03613287e-01 -1.29215598e-01 ... -3.86833549e-02\n",
      "    -1.03613287e-01 -1.29215598e-01]\n",
      "   ...\n",
      "   [-1.26918882e-01 -1.91064566e-01 -2.07815111e-01 ... -1.26918882e-01\n",
      "    -1.91064566e-01 -2.07815111e-01]\n",
      "   [-3.21484447e-01 -2.82549202e-01 -3.01036537e-01 ... -3.21484447e-01\n",
      "    -2.82549202e-01 -3.01036537e-01]\n",
      "   [-3.74285787e-01 -3.92885000e-01 -3.95574212e-01 ... -3.74285787e-01\n",
      "    -3.92885000e-01 -3.95574212e-01]]\n",
      "\n",
      "  [[-2.66246498e-01 -2.70588160e-01 -2.79243648e-01 ... -2.66246498e-01\n",
      "    -2.70588160e-01 -2.79243648e-01]\n",
      "   [-8.47338140e-02 -1.58375263e-01 -1.83193445e-01 ... -8.47338140e-02\n",
      "    -1.58375263e-01 -1.83193445e-01]\n",
      "   [ 3.19349766e-03 -3.95797193e-02 -4.98039424e-02 ...  3.19349766e-03\n",
      "    -3.95797193e-02 -4.98039424e-02]\n",
      "   ...\n",
      "   [-6.19607568e-02 -1.09804064e-01 -1.10616267e-01 ... -6.19607568e-02\n",
      "    -1.09804064e-01 -1.10616267e-01]\n",
      "   [-2.48767704e-01 -2.87899315e-01 -3.26190650e-01 ... -2.48767704e-01\n",
      "    -2.87899315e-01 -3.26190650e-01]\n",
      "   [-3.07787299e-01 -3.06218326e-01 -3.68767500e-01 ... -3.07787299e-01\n",
      "    -3.06218326e-01 -3.68767500e-01]]\n",
      "\n",
      "  [[-1.63165241e-01 -2.06750840e-01 -2.35966429e-01 ... -1.63165241e-01\n",
      "    -2.06750840e-01 -2.35966429e-01]\n",
      "   [-2.69466639e-02 -9.58822072e-02 -1.07002735e-01 ... -2.69466639e-02\n",
      "    -9.58822072e-02 -1.07002735e-01]\n",
      "   [ 4.64708805e-02  1.96158886e-04 -8.65539908e-03 ...  4.64708805e-02\n",
      "     1.96158886e-04 -8.65539908e-03]\n",
      "   ...\n",
      "   [-2.43697166e-02 -7.64988363e-02 -6.90197051e-02 ... -2.43697166e-02\n",
      "    -7.64988363e-02 -6.90197051e-02]\n",
      "   [-1.68907672e-01 -2.46834904e-01 -2.72997200e-01 ... -1.68907672e-01\n",
      "    -2.46834904e-01 -2.72997200e-01]\n",
      "   [-3.45994294e-01 -3.62268895e-01 -3.98515433e-01 ... -3.45994294e-01\n",
      "    -3.62268895e-01 -3.98515433e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.86835527e-02 -8.61906111e-02 -1.75574332e-01 ...  1.86835527e-02\n",
      "    -8.61906111e-02 -1.75574332e-01]\n",
      "   [-7.19892979e-03 -9.10644233e-02 -1.27955228e-01 ... -7.19892979e-03\n",
      "    -9.10644233e-02 -1.27955228e-01]\n",
      "   [ 3.27171683e-02 -1.47337615e-02 -6.38933778e-02 ...  3.27171683e-02\n",
      "    -1.47337615e-02 -6.38933778e-02]\n",
      "   ...\n",
      "   [ 2.45379210e-02  6.94398284e-02  6.02802038e-02 ...  2.45379210e-02\n",
      "     6.94398284e-02  6.02802038e-02]\n",
      "   [-7.38374591e-02 -9.74510610e-02 -1.23949647e-01 ... -7.38374591e-02\n",
      "    -9.74510610e-02 -1.23949647e-01]\n",
      "   [-1.22492880e-01 -1.74649715e-01 -1.80784225e-01 ... -1.22492880e-01\n",
      "    -1.74649715e-01 -1.80784225e-01]]\n",
      "\n",
      "  [[-1.01372510e-01 -1.94061667e-01 -2.33445391e-01 ... -1.01372510e-01\n",
      "    -1.94061667e-01 -2.33445391e-01]\n",
      "   [-3.01399827e-02 -1.11988872e-01 -1.45574242e-01 ... -3.01399827e-02\n",
      "    -1.11988872e-01 -1.45574242e-01]\n",
      "   [ 1.77031159e-02 -4.12884057e-02 -9.62183774e-02 ...  1.77031159e-02\n",
      "    -4.12884057e-02 -9.62183774e-02]\n",
      "   ...\n",
      "   [-2.32493877e-03 -2.62745917e-02 -4.00280952e-02 ... -2.32493877e-03\n",
      "    -2.62745917e-02 -4.00280952e-02]\n",
      "   [-6.28011227e-02 -9.23809707e-02 -1.20224118e-01 ... -6.28011227e-02\n",
      "    -9.23809707e-02 -1.20224118e-01]\n",
      "   [-1.95210010e-01 -2.35126093e-01 -2.43333325e-01 ... -1.95210010e-01\n",
      "    -2.35126093e-01 -2.43333325e-01]]\n",
      "\n",
      "  [[-1.36162430e-01 -1.45966291e-01 -2.60167897e-01 ... -1.36162430e-01\n",
      "    -1.45966291e-01 -2.60167897e-01]\n",
      "   [-5.42296767e-02 -1.28655583e-01 -1.68823540e-01 ... -5.42296767e-02\n",
      "    -1.28655583e-01 -1.68823540e-01]\n",
      "   [ 2.82901525e-03 -5.92156947e-02 -8.55462849e-02 ...  2.82901525e-03\n",
      "    -5.92156947e-02 -8.55462849e-02]\n",
      "   ...\n",
      "   [-2.81792283e-02 -4.71429527e-02 -5.45098782e-02 ... -2.81792283e-02\n",
      "    -4.71429527e-02 -5.45098782e-02]\n",
      "   [-7.78432190e-02 -1.18151307e-01 -1.40392184e-01 ... -7.78432190e-02\n",
      "    -1.18151307e-01 -1.40392184e-01]\n",
      "   [-2.17002958e-01 -2.46302530e-01 -2.78907657e-01 ... -2.17002958e-01\n",
      "    -2.46302530e-01 -2.78907657e-01]]]\n",
      "\n",
      "\n",
      " [[[-1.01288497e-01 -3.38123053e-01 -3.25770229e-01 ... -1.01288497e-01\n",
      "    -3.38123053e-01 -3.25770229e-01]\n",
      "   [-3.84005666e-01 -3.56890678e-01 -3.41904819e-01 ... -3.84005666e-01\n",
      "    -3.56890678e-01 -3.41904819e-01]\n",
      "   [-2.22997099e-01 -2.17338771e-01 -1.95882261e-01 ... -2.22997099e-01\n",
      "    -2.17338771e-01 -1.95882261e-01]\n",
      "   ...\n",
      "   [-1.34762019e-01 -1.63613588e-01 -1.41148448e-01 ... -1.34762019e-01\n",
      "    -1.63613588e-01 -1.41148448e-01]\n",
      "   [-1.92072660e-01 -2.23725677e-01 -2.22605139e-01 ... -1.92072660e-01\n",
      "    -2.23725677e-01 -2.22605139e-01]\n",
      "   [-2.68403411e-01 -2.94845760e-01 -2.97535002e-01 ... -2.68403411e-01\n",
      "    -2.94845760e-01 -2.97535002e-01]]\n",
      "\n",
      "  [[-4.07422960e-01 -3.56862664e-01 -3.49831909e-01 ... -4.07422960e-01\n",
      "    -3.56862664e-01 -3.49831909e-01]\n",
      "   [-3.16106379e-01 -2.95630157e-01 -2.85154223e-01 ... -3.16106379e-01\n",
      "    -2.95630157e-01 -2.85154223e-01]\n",
      "   [-1.81120247e-01 -1.72913045e-01 -1.40000015e-01 ... -1.81120247e-01\n",
      "    -1.72913045e-01 -1.40000015e-01]\n",
      "   ...\n",
      "   [-1.09019607e-01 -1.41176611e-01 -1.18459404e-01 ... -1.09019607e-01\n",
      "    -1.41176611e-01 -1.18459404e-01]\n",
      "   [-1.78179473e-01 -1.89860106e-01 -1.88935727e-01 ... -1.78179473e-01\n",
      "    -1.89860106e-01 -1.88935727e-01]\n",
      "   [-2.25434333e-01 -2.39551678e-01 -2.27591023e-01 ... -2.25434333e-01\n",
      "    -2.39551678e-01 -2.27591023e-01]]\n",
      "\n",
      "  [[-4.10224080e-01 -3.55770469e-01 -3.57535064e-01 ... -4.10224080e-01\n",
      "    -3.55770469e-01 -3.57535064e-01]\n",
      "   [-2.34789819e-01 -2.25293964e-01 -2.16806665e-01 ... -2.34789819e-01\n",
      "    -2.25293964e-01 -2.16806665e-01]\n",
      "   [-1.33921295e-01 -1.21372491e-01 -7.92436302e-02 ... -1.33921295e-01\n",
      "    -1.21372491e-01 -7.92436302e-02]\n",
      "   ...\n",
      "   [-9.49579477e-02 -1.23557657e-01 -8.07844102e-02 ... -9.49579477e-02\n",
      "    -1.23557657e-01 -8.07844102e-02]\n",
      "   [-1.45378262e-01 -1.88011378e-01 -1.59271717e-01 ... -1.45378262e-01\n",
      "    -1.88011378e-01 -1.59271717e-01]\n",
      "   [-2.04817802e-01 -2.25013971e-01 -2.33809546e-01 ... -2.04817802e-01\n",
      "    -2.25013971e-01 -2.33809546e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-6.36694133e-02 -7.05043375e-02 -7.75351226e-02 ... -6.36694133e-02\n",
      "    -7.05043375e-02 -7.75351226e-02]\n",
      "   [-1.64061695e-01  8.54061842e-02  8.38094950e-02 ... -1.64061695e-01\n",
      "     8.54061842e-02  8.38094950e-02]\n",
      "   [-8.88514519e-02  2.12717235e-01  2.18459576e-01 ... -8.88514519e-02\n",
      "     2.12717235e-01  2.18459576e-01]\n",
      "   ...\n",
      "   [-6.17365837e-02  2.26302564e-01  2.36750782e-01 ... -6.17365837e-02\n",
      "     2.26302564e-01  2.36750782e-01]\n",
      "   [-8.95237625e-02  1.92745030e-01  1.93697423e-01 ... -8.95237625e-02\n",
      "     1.92745030e-01  1.93697423e-01]\n",
      "   [-1.26414448e-01  1.23389512e-01  9.37255919e-02 ... -1.26414448e-01\n",
      "     1.23389512e-01  9.37255919e-02]]\n",
      "\n",
      "  [[-4.11176443e-01 -1.11708730e-01 -1.11876756e-01 ... -4.11176443e-01\n",
      "    -1.11708730e-01 -1.11876756e-01]\n",
      "   [ 4.43698168e-02  3.70307565e-02  3.48179340e-02 ...  4.43698168e-02\n",
      "     3.70307565e-02  3.48179340e-02]\n",
      "   [-1.07787102e-01  1.74397886e-01  1.74369872e-01 ... -1.07787102e-01\n",
      "     1.74397886e-01  1.74369872e-01]\n",
      "   ...\n",
      "   [-8.07563066e-02  2.01176405e-01  2.07030743e-01 ... -8.07563066e-02\n",
      "     2.01176405e-01  2.07030743e-01]\n",
      "   [-1.09859973e-01 -1.15910381e-01 -1.43753529e-01 ... -1.09859973e-01\n",
      "    -1.15910381e-01 -1.43753529e-01]\n",
      "   [-1.52072757e-01 -1.72380984e-01 -1.92352936e-01 ... -1.52072757e-01\n",
      "    -1.72380984e-01 -1.92352936e-01]]\n",
      "\n",
      "  [[-4.38123226e-01 -3.81260425e-01 -3.85658115e-01 ... -4.38123226e-01\n",
      "    -3.81260425e-01 -3.85658115e-01]\n",
      "   [-2.28571296e-02 -1.10085309e-02 -3.54902148e-02 ... -2.28571296e-02\n",
      "    -1.10085309e-02 -3.54902148e-02]\n",
      "   [-1.30504340e-01  1.36862755e-01  1.26218438e-01 ... -1.30504340e-01\n",
      "     1.36862755e-01  1.26218438e-01]\n",
      "   ...\n",
      "   [-9.87674892e-02  1.80308044e-01  1.80784255e-01 ... -9.87674892e-02\n",
      "     1.80308044e-01  1.80784255e-01]\n",
      "   [-1.24902040e-01  1.21064395e-01  9.49019492e-02 ... -1.24902040e-01\n",
      "     1.21064395e-01  9.49019492e-02]\n",
      "   [-1.81708843e-01 -1.87479004e-01 -2.35770419e-01 ... -1.81708843e-01\n",
      "    -1.87479004e-01 -2.35770419e-01]]]]\n",
      "Epoch 1/100\n",
      "Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:938: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (140, 40, 30, 9) (9 channels).\n",
      "  warnings.warn(\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:129: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (140, 40, 30, 9) (9 channels).\n",
      "  warnings.warn('NumpyArrayIterator is set to use the '\n",
      "c:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 9s 991ms/step - loss: 2.7088 - accuracy: 0.1714 - val_loss: 2.8548 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/100\n",
      "Learning rate:  0.0033333333333333335\n",
      "5/5 [==============================] - 4s 686ms/step - loss: 2.7140 - accuracy: 0.2000 - val_loss: 2.8201 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/100\n",
      "Learning rate:  0.006666666666666667\n",
      "5/5 [==============================] - 4s 721ms/step - loss: 2.6894 - accuracy: 0.1857 - val_loss: 2.7937 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/100\n",
      "Learning rate:  0.01\n",
      "5/5 [==============================] - 4s 693ms/step - loss: 2.6277 - accuracy: 0.2143 - val_loss: 2.7728 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/100\n",
      "Learning rate:  0.013333333333333334\n",
      "5/5 [==============================] - 4s 802ms/step - loss: 2.5539 - accuracy: 0.2786 - val_loss: 2.7570 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/100\n",
      "Learning rate:  0.016666666666666666\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-625644a2075e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfmeasures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0maccs\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprecisions\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-9e72c7eb0f94>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(pz)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;31m# Fit the model on the batches generated by datagen.flow().\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m         model.fit_generator(datagen.flow(x_train, y_train, batch_size = batch_size),\n\u001b[0m\u001b[0;32m    206\u001b[0m                             \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m                             \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1973\u001b[0m                   \u001b[1;34m'will be removed in a future version. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1974\u001b[0m                   'Please use `Model.fit`, which supports generators.')\n\u001b[1;32m-> 1975\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   1976\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1977\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\patcas rares\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accs=[]\n",
    "precisions = []\n",
    "recalls = []\n",
    "fmeasures = []\n",
    "for i in range(10):\n",
    "    ret = main(i)\n",
    "    accs+=[ret[0]]\n",
    "    precisions+=[ret[1]]\n",
    "    recalls+=[ret[2]]\n",
    "    fmeasures+=[ret[3]]\n",
    "print(accs)\n",
    "print(precisions)\n",
    "print(recalls)\n",
    "print(fmeasures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5675593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(accs))\n",
    "print('accs: ', accs)\n",
    "print('precisions: ', precisions)\n",
    "print('recalls: ', recalls)\n",
    "print('fmeasures: ', fmeasures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907dffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "#define sample data\n",
    "data = accs\n",
    "\n",
    "#create 95% confidence interval for population mean weight\n",
    "st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02434ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5bf6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "990f3e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs=  [0.3333333432674408, 0.5666666626930237, 0.5666666626930237, 0.4333333373069763, 0.4000000059604645, 0.46666666865348816, 0.4333333373069763, 0.46666666865348816, 0.5666666626930237, 0.6333333253860474]\n",
    "precisions=  [0.4483333333333333, 0.736904761904762, 0.6722222222222223, 0.43333333333333335, 0.7, 0.6844444444444444, 0.7225108225108224, 0.6288888888888889, 0.6428571428571428, 0.7252723311546841]\n",
    "recalls=  [0.3333333333333333, 0.5666666666666667, 0.5666666666666667, 0.43333333333333335, 0.4, 0.4666666666666667, 0.43333333333333335, 0.4666666666666667, 0.5666666666666667, 0.6333333333333333]\n",
    "fmeasures=  [0.36973039215686276, 0.637989417989418, 0.6140056022408964, 0.43333333333333335, 0.5066666666666666, 0.5473039215686274, 0.5414814814814815, 0.5333333333333333, 0.5830429732868756, 0.6759259259259259]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "276eca8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4199757689864667, 0.5533575659363238)\n",
      "(0.5605134999069246, 0.7184399562230022)\n",
      "(0.4199757642166506, 0.5533575691166828)\n",
      "(0.47819747292875353, 0.6103651366679308)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "#define sample data\n",
    "data = accs\n",
    "\n",
    "#create 95% confidence interval for population mean weight\n",
    "print(st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)))\n",
    "\n",
    "data = precisions\n",
    "\n",
    "#create 95% confidence interval for population mean weight\n",
    "print(st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)))\n",
    "\n",
    "data = recalls\n",
    "\n",
    "#create 95% confidence interval for population mean weight\n",
    "print(st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)))\n",
    "\n",
    "data = fmeasures\n",
    "\n",
    "#create 95% confidence interval for population mean weight\n",
    "print(st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edd9f401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "150\n",
      "234\n",
      "234\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "print(len(listCancer))\n",
    "print(len(listResults))\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(np.array(listCancer).reshape(len(listCancer),-1), listResults)\n",
    "print(len(X))\n",
    "print(len(y))\n",
    "listCancer = list(X)\n",
    "listResults = list(y)\n",
    "print(listResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a66da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
